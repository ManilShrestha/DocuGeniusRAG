{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "text = extract_text(\"../data/CVPR-Paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVPR\n",
      "#0007\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors to\n",
      "New Generators Using Limited Data\n",
      "\n",
      "Anonymous CVPR submission\n",
      "\n",
      "Paper ID 0007\n",
      "\n",
      "Abstract\n",
      "\n",
      "As generative AI progresses rapidly, new synthetic image\n",
      "generators continue to emerge at a swift pace. Traditional\n",
      "detection methods face two main challenges in adapting to\n",
      "these generators: the forensic traces of synthetic images\n",
      "from new techniques can vastly differ from those learned\n",
      "during training, and access to data for these new genera-\n",
      "tors is often limited. To address these issues, we introduce\n",
      "the Ensemble of Expert Embedders (E3), a novel continual\n",
      "learning framework for updating synthetic image detectors.\n",
      "E3 enables the accurate detection of images from newly\n",
      "emerged generators using minimal training data. Our ap-\n",
      "proach does this by first employing transfer learning to de-\n",
      "velop a suite of expert embedders, each specializing in the\n",
      "forensic traces of a specific generator. Then, all embed-\n",
      "dings are jointly analyzed by an Expert Knowledge Fusion\n",
      "Network to produce accurate and reliable detection deci-\n",
      "sion. Our experiments demonstrate that E3 outperforms\n",
      "existing continual learning methods, including those devel-\n",
      "oped specifically for synthetic image detection.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Over the past several years, a number of AI-based tech-\n",
      "niques have been developed to create visually realistic syn-\n",
      "thetic images. While these synthetic image generators can\n",
      "be used for creative or artistic purposes, they can also be\n",
      "used to malicious ones. Specifically, they enable the cre-\n",
      "ation of fake images that can be used for misinformation or\n",
      "disinformation.\n",
      "\n",
      "To address these challenges, considerable research ef-\n",
      "forts have been directed towards the development of syn-\n",
      "thetic image detection techniques. Prior work has demon-\n",
      "strated the effectiveness of forensic neural networks in dis-\n",
      "cerning synthetic images by identifying unique traces left\n",
      "behind by different image generators. However, a signif-\n",
      "icant limitation of existing detectors arises when they en-\n",
      "counter images generated by previously unseen or emerging\n",
      "\n",
      "001\n",
      "002\n",
      "003\n",
      "004\n",
      "005\n",
      "\n",
      "006\n",
      "007\n",
      "008\n",
      "009\n",
      "010\n",
      "011\n",
      "\n",
      "012\n",
      "013\n",
      "014\n",
      "015\n",
      "016\n",
      "\n",
      "017\n",
      "018\n",
      "019\n",
      "\n",
      "020\n",
      "\n",
      "021\n",
      "022\n",
      "023\n",
      "024\n",
      "025\n",
      "\n",
      "026\n",
      "027\n",
      "\n",
      "028\n",
      "029\n",
      "030\n",
      "\n",
      "031\n",
      "032\n",
      "033\n",
      "034\n",
      "035\n",
      "\n",
      "Figure 1.\n",
      "Illustration of the Ensemble of Expert Embedders\n",
      "(E3) Framework. This figure highlights our innovative approach,\n",
      "specifically crafted to improve the performance and adaptability of\n",
      "synthetic image detection in response to emerging generators.\n",
      "\n",
      "techniques, whose traces differ substantially from those in\n",
      "the training data.\n",
      "\n",
      "This poses a critical need for continually updating syn-\n",
      "thetic image detectors to adapt to new generators. How-\n",
      "ever, this task presents several challenges. Traditional ap-\n",
      "proaches to updating detectors often encounter issues such\n",
      "as catastrophic forgetting and the impracticality of storing\n",
      "and retraining on large datasets [38, 53, 56]. Moreover, lim-\n",
      "ited data availability from newly emerging generators, espe-\n",
      "cially those not yet publicly accessible, further complicates\n",
      "the updating process.\n",
      "\n",
      "In this paper, we propose a novel approach to address\n",
      "these challenges. Instead of relying on a single network to\n",
      "capture all forensic traces, we introduce an Ensemble of Ex-\n",
      "pert Embedders (E3) framework. Each expert embedder is\n",
      "created through transfer learning and specializes in captur-\n",
      "ing traces from a specific image generator. These expert\n",
      "embedders collectively generate a sequence of embeddings,\n",
      "which are then analyzed by an Expert Knowledge Fusion\n",
      "Network (EKFN). The EKFN leverages information from\n",
      "\n",
      "036\n",
      "037\n",
      "\n",
      "038\n",
      "039\n",
      "\n",
      "040\n",
      "041\n",
      "042\n",
      "043\n",
      "044\n",
      "045\n",
      "\n",
      "046\n",
      "\n",
      "047\n",
      "048\n",
      "049\n",
      "050\n",
      "051\n",
      "\n",
      "052\n",
      "053\n",
      "054\n",
      "055\n",
      "\n",
      "1\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "056\n",
      "057\n",
      "058\n",
      "059\n",
      "060\n",
      "\n",
      "061\n",
      "062\n",
      "\n",
      "063\n",
      "064\n",
      "065\n",
      "066\n",
      "\n",
      "067\n",
      "068\n",
      "069\n",
      "070\n",
      "071\n",
      "072\n",
      "\n",
      "073\n",
      "074\n",
      "075\n",
      "076\n",
      "077\n",
      "\n",
      "078\n",
      "079\n",
      "080\n",
      "\n",
      "081\n",
      "\n",
      "082\n",
      "083\n",
      "\n",
      "084\n",
      "085\n",
      "086\n",
      "087\n",
      "088\n",
      "089\n",
      "\n",
      "090\n",
      "091\n",
      "092\n",
      "093\n",
      "094\n",
      "\n",
      "095\n",
      "096\n",
      "097\n",
      "098\n",
      "099\n",
      "100\n",
      "\n",
      "101\n",
      "\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "all expert embedders to accurately perform synthetic im-\n",
      "age detection. Our experimental results demonstrate that\n",
      "the proposed E3 framework significantly outperforms exist-\n",
      "ing continual learning approaches, including those designed\n",
      "specifically for updating synthetic image detectors, and can\n",
      "perform strongly even with very limited data from new gen-\n",
      "erators. The novel contributions of this paper are:\n",
      "• We propose E3, a new approach that can update synthetic\n",
      "image detectors to accurately detect newly emerging gen-\n",
      "erators, while requiring only minimal amounts of training\n",
      "data to be retained in a memory buffer.\n",
      "\n",
      "• We develop a novel approach to create a set of expert em-\n",
      "bedders to accurately capture traces from each new tar-\n",
      "get generator. Each expert can be adapted using a small\n",
      "amount of images from its target generator.\n",
      "\n",
      "• We propose an expert knowledge fusion network able to\n",
      "examine forensic evidence produced by the set of all ex-\n",
      "perts and make accurate detection decision.\n",
      "\n",
      "• We conduct an extensive set of experiments that demon-\n",
      "strate that E3 outperforms competing approaches from\n",
      "continual learning, including approaches specifically de-\n",
      "signed to update synthetic image detectors. Notably, E3\n",
      "consistently outperforms competitors across various de-\n",
      "tector architectures and excels even with limited data\n",
      "from new generators.\n",
      "\n",
      "2. Background\n",
      "\n",
      "Synthetic Image Generators. Considerable effort has been\n",
      "devoted to developing systems that can visually understand\n",
      "the world through the process of mimicking; the first of\n",
      "such work is Variational Auto-Encoder by Kingma and\n",
      "Welling [35], which led to the development of Generative\n",
      "Adverserial Networks (GANs) by Goodfellow et al. [18].\n",
      "This work inspired many other subsequent works [8, 26,\n",
      "28, 47, 71], which continued to improve visual understand-\n",
      "ing through enhancing the generation’s quality, diversity,\n",
      "and realism. Notably, the introduction of diffusion model\n",
      "for image generation by Ho et al. [21] set the stage for the\n",
      "explosion of research in this area, resulting in many pop-\n",
      "ular generation methods such as: Stable Diffusion [54],\n",
      "DALL·E [50], Midjourney [1], Cascade Diffusion [22],\n",
      "etc. [3, 15, 64, 73]. Because the rate of new generation\n",
      "methods being emerged continues to rapidly increase, this\n",
      "present a formidable challenge for existing synthetic image\n",
      "detectors whose capability is often limited to only detecting\n",
      "generators seen during training [13, 60].\n",
      "\n",
      "Synthetic Image Traces and Detection. Numerous ef-\n",
      "forts have been undertaken to distinguish synthetic images\n",
      "from real ones. Marra et al. [41] demonstrated that GAN-\n",
      "generated images possess unique “fingerprints” useful for\n",
      "detection and source attribution. This work showed that in-\n",
      "dividual image generators all left behind identifiable arti-\n",
      "\n",
      "facts, called forensic traces, that can be used to detect their\n",
      "generated images. Subsequent that it is difficult for a de-\n",
      "tector to generalize to forensic traces from a new family of\n",
      "generator [13]. To address the rapid release of image gen-\n",
      "erators, a broad spectrum of new synthetic image detectors\n",
      "were developed [13, 25, 41, 58, 59, 62, 68, 69, 72]. Most\n",
      "notably, recent open-set approaches [17] were developed to\n",
      "detect unseen generators by classifying them to belong to\n",
      "an “unknown” class.\n",
      "\n",
      "Continual Learning For Synthetic Image Detection.\n",
      "Given the distinct forensic traces left by various generator\n",
      "families, it is crucial for image detectors to continually up-\n",
      "date their knowledge with new generators. Traditional re-\n",
      "training is often data inefficient and fine-tuning may often\n",
      "lead to catastrophic forgetting [45, 46]. Hence, a number\n",
      "of methods have been developed to allow the base model\n",
      "to adapt without losing prior knowledge [10, 38, 52, 56].\n",
      "Notably, Marra et al. [42] and Kim et al. [33] have success-\n",
      "fully applied such strategies to synthetic image detection,\n",
      "demonstrating the feasibility and effectiveness of continual\n",
      "learning in this domain.\n",
      "\n",
      "3. Problem Formulation\n",
      "\n",
      "We begin by assuming that an initial synthetic image detec-\n",
      "tor f0 has been created to detect images in a set of known\n",
      "generators G0. We will refer to this detector as the base-\n",
      "line detector. We further assume that f0 was trained using a\n",
      "large baseline training dataset B consisting of both real and\n",
      "synthetic images made using generators in G0.\n",
      "\n",
      "After the baseline detector has been trained, new syn-\n",
      "thetic image generators gk /∈ G0 will continue to emerge.\n",
      "As previous research has shown, f0 will have a difficult time\n",
      "detecting these new generators if the forensic traces or “fin-\n",
      "gerprints” left by these generators are substantially different\n",
      "than those left by the generators in G0 [13]. Because of this,\n",
      "the synthetic image detector will need to be continually up-\n",
      "dated as new generators emerge.\n",
      "\n",
      "This presents an important set of problems. Training a\n",
      "new detector from scratch can be resource intensive and\n",
      "requires that the baseline dataset B be stored indefinitely.\n",
      "Instead, it is preferable to retain only a small dataset M,\n",
      "known as the memory buffer, to update the detector. Care\n",
      "must be taken when updating the detector, however, because\n",
      "naively fine-tuning the detector can lead to catastrophic for-\n",
      "getting. When this happens, the detector is able to identify\n",
      "images produced by the new generator, but loses its ability\n",
      "to reliably detect images from previous generators.\n",
      "\n",
      "Additionally, challenges may arise when access to syn-\n",
      "thetic images generated by the new generator gk is limited.\n",
      "For instance, a generator may not be available to the general\n",
      "public, but a small number of examples from the generator\n",
      "may be publicly obtainable. An example of this is OpenAI’s\n",
      "\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "\n",
      "116\n",
      "\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "\n",
      "128\n",
      "\n",
      "129\n",
      "130\n",
      "\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "\n",
      "135\n",
      "\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "\n",
      "142\n",
      "\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "\n",
      "152\n",
      "\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "\n",
      "157\n",
      "\n",
      "2\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "\n",
      "179\n",
      "\n",
      "180\n",
      "181\n",
      "\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "\n",
      "188\n",
      "\n",
      "189\n",
      "190\n",
      "191\n",
      "\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "\n",
      "198\n",
      "\n",
      "199\n",
      "200\n",
      "\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "\n",
      "207\n",
      "208\n",
      "\n",
      "209\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "Sora generator [9]. Currently, access to Sora is restricted,\n",
      "however OpenAI has shared a small number of sample out-\n",
      "puts. Alternatively, a party engaged in a misinformation\n",
      "campaign may have developed a new generator for their\n",
      "purposes. In this case, access to images produced by this\n",
      "new generator is severely limited by the number of exam-\n",
      "ples that have been released into the wild. Training an ef-\n",
      "fective synthetic image detector that can continuously adapt\n",
      "to new generators with limited data is highly non-trivial.\n",
      "\n",
      "To formalize the problem of updating the detector given\n",
      "these constraints, we define Mk−1 as the memory buffer\n",
      "when the kth new generator is introduced. Mk−1 consists\n",
      "of two subsets: R which contains real images and Sk−1\n",
      "which contains images made by each of the previously seen\n",
      "generators. The memory buffer has a fixed size |Mℓ|= M ,\n",
      "that remains constant even as more generators emerge. Ad-\n",
      "ditionally, we define Dk as the set of images from the new\n",
      "generator gk that can be used to update the detector. We\n",
      "assume that |Dk|= N , and that N is significantly smaller\n",
      "than the number of images in B, i.e. we are allowed a small\n",
      "number of images from the new generator.\n",
      "\n",
      "4. Proposed Approach\n",
      "\n",
      "While a synthetic image detector f is often thought of as a\n",
      "single network, we can conceptually view it as the composi-\n",
      "tion of an embedder ϕ and a classifier h, such that f = h◦ϕ.\n",
      "When adopting this view, the embeddings produced by ϕ\n",
      "capture forensic traces left by synthetic image generators,\n",
      "while the detector maps these embeddings to detection de-\n",
      "In practice, lower layers of the network can be\n",
      "cisions.\n",
      "thought of as the embedder, while the final layer or layers\n",
      "can be thought of as the classifier.\n",
      "\n",
      "When a continual learning technique is used to update f ,\n",
      "this typically involves using a special process that retrains\n",
      "f to detect images from both a new generator and existing\n",
      "generators using Mk and Dk. This corresponds to updating\n",
      "ϕ so that it learns an embedding space that is able to jointly\n",
      "capture forensic traces from previous generators as well as\n",
      "the new generator. However, since forensic traces from dif-\n",
      "ferent generators can be substantially different [13], learn-\n",
      "ing an embedding space that successfully does this with a\n",
      "limited amount of data in Mk and Dk can be challenging.\n",
      "To overcome this challenge, we propose a new frame-\n",
      "work to update f called Ensemble of Expert Embedders\n",
      "(E3).\n",
      "In this framework, we do not attempt to learn a\n",
      "single embedding space to capture the traces left by all\n",
      "Instead, we form a set of expert embedders\n",
      "generators.\n",
      "Φk = {ϕ0, . . . , ϕk}, where each expert embedder ϕℓ is spe-\n",
      "cialized to capture traces from generator gℓ. When forming\n",
      "an expert embedder ϕk for a new generator gk, we allow it\n",
      "to experience catastrophic forgetting, since other experts in\n",
      "Φk are dedicated to capturing traces from other generators.\n",
      "To analyze an image, it is passed through all embedders\n",
      "\n",
      "Figure 2. End-to-end architecture workflow: An image is first pro-\n",
      "cessed by expert embedders to generate embeddings, which are\n",
      "passed through a transformer and an MLP layer to produce the de-\n",
      "tection decision.\n",
      "\n",
      "in Φk to produce a sequence of embeddings {x0, . . . , xk}.\n",
      "Each embedding xℓ captures evidence that the image was\n",
      "generated by gℓ. This set of embeddings is then analyzed\n",
      "using an Expert Knowledge Fusion Network (EKFN) to\n",
      "produce a single detection decision. As a result, E3 is able\n",
      "to leverage forensic evidence within the union of every ex-\n",
      "pert’s embedding space, as opposed to relying on a single\n",
      "embedding space to capture all forensic evidence.\n",
      "\n",
      "We describe the process to form each expert embedder,\n",
      "update the memory buffer, and train the expert knowledge\n",
      "fusion network in the following subsections.\n",
      "\n",
      "4.1. Creating A New Expert Embedder\n",
      "\n",
      "Let Gk−1 be the set of currently known synthetic image gen-\n",
      "erators. When a new synthetic image generator gk /∈ Gk−1\n",
      "emerges, we need to update our detector. To do this, we\n",
      "must first create a new expert embedder ϕk to capture traces\n",
      "left by gk. We accomplish this by adapting the baseline de-\n",
      "tector f0 using transfer learning. An overview of this pro-\n",
      "cess is shown in Fig. 3.\n",
      "\n",
      "We start by forming a new expert training set Tk of real\n",
      "and synthetic images that we will use to create ϕk. This\n",
      "is done by collecting a set Dk of N synthetic images cre-\n",
      "ated by the new generator. Real images are taken from the\n",
      "current memory buffer Mk−1, which contains the subset R\n",
      "consisting of M/2 real images. As a result, the new expert\n",
      "training set is Tk = Dk ∪ R.\n",
      "\n",
      "Next, we use Tk to update f0 to detect images made by\n",
      "gk. Specifically, we form a new detector ˆfk by fine tuning\n",
      "f0 with Tk using the loss function\n",
      "\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "\n",
      "221\n",
      "\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "Lϕ = −\n",
      "\n",
      "Iℓ∈Tk\n",
      "\n",
      "|Dk|\n",
      "|Tk|\n",
      "\n",
      "tℓlog( ˆfk(Iℓ))\n",
      "\n",
      "+\n",
      "\n",
      "|R|\n",
      "|Tk|\n",
      "\n",
      "(1 − tℓ)log(1 − ˆfk(Iℓ))\n",
      "\n",
      "(1)\n",
      "\n",
      "239\n",
      "\n",
      "3\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "Algorithm 1 Training Expert Knowledge Fusion Network\n",
      "Require: Mk−1, Dk, Φk\n",
      "\n",
      "be the procedure to sample images from\n",
      "\n",
      "D be the procedure to sample images from Dk\n",
      "\n",
      "M\n",
      "\n",
      "// Let ω(k−1)\n",
      "Mk−1 uniformly at random\n",
      "// Let ω(k)\n",
      "uniformly at random\n",
      "// Let S be the number of training steps\n",
      "M (Mk−1) (cid:83) ω(k)\n",
      "Mk ← ω(k−1)\n",
      "X ← {}\n",
      "for step = 1, · · · , S do\n",
      "\n",
      "D (Dk)\n",
      "\n",
      "Figure 3. This figure shows the creation of a new expert ϕk when\n",
      "a generator gk emerges. The baseline embedder f0 is fine-tuned\n",
      "with data Dk and real images R from memory buffer Mk−1. The\n",
      "new classifier ˆfk’s embedder ϕk is then preserved.\n",
      "\n",
      "where Iℓ is the ℓ-th image in Tk, and tℓ is the class label of\n",
      "the image Iℓ where 0 means the image is real and 1 means\n",
      "the image is synthetic. By doing this, the resulting detector\n",
      "ˆfk will be adapted to detect images made by gk. However, it\n",
      "will likely experience catastrophic forgetting and be poorly\n",
      "suited to detect generators in Gk−1.\n",
      "\n",
      "After obtaining the expert detector ˆfk, we decompose\n",
      "it into its corresponding embedder ϕk and classifier com-\n",
      "ponents. The classifier portion is discarded, while ϕk is\n",
      "retained. Finally, ϕk is added to the set of expert embed-\n",
      "ders Φk = Φk−1 ∪ {ϕk}. We note Φ0 is initialized as\n",
      "Φ0 = {ϕ0}.\n",
      "\n",
      "4.2. Expert Knowledge Fusion Network\n",
      "\n",
      "After adding the new expert embedder ϕk to the set of ex-\n",
      "isting expert embedders, we must update the Expert Knowl-\n",
      "edge Fusion Network (EKFN) ψ so that it can utilize the\n",
      "embeddings produced by ϕk. We do this by first updating\n",
      "the memory buffer Mk, then use this data to train ψ to per-\n",
      "form detection. This process is described below.\n",
      "Updating The Memory Buffer. When a new generator gk\n",
      "emerges, the current memory buffer Mk−1 does not include\n",
      "images generated by it. As a result, we need to add images\n",
      "from Dk to form the updated memory buffer Mk. How-\n",
      "ever, since the memory buffer has a fixed size M , we must\n",
      "remove synthetic images made by previous generators in\n",
      "Gk−1 to make space for these new images.\n",
      "\n",
      "We first note that Sk−1, i.e. the subset of Mk−1 that cor-\n",
      "responds to synthetic images, contains Pk−1 = ⌊M/(2k)⌋\n",
      "images from each generator in Gk−1. We now need to form\n",
      "an updated set of synthetically generated images Sk that\n",
      "contains an equal number of images from all generators in\n",
      "Gk. To do this, we retain Pk = (k/(k + 1))Pk−1 images\n",
      "corresponding to each generator in Gk−1 from Sk−1 drawn\n",
      "uniformly at random. These images are added to the new\n",
      "\n",
      "240\n",
      "241\n",
      "242\n",
      "\n",
      "243\n",
      "244\n",
      "245\n",
      "\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "\n",
      "251\n",
      "\n",
      "252\n",
      "\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "\n",
      "257\n",
      "258\n",
      "\n",
      "259\n",
      "260\n",
      "\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "\n",
      "266\n",
      "267\n",
      "268\n",
      "\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "\n",
      "for i = 1;\n",
      "\n",
      "i ≤ |Mk|;\n",
      "\n",
      "i += 1 do\n",
      "\n",
      "Ii = Mk [i]; Xi ← {}\n",
      "for j = 1; j ≤ |Φk|; j += 1 do\n",
      "\n",
      "ϕj ← Φk [j]\n",
      "Xi ← Xi ∪ {ϕj(Ii)}\n",
      "\n",
      "end for\n",
      "X ← X ∪ {Xi}\n",
      "\n",
      "end for\n",
      "L ← Lψ(Mk, X)\n",
      "ψ ← BackPropagate(L, ψ)\n",
      "\n",
      "▷ Compute loss with Eqn. 2\n",
      "\n",
      "end for\n",
      "return ψ\n",
      "\n",
      "updated memory buffer’s subset of synthetic images Sk. Fi-\n",
      "nally, we add Pk images drawn uniformly at random from\n",
      "Dk to Sk to form a complete set of synthetic images from\n",
      "all generators in Gk for the memory buffer. The updated\n",
      "memory buffer is now formed as Mk = Sk ∪ R.\n",
      "\n",
      "Training The EKFN. Once we have updated the memory\n",
      "buffer, we use Mk to train the EKFN ψ. To do this, we first\n",
      "use the set of expert embedders Φk to extract a sequence of\n",
      "embeddings {xℓ\n",
      "k} for each image Iℓ ∈ Mk. Next,\n",
      "we randomly initialize ψ and train it using the loss function\n",
      "\n",
      "0, . . . , xℓ\n",
      "\n",
      "274\n",
      "275\n",
      "276\n",
      "\n",
      "277\n",
      "278\n",
      "\n",
      "279\n",
      "\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "\n",
      "Lψ = −\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "Iℓ∈Mk\n",
      "\n",
      "ψ(xℓ\n",
      "\n",
      "0, . . . , xℓ\n",
      "+ (1 − ψ(xℓ\n",
      "\n",
      "k) log tℓ\n",
      "0, . . . , xℓ\n",
      "\n",
      "k)) log(1 − tℓ).\n",
      "\n",
      "(2)\n",
      "\n",
      "284\n",
      "\n",
      "After training the EKFN, the synthetic image detector has\n",
      "been completely updated and the network is ready to per-\n",
      "form detection.\n",
      "\n",
      "EKFN Architecture. We can think of each embedder ϕℓ\n",
      "as a mechanism that searches for evidence of traces left\n",
      "by generator gℓ. The resulting embedding xℓ can then be\n",
      "viewed as a measurement of this evidence. When viewed\n",
      "this way, the purpose of the EKFN is to examine all forms\n",
      "of evidence in the context of one another in order to make\n",
      "an accurate forensic decision. In light of this, we design the\n",
      "EKFN such that it uses a transformer’s self-attention mech-\n",
      "anism to examine the sequence of embeddings in the con-\n",
      "text of one another.\n",
      "\n",
      "An overview of our EKFN architecture is shown in\n",
      "\n",
      "285\n",
      "286\n",
      "287\n",
      "\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "\n",
      "298\n",
      "\n",
      "4\n",
      "\n",
      "\fCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "e\n",
      "n\n",
      "i\n",
      "l\n",
      "e\n",
      "s\n",
      "a\n",
      "B\n",
      "\n",
      "g\n",
      "n\n",
      "i\n",
      "g\n",
      "r\n",
      "e\n",
      "m\n",
      "E\n",
      "&\n",
      "w\n",
      "e\n",
      "N\n",
      "\n",
      "Abbrv. Generator Name\n",
      "\n",
      "Abbrv. Generator Name\n",
      "\n",
      "s SG\n",
      "r\n",
      "o\n",
      "t\n",
      "a\n",
      "r\n",
      "e\n",
      "n\n",
      "e\n",
      "G\n",
      "\n",
      "SG2\n",
      "\n",
      "ProG\n",
      "\n",
      "StyleGAN [29]\n",
      "\n",
      "StyleGAN2 [30]\n",
      "\n",
      "ProGAN [27]\n",
      "\n",
      "s\n",
      "r\n",
      "o\n",
      "t\n",
      "a\n",
      "r\n",
      "e\n",
      "n\n",
      "e\n",
      "G\n",
      "\n",
      "SD1.4\n",
      "\n",
      "Stable Diffusion 1.4 [54]\n",
      "\n",
      "GLD\n",
      "\n",
      "Glide [48]\n",
      "\n",
      "MJ\n",
      "\n",
      "Midjourney [1]\n",
      "\n",
      "DLM DALL-E Mini [14]\n",
      "\n",
      "TT\n",
      "\n",
      "Taming Transformers [16]\n",
      "\n",
      "SD2.1\n",
      "\n",
      "Stable Diffusion 2.1 [54]\n",
      "\n",
      "CIPS\n",
      "\n",
      "Cond.Indep. Pix.Synt. [2]\n",
      "\n",
      "BG\n",
      "\n",
      "BigGAN [8]\n",
      "\n",
      "s\n",
      "r\n",
      "o\n",
      "t\n",
      "a\n",
      "r\n",
      "e\n",
      "n\n",
      "e\n",
      "G\n",
      "g\n",
      "n\n",
      "i\n",
      "g\n",
      "r\n",
      "e\n",
      "m\n",
      "E\n",
      "&\n",
      "w\n",
      "e\n",
      "N\n",
      "\n",
      "VQD\n",
      "\n",
      "Vec. Quant. Diff. [19]\n",
      "\n",
      "DIG\n",
      "\n",
      "SG3\n",
      "\n",
      "GF\n",
      "\n",
      "DL2\n",
      "\n",
      "LD\n",
      "\n",
      "EG3\n",
      "\n",
      "PG\n",
      "\n",
      "Diffusion GAN [63]\n",
      "\n",
      "StyleGAN3 [31]\n",
      "\n",
      "GANformer [24]\n",
      "\n",
      "DALL-E 2 [51]\n",
      "\n",
      "Latent Diffusion [54]\n",
      "\n",
      "Eff.Geo. 3D GAN [11]\n",
      "\n",
      "Projected GAN [55]\n",
      "\n",
      "SD1.1\n",
      "\n",
      "Stable Diffusion 1.1 [54]\n",
      "\n",
      "DDG\n",
      "\n",
      "DDP\n",
      "\n",
      "Denoise Diff. GAN [65]\n",
      "\n",
      "Denoise Diff. Prob. [21]\n",
      "\n",
      "Table 1. The full names and abbreviations for all synthetic image\n",
      "generators used in this paper.\n",
      "\n",
      "Fig. 2. The sequence of embeddings is first passed di-\n",
      "rectly to a transformer. Each transformer output is used to\n",
      "weight its corresponding embedding through element-wise\n",
      "multiplication. The resulting weighted embeddings are then\n",
      "passed to an MLP, which makes a final detection decision.\n",
      "In practice, we form the MLP using 2 layers and the trans-\n",
      "former using 20 layers in accordance to the results of an\n",
      "ablation study reported on in Sec. 7. We note that simpler\n",
      "EKFN architectures can be utilized to reduce network com-\n",
      "plexity at the cost of decreased performance.\n",
      "\n",
      "4.3. E3 Framework Summary\n",
      "\n",
      "E3 uses the following steps to update a synthetic image de-\n",
      "tector after the emergence of a new generator:\n",
      "1. Form a new training set using real images from the mem-\n",
      "ory buffer and a set of images from the new generator.\n",
      "2. Utilize this training set to create an expert embedder\n",
      "specifically trained to capture forensic traces associated\n",
      "with the new generator, and incorporate it into the en-\n",
      "semble of expert embedders.\n",
      "\n",
      "3. Update the memory buffer with the data from the new\n",
      "\n",
      "generator.\n",
      "\n",
      "4. Retrain the Expert Knowledge Fusion Network (EKFN)\n",
      "\n",
      "using the data stored in the memory buffer.\n",
      "\n",
      "5. Experiments\n",
      "\n",
      "5.1. Experimental Setup\n",
      "\n",
      "To form a diverse set of real\n",
      "\n",
      "Baseline Detector Dataset. We created a baseline dataset\n",
      "in order to train and evaluate the baseline synthetic im-\n",
      "age detectors.\n",
      "images,\n",
      "we gathered 72,000 images equally distributed among the\n",
      "CoCo dataset [39], the LSUN dataset [67], and the CelebA\n",
      "dataset [40]. As for the set of synthetic images, we also\n",
      "gathered 72,000 GAN-generated images from the datasets\n",
      "used in prior work [17, 61]. The synthetic images in this set\n",
      "are equally distributed among three GANs: StyleGAN [29],\n",
      "StyleGAN 2 [30], and ProGAN [27]. Additionally, to form\n",
      "\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "\n",
      "308\n",
      "\n",
      "309\n",
      "\n",
      "310\n",
      "311\n",
      "\n",
      "312\n",
      "\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "\n",
      "322\n",
      "\n",
      "323\n",
      "\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "\n",
      "5\n",
      "\n",
      "the set of images used for training, validation, and testing,\n",
      "we split the set of real images and the set of synthetic im-\n",
      "ages each into their own disjoint subsets. Specifically, we\n",
      "used 132,000 images for training with 12,000 images held\n",
      "out for validation, and 12,000 images for testing.\n",
      "\n",
      "Emerging Generators Dataset. To simulate a constantly\n",
      "evolving environment in which new synthetic image gen-\n",
      "erators rapidly emerge, we created a dataset to train and\n",
      "evaluate our proposed approach in a continual learning set-\n",
      "ting. To accomplish this, we first selected a diverse set of\n",
      "generators, composed of 19 different generation techniques.\n",
      "These techniques are listed in Tab. 1. All synthetic im-\n",
      "ages in this dataset are assembled from publicly available\n",
      "datasets [17, 49, 58]. Then, from each generator, we gath-\n",
      "ered 500 images exclusively used for training and 400 im-\n",
      "ages exclusively used for testing.\n",
      "\n",
      "Baseline Detector Training. We obtained a baseline de-\n",
      "tector by training MISLnet [6] to distinguish between real\n",
      "and synthetic images in our baseline detector dataset. We\n",
      "chose MISLnet as the architecture for our experiments be-\n",
      "cause it is lightweight and has shown repeatedly to obtain\n",
      "strong performance on detecting synthetic images and other\n",
      "image forensic tasks [4, 5, 12, 43, 44]. We trained MISLnet\n",
      "using a Cross-Entropy Loss function via the ADAM opti-\n",
      "mizer [34] with a constant learning rate of 5.0 × 10−5. This\n",
      "model resulted in an accuracy of 0.97 on the testing portion\n",
      "of our baseline detector dataset.\n",
      "\n",
      "Memory Buffer. Throughout all experiments in this paper,\n",
      "we fixed the size of memory buffer, M, to 1000 images\n",
      "from which 500 are synthetic and 500 are real. The syn-\n",
      "thetic images are then equally distributed among all previ-\n",
      "ously known generators. We note that the memory buffer\n",
      "is continually updated after each step to include samples of\n",
      "the new generator.\n",
      "\n",
      "Performance Metrics. To evaluate the performance of our\n",
      "proposed approach and others, we chose the Area Under the\n",
      "ROC Curve (AUC) metric. Additionally, in order to obtain\n",
      "a more complete picture of our performance, we provided\n",
      "the Relative Error Reduction (RER) with respect to the sec-\n",
      "ond best performing method, reported as a percentage. This\n",
      "metric is calculated as follows:\n",
      "\n",
      "RER = 100 ×\n",
      "\n",
      "AUCN − AUCR\n",
      "1 − AUCR\n",
      "\n",
      ",\n",
      "\n",
      "(3)\n",
      "\n",
      "where AUCR is the AUC of the referencing method, and\n",
      "AUCN is the AUC of the method being compared against.\n",
      "Competing Methods. We compared our method against\n",
      "multiple approaches, including two naive strategies: Base-\n",
      "line, where the baseline detector is never updated, and\n",
      "Fine-tuning, where the baseline detector is updated exclu-\n",
      "sively with data from new generators. Additionally, we\n",
      "benchmarked our approach against seven well-established\n",
      "continual learning strategies: Learning Without Forgetting\n",
      "\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "\n",
      "339\n",
      "\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "\n",
      "350\n",
      "351\n",
      "352\n",
      "\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "\n",
      "359\n",
      "360\n",
      "\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "\n",
      "367\n",
      "\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "\n",
      "374\n",
      "\n",
      "375\n",
      "\n",
      "376\n",
      "377\n",
      "\n",
      "378\n",
      "379\n",
      "380\n",
      "\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "Method\n",
      "\n",
      "Baseline\n",
      "\n",
      "Fine-Tune\n",
      "\n",
      "LWF [38]\n",
      "\n",
      "ER [53]\n",
      "\n",
      "DER++ [10]\n",
      "\n",
      "UDIL [56]\n",
      "\n",
      "ICaRL [52]\n",
      "\n",
      "MT-SC [42]\n",
      "\n",
      "MT-MC [42]\n",
      "\n",
      "Ours\n",
      "\n",
      "Synthetic Image Generators\n",
      "\n",
      "SD1.4 GLD MJ\n",
      "\n",
      "DLM TT\n",
      "\n",
      "SD2.1 CIPS\n",
      "\n",
      "BG VQD DIG SG3\n",
      "\n",
      "GF\n",
      "\n",
      "DL2\n",
      "\n",
      "LD\n",
      "\n",
      "EG3\n",
      "\n",
      "PG\n",
      "\n",
      "SD1.1 DDG DDP\n",
      "\n",
      "0.89\n",
      "\n",
      "0.65\n",
      "\n",
      "0.91\n",
      "\n",
      "0.94\n",
      "\n",
      "0.94\n",
      "\n",
      "0.94\n",
      "\n",
      "0.88\n",
      "\n",
      "0.91\n",
      "\n",
      "0.92\n",
      "\n",
      "0.99\n",
      "\n",
      "0.98\n",
      "\n",
      "0.83\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "1.00\n",
      "\n",
      "0.96\n",
      "\n",
      "0.99\n",
      "\n",
      "1.00\n",
      "\n",
      "1.00\n",
      "\n",
      "0.85\n",
      "\n",
      "0.65\n",
      "\n",
      "0.92\n",
      "\n",
      "0.97\n",
      "\n",
      "0.96\n",
      "\n",
      "0.97\n",
      "\n",
      "0.91\n",
      "\n",
      "0.93\n",
      "\n",
      "0.95\n",
      "\n",
      "0.99\n",
      "\n",
      "0.86\n",
      "\n",
      "0.67\n",
      "\n",
      "0.82\n",
      "\n",
      "0.96\n",
      "\n",
      "0.95\n",
      "\n",
      "0.97\n",
      "\n",
      "0.91\n",
      "\n",
      "0.93\n",
      "\n",
      "0.95\n",
      "\n",
      "0.99\n",
      "\n",
      "0.81\n",
      "\n",
      "0.64\n",
      "\n",
      "0.85\n",
      "\n",
      "0.96\n",
      "\n",
      "0.95\n",
      "\n",
      "0.96\n",
      "\n",
      "0.89\n",
      "\n",
      "0.92\n",
      "\n",
      "0.94\n",
      "\n",
      "0.98\n",
      "\n",
      "0.89\n",
      "\n",
      "0.69\n",
      "\n",
      "0.92\n",
      "\n",
      "0.94\n",
      "\n",
      "0.93\n",
      "\n",
      "0.94\n",
      "\n",
      "0.91\n",
      "\n",
      "0.92\n",
      "\n",
      "0.93\n",
      "\n",
      "0.98\n",
      "\n",
      "0.76\n",
      "\n",
      "0.85\n",
      "\n",
      "0.85\n",
      "\n",
      "0.99\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.96\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.77\n",
      "\n",
      "0.75\n",
      "\n",
      "0.88\n",
      "\n",
      "0.91\n",
      "\n",
      "0.89\n",
      "\n",
      "0.92\n",
      "\n",
      "0.87\n",
      "\n",
      "0.90\n",
      "\n",
      "0.90\n",
      "\n",
      "0.99\n",
      "\n",
      "0.85\n",
      "\n",
      "0.88\n",
      "\n",
      "0.96\n",
      "\n",
      "0.98\n",
      "\n",
      "0.97\n",
      "\n",
      "0.98\n",
      "\n",
      "0.94\n",
      "\n",
      "0.96\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.76\n",
      "\n",
      "0.88\n",
      "\n",
      "0.94\n",
      "\n",
      "0.98\n",
      "\n",
      "0.97\n",
      "\n",
      "0.98\n",
      "\n",
      "0.95\n",
      "\n",
      "0.96\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.98\n",
      "\n",
      "0.90\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.91\n",
      "\n",
      "0.97\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.73\n",
      "\n",
      "0.92\n",
      "\n",
      "0.95\n",
      "\n",
      "0.99\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.95\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.97\n",
      "\n",
      "0.79\n",
      "\n",
      "0.94\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.95\n",
      "\n",
      "0.97\n",
      "\n",
      "0.99\n",
      "\n",
      "1.00\n",
      "\n",
      "0.81\n",
      "\n",
      "0.68\n",
      "\n",
      "0.72\n",
      "\n",
      "0.90\n",
      "\n",
      "0.90\n",
      "\n",
      "0.91\n",
      "\n",
      "0.86\n",
      "\n",
      "0.88\n",
      "\n",
      "0.88\n",
      "\n",
      "0.96\n",
      "\n",
      "0.98\n",
      "\n",
      "0.89\n",
      "\n",
      "0.97\n",
      "\n",
      "1.00\n",
      "\n",
      "1.00\n",
      "\n",
      "1.00\n",
      "\n",
      "0.97\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "1.00\n",
      "\n",
      "0.89\n",
      "\n",
      "0.76\n",
      "\n",
      "0.92\n",
      "\n",
      "0.96\n",
      "\n",
      "0.96\n",
      "\n",
      "0.97\n",
      "\n",
      "0.89\n",
      "\n",
      "0.94\n",
      "\n",
      "0.96\n",
      "\n",
      "0.99\n",
      "\n",
      "0.87\n",
      "\n",
      "0.74\n",
      "\n",
      "0.91\n",
      "\n",
      "0.94\n",
      "\n",
      "0.93\n",
      "\n",
      "0.93\n",
      "\n",
      "0.88\n",
      "\n",
      "0.91\n",
      "\n",
      "0.92\n",
      "\n",
      "0.99\n",
      "\n",
      "0.67\n",
      "\n",
      "0.87\n",
      "\n",
      "0.95\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "0.93\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "1.00\n",
      "\n",
      "0.78\n",
      "\n",
      "0.84\n",
      "\n",
      "0.94\n",
      "\n",
      "0.99\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.96\n",
      "\n",
      "0.98\n",
      "\n",
      "0.99\n",
      "\n",
      "0.99\n",
      "\n",
      "Avg. ± Std.\n",
      "\n",
      "0.85 ± .090\n",
      "\n",
      "0.78 ± .098\n",
      "\n",
      "0.91 ± .066\n",
      "\n",
      "0.97 ± .029\n",
      "\n",
      "0.96 ± .031\n",
      "\n",
      "0.97 ± .028\n",
      "\n",
      "0.92 ± .034\n",
      "\n",
      "0.95 ± .034\n",
      "\n",
      "0.96 ± .036\n",
      "\n",
      "0.99 ± .008\n",
      "\n",
      "Table 2. Detection performance of each method when adapting to a single new generator. Note that the “baseline” detector has only seen\n",
      "data from the baseline detector dataset and all approaches started from the same baseline detector network. Performance numbers are\n",
      "measured in AUC.\n",
      "\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "\n",
      "390\n",
      "391\n",
      "\n",
      "392\n",
      "\n",
      "393\n",
      "\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "\n",
      "399\n",
      "400\n",
      "\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "\n",
      "406\n",
      "407\n",
      "408\n",
      "\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "\n",
      "(LWF) [38], Experience Replay (ER) [53], Dark Experience\n",
      "Replay (DER++) [10], Unified Domain Incremental Learn-\n",
      "ing (UDIL) [56], Incremental Classifier and Representation\n",
      "Learning (ICaRL) [52], Multi-Task Single-Classifier (MT-\n",
      "SC) [42], and Multi-Task Multi-Classifier (MT-MC) [42].\n",
      "Additionally we benchmarked ICaRL, MT-SC, and MT-MC\n",
      "using our own implementations.\n",
      "\n",
      "5.2. Adapting to One New Generator\n",
      "\n",
      "In this experiment, we evaluated E3’s ability to detect syn-\n",
      "thetic images from one new generator. This is important\n",
      "because in reality we do not know beforehand which gen-\n",
      "erator we need to adapt our detector to. As a result, we re-\n",
      "peated this experiment for all 19 generators in our dataset.\n",
      "We then compared our method’s performance to other tech-\n",
      "niques used to update the baseline detector. We note that all\n",
      "approaches started with the same baseline detector network.\n",
      "We present the results of these experiments in Tab. 2. In\n",
      "this table, each column represents the performance of differ-\n",
      "ent algorithms adapting the baseline detector to one specific\n",
      "generator. The final column calculates the average AUC and\n",
      "standard deviation over all generators.\n",
      "\n",
      "This experiment’s results in Tab. 2 show that our method\n",
      "achieves the best detection performance across all possible\n",
      "new generators with an average AUC of 0.99. Compared\n",
      "to the second best method, UDIL, whose an average AUC\n",
      "is 0.97, we obtained a significant relative error reduction\n",
      "of 66%. This result shows that our approach is better at\n",
      "adapting to any one new generator than other approaches.\n",
      "\n",
      "Additionally, we note that other competing methods’\n",
      "performance has standard deviations of about 3 to 10 times\n",
      "larger than ours. This result can be further examined\n",
      "by looking at certain occasions when the baseline detec-\n",
      "tor experiences a significant performance drop detecting\n",
      "a specific unseen generator. When this happens, other\n",
      "approaches also tend to exhibit a similar drop in perfor-\n",
      "mance. For example, all competing methods have a signif-\n",
      "\n",
      "icant drop adapting to Latent Diffusion (LD) and BigGAN\n",
      "(BG). Similarly, LWF experiences a performance drop on\n",
      "DLM, DER++, MT-SC and MT-MC on SD1.1, SD1.4 &\n",
      "SD2.1, and ICaRL on SD1.1 & SD1.4. This result is not\n",
      "surprising, however, because the baseline detector, which\n",
      "was trained on GAN, had to be adapted to detect diffusion\n",
      "models’ images. This aligns with findings in prior work,\n",
      "which showed that synthetic image detectors “cannot reli-\n",
      "ably detect images that present artifacts significantly differ-\n",
      "ent from those seen during training.” [13]\n",
      "\n",
      "In contrast, our approach displays strong and stable per-\n",
      "formance irrespective of which generator were added. This\n",
      "is likely because our approach does not need to rely heavily\n",
      "on learning a single embedding space to capture traces of\n",
      "both existing and the new generators.\n",
      "\n",
      "5.3. Adapting to Multiple Emerging Generators\n",
      "\n",
      "In this experiment, we tested E3’s ability to adapt to mul-\n",
      "tiple sequentially emerging generators. We conducted this\n",
      "evaluation to mimic real world scenarios in which a detec-\n",
      "tor needs to be able to adapt to detect synthetic images from\n",
      "each and every newly emerging generator. We then com-\n",
      "pared our method’s performance to other dedicated contin-\n",
      "ual learning techniques. After adapting to a new generator,\n",
      "we measured E3’s and competing methods’ performance in\n",
      "terms of average AUC over the current and the previous\n",
      "generators. We repeated this process until we exhausted all\n",
      "19 emerging generators in our dataset.\n",
      "\n",
      "We present the results of this experiment in Tab. 3. These\n",
      "results show that our approach achieved the best perfor-\n",
      "mance irrespective of the number of new generators added.\n",
      "Furthermore, we observe that after continually adapting to\n",
      "19 different generators, our method obtained an average\n",
      "AUC of 0.97, a reduction of only 0.02 when compared to\n",
      "the initial performance of 0.99 average AUC. This result\n",
      "is a significant improvement over the second best method,\n",
      "UDIL, with 0.93 average AUC. Additionally, it can be seen\n",
      "\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "\n",
      "436\n",
      "\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "\n",
      "447\n",
      "\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "\n",
      "6\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "Method\n",
      "\n",
      "Baseline\n",
      "\n",
      "Fine-Tune\n",
      "\n",
      "LWF [38]\n",
      "\n",
      "ER [53]\n",
      "\n",
      "DER++ [10]\n",
      "\n",
      "UDIL [56]\n",
      "\n",
      "ICaRL [52]\n",
      "\n",
      "MT-SC [42]\n",
      "\n",
      "MT-MC [42]\n",
      "\n",
      "Ours\n",
      "\n",
      "Synthetic Image Generators\n",
      "\n",
      "SD1.4 GLD MJ\n",
      "\n",
      "DLM TT\n",
      "\n",
      "SD2.1 CIPS\n",
      "\n",
      "BG VQD DIG SG3\n",
      "\n",
      "GF\n",
      "\n",
      "DL2\n",
      "\n",
      "LD\n",
      "\n",
      "EG3\n",
      "\n",
      "PG\n",
      "\n",
      "SD1 DDG DDP\n",
      "\n",
      "0.89\n",
      "\n",
      "0.65\n",
      "\n",
      "0.90\n",
      "\n",
      "0.94\n",
      "\n",
      "0.96\n",
      "\n",
      "0.95\n",
      "\n",
      "0.91\n",
      "\n",
      "0.87\n",
      "\n",
      "0.92\n",
      "\n",
      "0.99\n",
      "\n",
      "0.91\n",
      "\n",
      "0.79\n",
      "\n",
      "0.91\n",
      "\n",
      "0.96\n",
      "\n",
      "0.96\n",
      "\n",
      "0.97\n",
      "\n",
      "0.93\n",
      "\n",
      "0.85\n",
      "\n",
      "0.96\n",
      "\n",
      "0.99\n",
      "\n",
      "0.86\n",
      "\n",
      "0.73\n",
      "\n",
      "0.89\n",
      "\n",
      "0.96\n",
      "\n",
      "0.92\n",
      "\n",
      "0.96\n",
      "\n",
      "0.92\n",
      "\n",
      "0.77\n",
      "\n",
      "0.94\n",
      "\n",
      "0.99\n",
      "\n",
      "0.83\n",
      "\n",
      "0.81\n",
      "\n",
      "0.89\n",
      "\n",
      "0.96\n",
      "\n",
      "0.91\n",
      "\n",
      "0.96\n",
      "\n",
      "0.91\n",
      "\n",
      "0.76\n",
      "\n",
      "0.94\n",
      "\n",
      "0.99\n",
      "\n",
      "0.80\n",
      "\n",
      "0.83\n",
      "\n",
      "0.88\n",
      "\n",
      "0.95\n",
      "\n",
      "0.88\n",
      "\n",
      "0.96\n",
      "\n",
      "0.90\n",
      "\n",
      "0.75\n",
      "\n",
      "0.93\n",
      "\n",
      "0.99\n",
      "\n",
      "0.79\n",
      "\n",
      "0.83\n",
      "\n",
      "0.89\n",
      "\n",
      "0.95\n",
      "\n",
      "0.89\n",
      "\n",
      "0.95\n",
      "\n",
      "0.92\n",
      "\n",
      "0.74\n",
      "\n",
      "0.91\n",
      "\n",
      "0.98\n",
      "\n",
      "0.76\n",
      "\n",
      "0.80\n",
      "\n",
      "0.86\n",
      "\n",
      "0.95\n",
      "\n",
      "0.86\n",
      "\n",
      "0.96\n",
      "\n",
      "0.91\n",
      "\n",
      "0.76\n",
      "\n",
      "0.92\n",
      "\n",
      "0.98\n",
      "\n",
      "0.73\n",
      "\n",
      "0.84\n",
      "\n",
      "0.85\n",
      "\n",
      "0.91\n",
      "\n",
      "0.84\n",
      "\n",
      "0.93\n",
      "\n",
      "0.89\n",
      "\n",
      "0.69\n",
      "\n",
      "0.88\n",
      "\n",
      "0.98\n",
      "\n",
      "0.73\n",
      "\n",
      "0.82\n",
      "\n",
      "0.85\n",
      "\n",
      "0.93\n",
      "\n",
      "0.83\n",
      "\n",
      "0.94\n",
      "\n",
      "0.90\n",
      "\n",
      "0.73\n",
      "\n",
      "0.89\n",
      "\n",
      "0.98\n",
      "\n",
      "0.71\n",
      "\n",
      "0.82\n",
      "\n",
      "0.85\n",
      "\n",
      "0.93\n",
      "\n",
      "0.82\n",
      "\n",
      "0.94\n",
      "\n",
      "0.91\n",
      "\n",
      "0.74\n",
      "\n",
      "0.90\n",
      "\n",
      "0.98\n",
      "\n",
      "0.73\n",
      "\n",
      "0.85\n",
      "\n",
      "0.82\n",
      "\n",
      "0.94\n",
      "\n",
      "0.82\n",
      "\n",
      "0.94\n",
      "\n",
      "0.90\n",
      "\n",
      "0.74\n",
      "\n",
      "0.91\n",
      "\n",
      "0.97\n",
      "\n",
      "0.71\n",
      "\n",
      "0.85\n",
      "\n",
      "0.82\n",
      "\n",
      "0.94\n",
      "\n",
      "0.82\n",
      "\n",
      "0.94\n",
      "\n",
      "0.91\n",
      "\n",
      "0.73\n",
      "\n",
      "0.92\n",
      "\n",
      "0.97\n",
      "\n",
      "0.73\n",
      "\n",
      "0.73\n",
      "\n",
      "0.72\n",
      "\n",
      "0.93\n",
      "\n",
      "0.79\n",
      "\n",
      "0.94\n",
      "\n",
      "0.91\n",
      "\n",
      "0.74\n",
      "\n",
      "0.91\n",
      "\n",
      "0.97\n",
      "\n",
      "0.72\n",
      "\n",
      "0.79\n",
      "\n",
      "0.80\n",
      "\n",
      "0.91\n",
      "\n",
      "0.81\n",
      "\n",
      "0.93\n",
      "\n",
      "0.87\n",
      "\n",
      "0.70\n",
      "\n",
      "0.86\n",
      "\n",
      "0.97\n",
      "\n",
      "0.74\n",
      "\n",
      "0.81\n",
      "\n",
      "0.82\n",
      "\n",
      "0.93\n",
      "\n",
      "0.82\n",
      "\n",
      "0.94\n",
      "\n",
      "0.90\n",
      "\n",
      "0.69\n",
      "\n",
      "0.90\n",
      "\n",
      "0.97\n",
      "\n",
      "0.74\n",
      "\n",
      "0.70\n",
      "\n",
      "0.81\n",
      "\n",
      "0.90\n",
      "\n",
      "0.82\n",
      "\n",
      "0.92\n",
      "\n",
      "0.89\n",
      "\n",
      "0.70\n",
      "\n",
      "0.87\n",
      "\n",
      "0.97\n",
      "\n",
      "0.74\n",
      "\n",
      "0.76\n",
      "\n",
      "0.80\n",
      "\n",
      "0.90\n",
      "\n",
      "0.82\n",
      "\n",
      "0.92\n",
      "\n",
      "0.90\n",
      "\n",
      "0.70\n",
      "\n",
      "0.87\n",
      "\n",
      "0.97\n",
      "\n",
      "0.72\n",
      "\n",
      "0.83\n",
      "\n",
      "0.83\n",
      "\n",
      "0.93\n",
      "\n",
      "0.82\n",
      "\n",
      "0.93\n",
      "\n",
      "0.90\n",
      "\n",
      "0.76\n",
      "\n",
      "0.91\n",
      "\n",
      "0.97\n",
      "\n",
      "0.71\n",
      "\n",
      "0.74\n",
      "\n",
      "0.79\n",
      "\n",
      "0.93\n",
      "\n",
      "0.80\n",
      "\n",
      "0.93\n",
      "\n",
      "0.90\n",
      "\n",
      "0.74\n",
      "\n",
      "0.90\n",
      "\n",
      "0.97\n",
      "\n",
      "Table 3. Detection performance of each method when sequentially adapting to a series of new generators. Note that the “baseline” detector\n",
      "has only seen data from the baseline detector dataset and all approaches started from the same baseline detector network. Performance\n",
      "numbers are measured in AUC.\n",
      "\n",
      "Architecture\n",
      "\n",
      "Accuracy\n",
      "\n",
      "AUC\n",
      "\n",
      "Ours UDIL Ours UDIL\n",
      "\n",
      "MISLnet [6]\n",
      "\n",
      "0.922\n",
      "\n",
      "ResNet-50 [20]\n",
      "\n",
      "0.901\n",
      "\n",
      "DenseNet [23]\n",
      "\n",
      "SR-Net [7]\n",
      "\n",
      "0.810\n",
      "\n",
      "0.971\n",
      "\n",
      "0.860\n",
      "\n",
      "0.890\n",
      "\n",
      "0.800\n",
      "\n",
      "0.964\n",
      "\n",
      "0.970\n",
      "\n",
      "0.817\n",
      "\n",
      "0.890\n",
      "\n",
      "0.996\n",
      "\n",
      "0.932\n",
      "\n",
      "0.790\n",
      "\n",
      "0.869\n",
      "\n",
      "0.994\n",
      "\n",
      "AUC\n",
      "RER\n",
      "\n",
      "55.9%\n",
      "\n",
      "12.9%\n",
      "\n",
      "16.0%\n",
      "\n",
      "33.3%\n",
      "\n",
      "Average\n",
      "\n",
      "0.901\n",
      "\n",
      "0.878\n",
      "\n",
      "0.918\n",
      "\n",
      "0.896\n",
      "\n",
      "21.2%\n",
      "\n",
      "Table 4. The performance of our approach versus UDIL using\n",
      "different basline architectures.\n",
      "\n",
      "Figure 4. Our method shows slight performance decline when\n",
      "adapting to 19 generators with training image counts reduced from\n",
      "500 to 50 per generator.\n",
      "\n",
      "from Tab. 3 that as the number of new generators introduced\n",
      "increases, the more degraded the performance of some com-\n",
      "peting approaches becomes (i.e. LWF, DER++, MT-SC). In\n",
      "contrast, our method retained strong performance through-\n",
      "out our experiment, with minimal reduction in performance.\n",
      "Additionally, we note that other competing methods all\n",
      "experience difficulties detecting new synthetic images from\n",
      "some particular generator. For example, LWF & ER ex-\n",
      "perienced significant performance drop when adapting to\n",
      "DALL-E 2 (DL2), similarly, UDIL’s performance dropped\n",
      "adapting to BigGAN (BG), and ICaRL, MT-SC, MT-MC\n",
      "have substantial difficulties adapting to Latent Diffusion\n",
      "(LD). However, our proposed approach does not experience\n",
      "such issues. This suggests that by fusing the embedding\n",
      "spaces of all past and current expert embedders, we can\n",
      "gracefully adapt to any new generator while avoid having\n",
      "significant performance degradation as the number of gen-\n",
      "erator being added increases.\n",
      "\n",
      "5.4. Effects of New Generator’s Training Data Size\n",
      "\n",
      "In this experiment, we examined the impact of different\n",
      "training data size from each new generator on our proposed\n",
      "\n",
      "457\n",
      "458\n",
      "\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "\n",
      "475\n",
      "\n",
      "476\n",
      "477\n",
      "\n",
      "approach. This is important because in real world scenarios,\n",
      "the amount of available data from a new generator is often\n",
      "limited, especially if such generator is a proprietary prod-\n",
      "uct. Therefore, to conduct this experiment, we repeated the\n",
      "experiment in Sec 5.3, with different numbers of available\n",
      "data ranging from 50 to 500 data points. We then reported\n",
      "the average AUC after sequentially adding all 19 generators\n",
      "to our dataset.\n",
      "\n",
      "The results of this experiment are depicted in Fig. 4.\n",
      "These results show that our method’s performance experi-\n",
      "enced minimal reduction in detection performance as the\n",
      "number of available training images decreased. Specifi-\n",
      "cally, we received a very slight drop in performance (0.97\n",
      "to 0.95 average AUC) while having 5 times less training\n",
      "data, and a small drop in performance (0.97 to 0.94 average\n",
      "AUC) with 10 times less data. Notably, our performance\n",
      "of 0.94 average AUC is still an improvement over UDIL\n",
      "(0.93 AUC), whose training data size is 10 times larger than\n",
      "ours (see Sec. 5.3). This suggests that our approach not\n",
      "only achieves the highest performance in adapting to de-\n",
      "tect synthetic images from new generators but also requires\n",
      "significantly less data than other methods to attain strong\n",
      "performance.\n",
      "\n",
      "478\n",
      "\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "\n",
      "484\n",
      "485\n",
      "\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "\n",
      "7\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "501\n",
      "\n",
      "502\n",
      "\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "\n",
      "526\n",
      "\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "\n",
      "548\n",
      "\n",
      "549\n",
      "550\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "5.5. Effects of Different Detector Architectures\n",
      "\n",
      "In this experiment, we examined the effects of different\n",
      "network architectures for the baseline detector. This is to\n",
      "demonstrate the generality of our approach over the di-\n",
      "verse set of detection algorithms in the wild. To do this,\n",
      "we repeated the experiment in Sec. 5.3 with three addi-\n",
      "tional network architectures: ResNet-50 [20], SR-Net [7],\n",
      "and DenseNet [23]. These networks are widely used in\n",
      "prior work for synthetic image detection and other forensic\n",
      "tasks [32, 36, 37, 57, 66, 70]. We then measured accuracy\n",
      "and AUC after adapting to all 19 generators in our dataset.\n",
      "We note that due to space limitation, we will only compare\n",
      "to UDIL, the best performing method in both experiments\n",
      "in Secs. 5.2 and 5.3.\n",
      "\n",
      "We present this experiment’s results in Tab. 4. These re-\n",
      "sults show that our proposed approach achieves the best per-\n",
      "formance irrespective of the underlying detector architec-\n",
      "ture. In particular, we notably outperform UDIL in both ac-\n",
      "curacy and AUC when employing MISLnet, ResNet-50, or\n",
      "DenseNet as the network architectures for the baseline de-\n",
      "tector. Across all architectures, our performance is a 21.2%\n",
      "relative error reduction when compared to UDIL. These re-\n",
      "sults suggest that our proposed method is invariant to dif-\n",
      "ferent architecture designs and can be applied in a general\n",
      "manner to most, if not all, synthetic image detectors.\n",
      "\n",
      "6. Discussion\n",
      "\n",
      "Our method’s strong performance can be attributed to its\n",
      "utilization of dedicated embedders for each new generator.\n",
      "These embeddings are specifically learned to capture foren-\n",
      "sic traces left by their corresponding generators. As a result,\n",
      "our algorithm does not rely solely on a single embedding\n",
      "space, but rather on the union of all embedding spaces from\n",
      "every expert embedder.\n",
      "\n",
      "However, our approach does come at a cost of increased\n",
      "network size. Despite this, in many cases, the cost of mis-\n",
      "detecting AI-generated images, or false-alarming real im-\n",
      "ages as synthetic, is worth the increase in network param-\n",
      "eters. Additionally, it is important to note that after our\n",
      "method adapted the baseline detector (based on MISLnet)\n",
      "to 19 different generators, the total number of parameters\n",
      "in our network is 27.8M, a comparable number to a sin-\n",
      "gle ResNet-50 model with 23.5M parameters. Furthermore,\n",
      "25.1M of these parameters are frozen because they come\n",
      "from the ensemble of embedders. In reality, only about 4M\n",
      "parameters in our network are trainable. For perspective, af-\n",
      "ter adding 100 new generators, the trainable portion of our\n",
      "network is only 13.6M parameters.\n",
      "\n",
      "7. Ablation Results\n",
      "\n",
      "We conducted an ablation study to assess the impact of dif-\n",
      "ferent design choices of our Expert Knowledge Fusion Net-\n",
      "\n",
      "Setup\n",
      "\n",
      "Proposed\n",
      "\n",
      "Accuracy\n",
      "\n",
      "0.93\n",
      "\n",
      "AUC\n",
      "\n",
      "0.97\n",
      "\n",
      "Majority Voting\n",
      "\n",
      "0.50 (-0.43)\n",
      "\n",
      "0.90 (-0.07)\n",
      "\n",
      "Knowledge Fusion w/ MLP only\n",
      "\n",
      "0.91 (-0.02)\n",
      "\n",
      "0.96 (-0.01)\n",
      "\n",
      "No Transformer Weighting\n",
      "\n",
      "0.88 (-0.05)\n",
      "\n",
      "0.96 (-0.01)\n",
      "\n",
      "5 Transformer Layers\n",
      "\n",
      "0.91 (-0.02)\n",
      "\n",
      "0.97 (-0.00)\n",
      "\n",
      "10 Transformer Layers\n",
      "\n",
      "0.91 (-0.02)\n",
      "\n",
      "0.96 (-0.01)\n",
      "\n",
      "Table 5. Ablation study of the different design choices of the Ex-\n",
      "pert Knowledge Fusion Network.\n",
      "\n",
      "work (EKFN). We present these results in Table 5.\n",
      "\n",
      "Majority Voting.\n",
      "In this setup, we fused the knowledge\n",
      "from each expert embedder using a majority voting strat-\n",
      "egy. This approach involves each expert embedder produc-\n",
      "ing their own decisions about the input image and the final\n",
      "detection score is decided using a majority vote. As shown\n",
      "in Tab. 5, this approach resulted in a significant drop in both\n",
      "accuracy and AUC. We note that the reason why this ver-\n",
      "sion has an AUC of 0.90 with an accuracy of 0.50 is be-\n",
      "cause it produces an uncalibrated decision score, a similar\n",
      "phenomenon observed in Corvi et al. [13].\n",
      "\n",
      "Knowledge Fusion with MLP Only. In this experiment,\n",
      "we removed the transformer from the EKFN and evaluated\n",
      "the performance of our approach. Tab. 5 shows that this ver-\n",
      "sion experience a performance reduction compared to our\n",
      "proposed method. Hence, the transformer is important to\n",
      "obtain strong performance.\n",
      "\n",
      "No Transformer Weighting. In this experiment setup, we\n",
      "discarded the weighting between the transformer output and\n",
      "its input. Results in Tab. 5 show that this version achieves\n",
      "worse performance than our proposed method. Hence, this\n",
      "integration approach is important for our method.\n",
      "\n",
      "Number of Transformer Layers. We tested our ap-\n",
      "proach with the transformer made using only 5 or 10 layers.\n",
      "Tab. 5’s results show that this is sub-optimal to our approach\n",
      "in terms of AUC and accuracy.\n",
      "\n",
      "8. Conclusion\n",
      "\n",
      "This paper introduces E3, a novel approach for effec-\n",
      "tively updating synthetic image detectors to accurately de-\n",
      "tect newly emerging generators with minimal training data.\n",
      "By developing expert embedders tailored to capture traces\n",
      "from each new target generator, our method demonstrates\n",
      "strong adaptability. The proposed expert knowledge fusion\n",
      "network analyzes forensic evidence from all experts, facil-\n",
      "itating precise detection decisions. Through extensive ex-\n",
      "perimentation, E3 consistently outperforms competing con-\n",
      "tinual learning approaches, even across various detector ar-\n",
      "chitectures and with limited data from new generators.\n",
      "\n",
      "551\n",
      "\n",
      "552\n",
      "553\n",
      "554\n",
      "\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "\n",
      "560\n",
      "561\n",
      "\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "\n",
      "568\n",
      "\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "\n",
      "577\n",
      "\n",
      "578\n",
      "\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "\n",
      "8\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "589\n",
      "\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "References\n",
      "\n",
      "[1] Midjourney. https://www.midjourney.com/. Ac-\n",
      "\n",
      "cessed: March 19, 2024. 2, 5\n",
      "\n",
      "[2] Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb\n",
      "Image\n",
      "Sterkin, Victor Lempitsky, and Denis Korzhenkov.\n",
      "generators with conditionally-independent pixel synthesis.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pages 14278–14287, 2021.\n",
      "5\n",
      "\n",
      "[3] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¨onlieb,\n",
      "and Christian Etmann. Conditional image generation with\n",
      "score-based diffusion models, 2021. 2\n",
      "\n",
      "[4] Belhassen Bayar and Matthew C Stamm. A deep learning\n",
      "approach to universal image manipulation detection using\n",
      "a new convolutional layer. In Proceedings of the 4th ACM\n",
      "workshop on information hiding and multimedia security,\n",
      "pages 5–10, 2016. 5\n",
      "\n",
      "[5] Belhassen Bayar and Matthew C. Stamm. On the robustness\n",
      "of constrained convolutional neural networks to jpeg post-\n",
      "In ICASSP,\n",
      "compression for image resampling detection.\n",
      "pages 2152–2156, 2017. 5\n",
      "\n",
      "[6] Belhassen Bayar and Matthew C. Stamm. Constrained con-\n",
      "volutional neural networks: A new approach towards general\n",
      "purpose image manipulation detection. IEEE Transactions\n",
      "on Information Forensics and Security, 13(11):2691–2706,\n",
      "2018. 5, 7\n",
      "\n",
      "[7] Mehdi Boroumand, Mo Chen, and Jessica Fridrich. Deep\n",
      "IEEE\n",
      "residual network for steganalysis of digital images.\n",
      "Transactions on Information Forensics and Security, 14(5):\n",
      "1181–1193, 2019. 7, 8\n",
      "\n",
      "[8] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\n",
      "scale gan training for high fidelity natural image synthesis.\n",
      "arXiv preprint arXiv:1809.11096, 2018. 2, 5\n",
      "\n",
      "[9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\n",
      "Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\n",
      "man, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\n",
      "Ramesh. Video generation models as world simulators.\n",
      "2024. 3\n",
      "\n",
      "[10] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide\n",
      "Abati, and Simone Calderara. Dark experience for gen-\n",
      "eral continual learning: a strong, simple baseline. Advances\n",
      "in neural information processing systems, 33:15920–15930,\n",
      "2020. 2, 6, 7\n",
      "\n",
      "[11] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\n",
      "Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\n",
      "Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\n",
      "geometry-aware 3d generative adversarial networks. In Pro-\n",
      "ceedings of the IEEE/CVF conference on computer vision\n",
      "and pattern recognition, pages 16123–16133, 2022. 5\n",
      "[12] Chen Chen, Xinwei Zhao, and Matthew C. Stamm. Mislgan:\n",
      "An anti-forensic camera model falsification framework using\n",
      "a generative adversarial network. In ICIP, pages 535–539,\n",
      "2018. 5\n",
      "\n",
      "[13] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Gio-\n",
      "vanni Poggi, Koki Nagano, and Luisa Verdoliva. On the\n",
      "detection of synthetic images generated by diffusion mod-\n",
      "els. In ICASSP 2023-2023 IEEE International Conference\n",
      "\n",
      "on Acoustics, Speech and Signal Processing (ICASSP), pages\n",
      "1–5. IEEE, 2023. 2, 3, 6, 8\n",
      "\n",
      "[14] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah,\n",
      "Tanishq Abraham, Ph´uc Lˆe Khac, Luke Melas, and Ritobrata\n",
      "Ghosh. Dall·e mini, 2021. 5\n",
      "\n",
      "[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models\n",
      "beat gans on image synthesis. In Advances in Neural Infor-\n",
      "mation Processing Systems, pages 8780–8794. Curran Asso-\n",
      "ciates, Inc., 2021. 2\n",
      "\n",
      "[16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\n",
      "In Pro-\n",
      "transformers for high-resolution image synthesis.\n",
      "ceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition (CVPR), pages 12873–12883, 2021.\n",
      "5\n",
      "\n",
      "[17] Shengbang Fang, Tai D Nguyen, and Matthew c Stamm.\n",
      "Open set synthetic image source attribution. In 34th British\n",
      "Machine Vision Conference 2023, BMVC 2023, Aberdeen,\n",
      "UK, November 20-24, 2023. BMVA, 2023. 2, 5\n",
      "\n",
      "[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\n",
      "Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\n",
      "Yoshua Bengio. Generative adversarial nets. Advances in\n",
      "neural information processing systems, 27, 2014. 2\n",
      "\n",
      "[19] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\n",
      "Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\n",
      "tor quantized diffusion model for text-to-image synthesis. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vi-\n",
      "sion and Pattern Recognition, pages 10696–10706, 2022. 5\n",
      "[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "Deep residual learning for image recognition. In Proceed-\n",
      "ings of the IEEE conference on computer vision and pattern\n",
      "recognition, pages 770–778, 2016. 7, 8\n",
      "\n",
      "[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\n",
      "fusion probabilistic models. Advances in neural information\n",
      "processing systems, 33:6840–6851, 2020. 2, 5\n",
      "\n",
      "[22] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,\n",
      "Mohammad Norouzi, and Tim Salimans. Cascaded diffu-\n",
      "sion models for high fidelity image generation. Journal of\n",
      "Machine Learning Research, 23(47):1–33, 2022. 2\n",
      "\n",
      "[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\n",
      "ian Q Weinberger. Densely connected convolutional net-\n",
      "works. In Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition, pages 4700–4708, 2017. 7, 8\n",
      "[24] Drew A Hudson and Larry Zitnick. Generative adversar-\n",
      "In Proceedings of the 38th International\n",
      "ial transformers.\n",
      "Conference on Machine Learning, pages 4487–4499. PMLR,\n",
      "2021. 5\n",
      "\n",
      "[25] Nils Hulzebosch, Sarah Ibrahimi, and Marcel Worring. De-\n",
      "tecting cnn-generated facial images in real-world scenarios.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR) Workshops, 2020. 2\n",
      "[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\n",
      "Efros.\n",
      "Image-to-image translation with conditional adver-\n",
      "sarial networks. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition, pages 1125–1134,\n",
      "2017. 2\n",
      "\n",
      "[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\n",
      "Progressive growing of GANs for improved quality, stabil-\n",
      "\n",
      "646\n",
      "647\n",
      "\n",
      "648\n",
      "649\n",
      "650\n",
      "\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "\n",
      "677\n",
      "678\n",
      "679\n",
      "\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "\n",
      "701\n",
      "702\n",
      "\n",
      "9\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "703\n",
      "704\n",
      "\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "\n",
      "734\n",
      "735\n",
      "\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "\n",
      "744\n",
      "745\n",
      "746\n",
      "\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "\n",
      "753\n",
      "754\n",
      "755\n",
      "\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "ity, and variation. In International Conference on Learning\n",
      "Representations, 2018. 5\n",
      "\n",
      "[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "generator architecture for generative adversarial networks.\n",
      "In Proceedings of the IEEE/CVF conference on computer vi-\n",
      "sion and pattern recognition, pages 4401–4410, 2019. 2\n",
      "[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based\n",
      "generator architecture for generative adversarial networks.\n",
      "In Proceedings of the IEEE/CVF conference on computer vi-\n",
      "sion and pattern recognition, pages 4401–4410, 2019. 5\n",
      "[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\n",
      "Jaakko Lehtinen, and Timo Aila. Analyzing and improv-\n",
      "In Proceedings of\n",
      "ing the image quality of stylegan.\n",
      "the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, pages 8110–8119, 2020. 5\n",
      "\n",
      "[31] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,\n",
      "Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\n",
      "generative adversarial networks. Advances in neural infor-\n",
      "mation processing systems, 34:852–863, 2021. 5\n",
      "\n",
      "[32] Minha Kim, Shahroz Tariq, and Simon S Woo. Cored: Gen-\n",
      "eralizing fake media detection with continual representation\n",
      "using distillation. In Proceedings of the 29th ACM Interna-\n",
      "tional Conference on Multimedia, pages 337–346, 2021. 8\n",
      "\n",
      "[33] Minha Kim, Shahroz Tariq, and Simon S Woo. Cored: Gen-\n",
      "eralizing fake media detection with continual representation\n",
      "using distillation. In Proceedings of the 29th ACM Interna-\n",
      "tional Conference on Multimedia, pages 337–346, 2021. 2\n",
      "\n",
      "[34] Diederik Kingma and Jimmy Ba. Adam: A method for\n",
      "In International Conference on\n",
      "stochastic optimization.\n",
      "Learning Representations (ICLR), San Diega, CA, USA,\n",
      "2015. 5\n",
      "\n",
      "[35] Diederik P Kingma and Max Welling. Auto-encoding varia-\n",
      "tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n",
      "[36] Sangyup Lee, Shahroz Tariq, Youjin Shin, and Simon S Woo.\n",
      "Detecting handcrafted facial image manipulations and gan-\n",
      "generated facial images using shallow-fakefacenet. Applied\n",
      "soft computing, 105:107256, 2021. 8\n",
      "\n",
      "[37] Weichuang Li, Peisong He, Haoliang Li, Hongxia Wang, and\n",
      "Ruimei Zhang. Detection of gan-generated images by esti-\n",
      "mating artifact similarity. IEEE Signal Processing Letters,\n",
      "29:862–866, 2022. 8\n",
      "\n",
      "[38] Zhizhong Li and Derek Hoiem. Learning without forgetting.\n",
      "IEEE transactions on pattern analysis and machine intelli-\n",
      "gence, 40(12):2935–2947, 2017. 1, 2, 6, 7\n",
      "\n",
      "[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\n",
      "Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\n",
      "Zitnick. Microsoft coco: Common objects in context.\n",
      "In\n",
      "Computer Vision–ECCV 2014: 13th European Conference,\n",
      "Zurich, Switzerland, September 6-12, 2014, Proceedings,\n",
      "Part V 13, pages 740–755. Springer, 2014. 5\n",
      "\n",
      "[40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\n",
      "Large-scale celebfaces attributes (celeba) dataset. Retrieved\n",
      "August, 15(2018):11, 2018. 5\n",
      "\n",
      "[41] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and\n",
      "In\n",
      "Giovanni Poggi. Do gans leave artificial fingerprints?\n",
      "2019 IEEE conference on multimedia information process-\n",
      "ing and retrieval (MIPR), pages 506–511. IEEE, 2019. 2\n",
      "\n",
      "[42] Francesco Marra, Cristiano Saltori, Giulia Boato, and Luisa\n",
      "Verdoliva. Incremental learning for the detection and clas-\n",
      "In 2019 IEEE inter-\n",
      "sification of gan-generated images.\n",
      "national workshop on information forensics and security\n",
      "(WIFS), pages 1–6. IEEE, 2019. 2, 6, 7\n",
      "\n",
      "[43] Owen Mayer and Matthew C. Stamm. Exposing fake images\n",
      "IEEE Journal of Selected\n",
      "\n",
      "with forensic similarity graphs.\n",
      "Topics in Signal Processing, 14(5):1049–1064, 2020. 5\n",
      "[44] Owen Mayer, Brian Hosler, and Matthew C Stamm. Open\n",
      "set video camera model verification. In ICASSP 2020-2020\n",
      "IEEE International Conference on Acoustics, Speech and\n",
      "Signal Processing (ICASSP), pages 2962–2966. IEEE, 2020.\n",
      "5\n",
      "\n",
      "[45] James L McClelland, Bruce L McNaughton, and Randall C\n",
      "O’Reilly. Why there are complementary learning systems in\n",
      "the hippocampus and neocortex: insights from the successes\n",
      "and failures of connectionist models of learning and memory.\n",
      "Psychological review, 102(3):419, 1995. 2\n",
      "\n",
      "[46] Michael McCloskey and Neal J Cohen. Catastrophic inter-\n",
      "ference in connectionist networks: The sequential learning\n",
      "problem. In Psychology of learning and motivation, pages\n",
      "109–165. Elsevier, 1989. 2\n",
      "\n",
      "[47] Mehdi Mirza and Simon Osindero. Conditional generative\n",
      "\n",
      "adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2\n",
      "\n",
      "[48] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\n",
      "Pranav Shyam, Pamela Mishkin, Bob Mcgrew,\n",
      "Ilya\n",
      "Sutskever, and Mark Chen. GLIDE: Towards photorealis-\n",
      "tic image generation and editing with text-guided diffusion\n",
      "models. In Proceedings of the 39th International Conference\n",
      "on Machine Learning, pages 16784–16804. PMLR, 2022. 5\n",
      "[49] Md Awsafur Rahman, Bishmoy Paul, Najibul Haque Sarker,\n",
      "Zaber Ibn Abdul Hakim, and Shaikh Anowarul Fattah. Arti-\n",
      "fact: A large-scale dataset with artificial and factual images\n",
      "for generalizable and robust synthetic image detection.\n",
      "In\n",
      "2023 IEEE International Conference on Image Processing\n",
      "(ICIP), pages 2200–2204. IEEE, 2023. 5\n",
      "\n",
      "[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\n",
      "Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\n",
      "Zero-shot text-to-image generation. In International confer-\n",
      "ence on machine learning, pages 8821–8831. Pmlr, 2021. 2\n",
      "[51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\n",
      "and Mark Chen. Hierarchical text-conditional image gener-\n",
      "ation with clip latents. arXiv preprint arXiv:2204.06125, 1\n",
      "(2):3, 2022. 5\n",
      "\n",
      "[52] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\n",
      "Sperl, and Christoph H Lampert. icarl: Incremental classifier\n",
      "and representation learning. In Proceedings of the IEEE con-\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "2001–2010, 2017. 2, 6, 7\n",
      "\n",
      "[53] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,\n",
      "Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn\n",
      "without forgetting by maximizing transfer and minimizing\n",
      "interference. arXiv preprint arXiv:1810.11910, 2018. 1, 6, 7\n",
      "[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n",
      "Patrick Esser, and Bj¨orn Ommer. High-resolution image\n",
      "In Proceedings of\n",
      "synthesis with latent diffusion models.\n",
      "the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, pages 10684–10695, 2022. 2, 5\n",
      "\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "\n",
      "10\n",
      "\n",
      "\fCVPR\n",
      "#0007\n",
      "\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "\n",
      "CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n",
      "\n",
      "CVPR\n",
      "#0007\n",
      "\n",
      "Proceedings of the IEEE/CVF international conference on\n",
      "computer vision, pages 7556–7566, 2019. 2\n",
      "\n",
      "[69] Xu Zhang, Svebor Karaman, and Shih-Fu Chang. Detecting\n",
      "and simulating artifacts in gan fake images. In 2019 IEEE in-\n",
      "ternational workshop on information forensics and security\n",
      "(WIFS), pages 1–6. IEEE, 2019. 2\n",
      "\n",
      "[70] Junjie Zhao, Junfeng Wu, James Msughter Adeke, Sen Qiao,\n",
      "and Jinwei Wang. Detecting high-resolution adversarial im-\n",
      "ages with few-shot deep learning. Remote Sensing, 15(9):\n",
      "2379, 2023. 8\n",
      "\n",
      "[71] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\n",
      "Efros. Unpaired image-to-image translation using cycle-\n",
      "consistent adversarial networks. In Proceedings of the IEEE\n",
      "international conference on computer vision, pages 2223–\n",
      "2232, 2017. 2\n",
      "\n",
      "[72] Mingjian Zhu, Hanting Chen, Qiangyu YAN, Xudong\n",
      "Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu,\n",
      "and Yunhe Wang. Genimage: A million-scale benchmark\n",
      "for detecting ai-generated image. In Advances in Neural In-\n",
      "formation Processing Systems, pages 77771–77782. Curran\n",
      "Associates, Inc., 2023. 2\n",
      "\n",
      "[73] Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, and\n",
      "Cong Yao. Conditional text image generation with diffu-\n",
      "sion models. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR), pages\n",
      "14235–14245, 2023. 2\n",
      "\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "\n",
      "[55] Axel Sauer, Kashyap Chitta, Jens M¨uller, and Andreas\n",
      "Geiger. Projected gans converge faster. In Advances in Neu-\n",
      "ral Information Processing Systems, pages 17480–17492.\n",
      "Curran Associates, Inc., 2021. 5\n",
      "\n",
      "[56] Haizhou Shi and Hao Wang. A unified approach to do-\n",
      "main incremental learning with memory: Theory and algo-\n",
      "rithm. Advances in Neural Information Processing Systems,\n",
      "36, 2024. 1, 2, 6, 7\n",
      "\n",
      "[57] Brijesh Singh, Arijit Sur, and Pinaki Mitra. Steganalysis of\n",
      "IEEE Transac-\n",
      "digital images using deep fractal network.\n",
      "tions on Computational Social Systems, 8(3):599–606, 2021.\n",
      "8\n",
      "\n",
      "[58] Sergey Sinitsa and Ohad Fried. Deep image fingerprint: To-\n",
      "wards low budget synthetic image detection and model lin-\n",
      "eage analysis. In Proceedings of the IEEE/CVF Winter Con-\n",
      "ference on Applications of Computer Vision (WACV), pages\n",
      "4067–4076, 2024. 2, 5\n",
      "\n",
      "[59] Sergey Sinitsa and Ohad Fried. Deep image fingerprint: To-\n",
      "wards low budget synthetic image detection and model lin-\n",
      "eage analysis. In Proceedings of the IEEE/CVF Winter Con-\n",
      "ference on Applications of Computer Vision, pages 4067–\n",
      "4076, 2024. 2\n",
      "\n",
      "[60] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez,\n",
      "Aythami Morales, and Javier Ortega-Garcia. Deepfakes and\n",
      "beyond: A survey of face manipulation and fake detection.\n",
      "Information Fusion, 64:131–148, 2020. 2\n",
      "\n",
      "[61] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\n",
      "Owens, and Alexei A Efros. Cnn-generated images are\n",
      "In Proceedings of\n",
      "surprisingly easy to spot... for now.\n",
      "the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, pages 8695–8704, 2020. 5\n",
      "\n",
      "[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\n",
      "Owens, and Alexei A Efros. Cnn-generated images are\n",
      "In Proceedings of\n",
      "surprisingly easy to spot... for now.\n",
      "the IEEE/CVF conference on computer vision and pattern\n",
      "recognition, pages 8695–8704, 2020. 2\n",
      "\n",
      "[63] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu\n",
      "Chen, and Mingyuan Zhou. Diffusion-gan: Training gans\n",
      "with diffusion. arXiv preprint arXiv:2206.02262, 2022. 5\n",
      "\n",
      "[64] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin\n",
      "Tong. 3d-aware image generation using 2d diffusion mod-\n",
      "els. In Proceedings of the IEEE/CVF International Confer-\n",
      "ence on Computer Vision (ICCV), pages 2383–2393, 2023.\n",
      "2\n",
      "\n",
      "[65] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling\n",
      "the generative learning trilemma with denoising diffusion\n",
      "GANs. In International Conference on Learning Represen-\n",
      "tations (ICLR), 2022. 5\n",
      "\n",
      "[66] Yassine Yousfi, Jan Butora, Eugene Khvedchenya, and Jes-\n",
      "sica Fridrich. Imagenet pre-trained cnns for jpeg steganal-\n",
      "ysis. In 2020 IEEE International Workshop on Information\n",
      "Forensics and Security (WIFS), pages 1–6. IEEE, 2020. 8\n",
      "\n",
      "[67] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\n",
      "Funkhouser, and Jianxiong Xiao. Lsun: Construction of a\n",
      "large-scale image dataset using deep learning with humans\n",
      "in the loop. arXiv preprint arXiv:1506.03365, 2015. 5\n",
      "[68] Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake\n",
      "images to gans: Learning and analyzing gan fingerprints. In\n",
      "\n",
      "11\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "print(text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = text\n",
    "n=512\n",
    "\n",
    "chunks = [x[i:i+n] for i in range(0, len(x), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVPR\\n#0007\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nE3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors to\\nNew Generators Using Limited Data\\n\\nAnonymous CVPR submission\\n\\nPaper ID 0007\\n\\nAbstract\\n\\nAs generative AI progresses rapidly, new synthetic image\\ngenerators continue to emerge at a swift pace. Traditional\\ndetection methods face two main challenges in adapting to\\nthese generators: the forensic traces of synthetic images\\nfrom new techniques ',\n",
       " 'can vastly differ from those learned\\nduring training, and access to data for these new genera-\\ntors is often limited. To address these issues, we introduce\\nthe Ensemble of Expert Embedders (E3), a novel continual\\nlearning framework for updating synthetic image detectors.\\nE3 enables the accurate detection of images from newly\\nemerged generators using minimal training data. Our ap-\\nproach does this by first employing transfer learning to de-\\nvelop a suite of expert embedders, each specializing in the\\nforensic',\n",
       " ' traces of a specific generator. Then, all embed-\\ndings are jointly analyzed by an Expert Knowledge Fusion\\nNetwork to produce accurate and reliable detection deci-\\nsion. Our experiments demonstrate that E3 outperforms\\nexisting continual learning methods, including those devel-\\noped specifically for synthetic image detection.\\n\\n1. Introduction\\n\\nOver the past several years, a number of AI-based tech-\\nniques have been developed to create visually realistic syn-\\nthetic images. While these synthetic image generat',\n",
       " 'ors can\\nbe used for creative or artistic purposes, they can also be\\nused to malicious ones. Specifically, they enable the cre-\\nation of fake images that can be used for misinformation or\\ndisinformation.\\n\\nTo address these challenges, considerable research ef-\\nforts have been directed towards the development of syn-\\nthetic image detection techniques. Prior work has demon-\\nstrated the effectiveness of forensic neural networks in dis-\\ncerning synthetic images by identifying unique traces left\\nbehind by differen',\n",
       " 't image generators. However, a signif-\\nicant limitation of existing detectors arises when they en-\\ncounter images generated by previously unseen or emerging\\n\\n001\\n002\\n003\\n004\\n005\\n\\n006\\n007\\n008\\n009\\n010\\n011\\n\\n012\\n013\\n014\\n015\\n016\\n\\n017\\n018\\n019\\n\\n020\\n\\n021\\n022\\n023\\n024\\n025\\n\\n026\\n027\\n\\n028\\n029\\n030\\n\\n031\\n032\\n033\\n034\\n035\\n\\nFigure 1.\\nIllustration of the Ensemble of Expert Embedders\\n(E3) Framework. This figure highlights our innovative approach,\\nspecifically crafted to improve the performance and adaptability of\\nsynthetic imag',\n",
       " 'e detection in response to emerging generators.\\n\\ntechniques, whose traces differ substantially from those in\\nthe training data.\\n\\nThis poses a critical need for continually updating syn-\\nthetic image detectors to adapt to new generators. How-\\never, this task presents several challenges. Traditional ap-\\nproaches to updating detectors often encounter issues such\\nas catastrophic forgetting and the impracticality of storing\\nand retraining on large datasets [38, 53, 56]. Moreover, lim-\\nited data availability from',\n",
       " ' newly emerging generators, espe-\\ncially those not yet publicly accessible, further complicates\\nthe updating process.\\n\\nIn this paper, we propose a novel approach to address\\nthese challenges. Instead of relying on a single network to\\ncapture all forensic traces, we introduce an Ensemble of Ex-\\npert Embedders (E3) framework. Each expert embedder is\\ncreated through transfer learning and specializes in captur-\\ning traces from a specific image generator. These expert\\nembedders collectively generate a sequence of',\n",
       " ' embeddings,\\nwhich are then analyzed by an Expert Knowledge Fusion\\nNetwork (EKFN). The EKFN leverages information from\\n\\n036\\n037\\n\\n038\\n039\\n\\n040\\n041\\n042\\n043\\n044\\n045\\n\\n046\\n\\n047\\n048\\n049\\n050\\n051\\n\\n052\\n053\\n054\\n055\\n\\n1\\n\\n\\x0cCVPR\\n#0007\\n\\n056\\n057\\n058\\n059\\n060\\n\\n061\\n062\\n\\n063\\n064\\n065\\n066\\n\\n067\\n068\\n069\\n070\\n071\\n072\\n\\n073\\n074\\n075\\n076\\n077\\n\\n078\\n079\\n080\\n\\n081\\n\\n082\\n083\\n\\n084\\n085\\n086\\n087\\n088\\n089\\n\\n090\\n091\\n092\\n093\\n094\\n\\n095\\n096\\n097\\n098\\n099\\n100\\n\\n101\\n\\n102\\n103\\n104\\n105\\n106\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n',\n",
       " '\\nCVPR\\n#0007\\n\\nall expert embedders to accurately perform synthetic im-\\nage detection. Our experimental results demonstrate that\\nthe proposed E3 framework significantly outperforms exist-\\ning continual learning approaches, including those designed\\nspecifically for updating synthetic image detectors, and can\\nperform strongly even with very limited data from new gen-\\nerators. The novel contributions of this paper are:\\n• We propose E3, a new approach that can update synthetic\\nimage detectors to accurately detect',\n",
       " ' newly emerging gen-\\nerators, while requiring only minimal amounts of training\\ndata to be retained in a memory buffer.\\n\\n• We develop a novel approach to create a set of expert em-\\nbedders to accurately capture traces from each new tar-\\nget generator. Each expert can be adapted using a small\\namount of images from its target generator.\\n\\n• We propose an expert knowledge fusion network able to\\nexamine forensic evidence produced by the set of all ex-\\nperts and make accurate detection decision.\\n\\n• We conduct an e',\n",
       " 'xtensive set of experiments that demon-\\nstrate that E3 outperforms competing approaches from\\ncontinual learning, including approaches specifically de-\\nsigned to update synthetic image detectors. Notably, E3\\nconsistently outperforms competitors across various de-\\ntector architectures and excels even with limited data\\nfrom new generators.\\n\\n2. Background\\n\\nSynthetic Image Generators. Considerable effort has been\\ndevoted to developing systems that can visually understand\\nthe world through the process of mimickin',\n",
       " 'g; the first of\\nsuch work is Variational Auto-Encoder by Kingma and\\nWelling [35], which led to the development of Generative\\nAdverserial Networks (GANs) by Goodfellow et al. [18].\\nThis work inspired many other subsequent works [8, 26,\\n28, 47, 71], which continued to improve visual understand-\\ning through enhancing the generation’s quality, diversity,\\nand realism. Notably, the introduction of diffusion model\\nfor image generation by Ho et al. [21] set the stage for the\\nexplosion of research in this area, resu',\n",
       " 'lting in many pop-\\nular generation methods such as: Stable Diffusion [54],\\nDALL·E [50], Midjourney [1], Cascade Diffusion [22],\\netc. [3, 15, 64, 73]. Because the rate of new generation\\nmethods being emerged continues to rapidly increase, this\\npresent a formidable challenge for existing synthetic image\\ndetectors whose capability is often limited to only detecting\\ngenerators seen during training [13, 60].\\n\\nSynthetic Image Traces and Detection. Numerous ef-\\nforts have been undertaken to distinguish synthetic i',\n",
       " 'mages\\nfrom real ones. Marra et al. [41] demonstrated that GAN-\\ngenerated images possess unique “fingerprints” useful for\\ndetection and source attribution. This work showed that in-\\ndividual image generators all left behind identifiable arti-\\n\\nfacts, called forensic traces, that can be used to detect their\\ngenerated images. Subsequent that it is difficult for a de-\\ntector to generalize to forensic traces from a new family of\\ngenerator [13]. To address the rapid release of image gen-\\nerators, a broad spectrum',\n",
       " ' of new synthetic image detectors\\nwere developed [13, 25, 41, 58, 59, 62, 68, 69, 72]. Most\\nnotably, recent open-set approaches [17] were developed to\\ndetect unseen generators by classifying them to belong to\\nan “unknown” class.\\n\\nContinual Learning For Synthetic Image Detection.\\nGiven the distinct forensic traces left by various generator\\nfamilies, it is crucial for image detectors to continually up-\\ndate their knowledge with new generators. Traditional re-\\ntraining is often data inefficient and fine-tuning',\n",
       " ' may often\\nlead to catastrophic forgetting [45, 46]. Hence, a number\\nof methods have been developed to allow the base model\\nto adapt without losing prior knowledge [10, 38, 52, 56].\\nNotably, Marra et al. [42] and Kim et al. [33] have success-\\nfully applied such strategies to synthetic image detection,\\ndemonstrating the feasibility and effectiveness of continual\\nlearning in this domain.\\n\\n3. Problem Formulation\\n\\nWe begin by assuming that an initial synthetic image detec-\\ntor f0 has been created to detect imag',\n",
       " 'es in a set of known\\ngenerators G0. We will refer to this detector as the base-\\nline detector. We further assume that f0 was trained using a\\nlarge baseline training dataset B consisting of both real and\\nsynthetic images made using generators in G0.\\n\\nAfter the baseline detector has been trained, new syn-\\nthetic image generators gk /∈ G0 will continue to emerge.\\nAs previous research has shown, f0 will have a difficult time\\ndetecting these new generators if the forensic traces or “fin-\\ngerprints” left by these',\n",
       " ' generators are substantially different\\nthan those left by the generators in G0 [13]. Because of this,\\nthe synthetic image detector will need to be continually up-\\ndated as new generators emerge.\\n\\nThis presents an important set of problems. Training a\\nnew detector from scratch can be resource intensive and\\nrequires that the baseline dataset B be stored indefinitely.\\nInstead, it is preferable to retain only a small dataset M,\\nknown as the memory buffer, to update the detector. Care\\nmust be taken when updatin',\n",
       " 'g the detector, however, because\\nnaively fine-tuning the detector can lead to catastrophic for-\\ngetting. When this happens, the detector is able to identify\\nimages produced by the new generator, but loses its ability\\nto reliably detect images from previous generators.\\n\\nAdditionally, challenges may arise when access to syn-\\nthetic images generated by the new generator gk is limited.\\nFor instance, a generator may not be available to the general\\npublic, but a small number of examples from the generator\\nmay be ',\n",
       " 'publicly obtainable. An example of this is OpenAI’s\\n\\n107\\n108\\n109\\n110\\n\\n111\\n112\\n113\\n114\\n115\\n\\n116\\n\\n117\\n118\\n119\\n120\\n121\\n122\\n\\n123\\n124\\n125\\n126\\n127\\n\\n128\\n\\n129\\n130\\n\\n131\\n132\\n133\\n134\\n\\n135\\n\\n136\\n137\\n138\\n139\\n140\\n141\\n\\n142\\n\\n143\\n144\\n145\\n146\\n\\n147\\n148\\n149\\n150\\n151\\n\\n152\\n\\n153\\n154\\n155\\n156\\n\\n157\\n\\n2\\n\\n\\x0cCVPR\\n#0007\\n\\n158\\n159\\n160\\n161\\n162\\n\\n163\\n164\\n165\\n166\\n\\n167\\n168\\n169\\n170\\n171\\n172\\n\\n173\\n174\\n175\\n176\\n177\\n178\\n\\n179\\n\\n180\\n181\\n\\n182\\n183\\n184\\n185\\n186\\n187\\n\\n188\\n\\n189\\n190\\n191\\n\\n192\\n193\\n194\\n195\\n196\\n197\\n\\n198\\n\\n199\\n200\\n\\n201\\n202\\n203\\n204\\n205\\n206\\n\\n207\\n208\\n\\n209\\n\\n',\n",
       " 'CVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nSora generator [9]. Currently, access to Sora is restricted,\\nhowever OpenAI has shared a small number of sample out-\\nputs. Alternatively, a party engaged in a misinformation\\ncampaign may have developed a new generator for their\\npurposes. In this case, access to images produced by this\\nnew generator is severely limited by the number of exam-\\nples that have been released into the wild. Training an ef-\\nfective synthetic image',\n",
       " ' detector that can continuously adapt\\nto new generators with limited data is highly non-trivial.\\n\\nTo formalize the problem of updating the detector given\\nthese constraints, we define Mk−1 as the memory buffer\\nwhen the kth new generator is introduced. Mk−1 consists\\nof two subsets: R which contains real images and Sk−1\\nwhich contains images made by each of the previously seen\\ngenerators. The memory buffer has a fixed size |Mℓ|= M ,\\nthat remains constant even as more generators emerge. Ad-\\nditionally, we defin',\n",
       " 'e Dk as the set of images from the new\\ngenerator gk that can be used to update the detector. We\\nassume that |Dk|= N , and that N is significantly smaller\\nthan the number of images in B, i.e. we are allowed a small\\nnumber of images from the new generator.\\n\\n4. Proposed Approach\\n\\nWhile a synthetic image detector f is often thought of as a\\nsingle network, we can conceptually view it as the composi-\\ntion of an embedder ϕ and a classifier h, such that f = h◦ϕ.\\nWhen adopting this view, the embeddings produced by ϕ',\n",
       " '\\ncapture forensic traces left by synthetic image generators,\\nwhile the detector maps these embeddings to detection de-\\nIn practice, lower layers of the network can be\\ncisions.\\nthought of as the embedder, while the final layer or layers\\ncan be thought of as the classifier.\\n\\nWhen a continual learning technique is used to update f ,\\nthis typically involves using a special process that retrains\\nf to detect images from both a new generator and existing\\ngenerators using Mk and Dk. This corresponds to updating\\nϕ s',\n",
       " 'o that it learns an embedding space that is able to jointly\\ncapture forensic traces from previous generators as well as\\nthe new generator. However, since forensic traces from dif-\\nferent generators can be substantially different [13], learn-\\ning an embedding space that successfully does this with a\\nlimited amount of data in Mk and Dk can be challenging.\\nTo overcome this challenge, we propose a new frame-\\nwork to update f called Ensemble of Expert Embedders\\n(E3).\\nIn this framework, we do not attempt to learn',\n",
       " ' a\\nsingle embedding space to capture the traces left by all\\nInstead, we form a set of expert embedders\\ngenerators.\\nΦk = {ϕ0, . . . , ϕk}, where each expert embedder ϕℓ is spe-\\ncialized to capture traces from generator gℓ. When forming\\nan expert embedder ϕk for a new generator gk, we allow it\\nto experience catastrophic forgetting, since other experts in\\nΦk are dedicated to capturing traces from other generators.\\nTo analyze an image, it is passed through all embedders\\n\\nFigure 2. End-to-end architecture workfl',\n",
       " 'ow: An image is first pro-\\ncessed by expert embedders to generate embeddings, which are\\npassed through a transformer and an MLP layer to produce the de-\\ntection decision.\\n\\nin Φk to produce a sequence of embeddings {x0, . . . , xk}.\\nEach embedding xℓ captures evidence that the image was\\ngenerated by gℓ. This set of embeddings is then analyzed\\nusing an Expert Knowledge Fusion Network (EKFN) to\\nproduce a single detection decision. As a result, E3 is able\\nto leverage forensic evidence within the union of every ',\n",
       " 'ex-\\npert’s embedding space, as opposed to relying on a single\\nembedding space to capture all forensic evidence.\\n\\nWe describe the process to form each expert embedder,\\nupdate the memory buffer, and train the expert knowledge\\nfusion network in the following subsections.\\n\\n4.1. Creating A New Expert Embedder\\n\\nLet Gk−1 be the set of currently known synthetic image gen-\\nerators. When a new synthetic image generator gk /∈ Gk−1\\nemerges, we need to update our detector. To do this, we\\nmust first create a new expert e',\n",
       " 'mbedder ϕk to capture traces\\nleft by gk. We accomplish this by adapting the baseline de-\\ntector f0 using transfer learning. An overview of this pro-\\ncess is shown in Fig. 3.\\n\\nWe start by forming a new expert training set Tk of real\\nand synthetic images that we will use to create ϕk. This\\nis done by collecting a set Dk of N synthetic images cre-\\nated by the new generator. Real images are taken from the\\ncurrent memory buffer Mk−1, which contains the subset R\\nconsisting of M/2 real images. As a result, the new',\n",
       " ' expert\\ntraining set is Tk = Dk ∪ R.\\n\\nNext, we use Tk to update f0 to detect images made by\\ngk. Specifically, we form a new detector ˆfk by fine tuning\\nf0 with Tk using the loss function\\n\\n210\\n211\\n212\\n213\\n214\\n\\n215\\n216\\n217\\n218\\n219\\n220\\n\\n221\\n\\n222\\n223\\n224\\n225\\n226\\n227\\n\\n228\\n229\\n230\\n231\\n232\\n233\\n\\n234\\n235\\n236\\n237\\n238\\n\\n(cid:88)\\n\\nLϕ = −\\n\\nIℓ∈Tk\\n\\n|Dk|\\n|Tk|\\n\\ntℓlog( ˆfk(Iℓ))\\n\\n+\\n\\n|R|\\n|Tk|\\n\\n(1 − tℓ)log(1 − ˆfk(Iℓ))\\n\\n(1)\\n\\n239\\n\\n3\\n\\n\\x0cCVPR\\n#0007\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007',\n",
       " '\\n\\nAlgorithm 1 Training Expert Knowledge Fusion Network\\nRequire: Mk−1, Dk, Φk\\n\\nbe the procedure to sample images from\\n\\nD be the procedure to sample images from Dk\\n\\nM\\n\\n// Let ω(k−1)\\nMk−1 uniformly at random\\n// Let ω(k)\\nuniformly at random\\n// Let S be the number of training steps\\nM (Mk−1) (cid:83) ω(k)\\nMk ← ω(k−1)\\nX ← {}\\nfor step = 1, · · · , S do\\n\\nD (Dk)\\n\\nFigure 3. This figure shows the creation of a new expert ϕk when\\na generator gk emerges. The baseline embedder f0 is fine-tuned\\nwith data Dk and real images',\n",
       " ' R from memory buffer Mk−1. The\\nnew classifier ˆfk’s embedder ϕk is then preserved.\\n\\nwhere Iℓ is the ℓ-th image in Tk, and tℓ is the class label of\\nthe image Iℓ where 0 means the image is real and 1 means\\nthe image is synthetic. By doing this, the resulting detector\\nˆfk will be adapted to detect images made by gk. However, it\\nwill likely experience catastrophic forgetting and be poorly\\nsuited to detect generators in Gk−1.\\n\\nAfter obtaining the expert detector ˆfk, we decompose\\nit into its corresponding embed',\n",
       " 'der ϕk and classifier com-\\nponents. The classifier portion is discarded, while ϕk is\\nretained. Finally, ϕk is added to the set of expert embed-\\nders Φk = Φk−1 ∪ {ϕk}. We note Φ0 is initialized as\\nΦ0 = {ϕ0}.\\n\\n4.2. Expert Knowledge Fusion Network\\n\\nAfter adding the new expert embedder ϕk to the set of ex-\\nisting expert embedders, we must update the Expert Knowl-\\nedge Fusion Network (EKFN) ψ so that it can utilize the\\nembeddings produced by ϕk. We do this by first updating\\nthe memory buffer Mk, then use this da',\n",
       " 'ta to train ψ to per-\\nform detection. This process is described below.\\nUpdating The Memory Buffer. When a new generator gk\\nemerges, the current memory buffer Mk−1 does not include\\nimages generated by it. As a result, we need to add images\\nfrom Dk to form the updated memory buffer Mk. How-\\never, since the memory buffer has a fixed size M , we must\\nremove synthetic images made by previous generators in\\nGk−1 to make space for these new images.\\n\\nWe first note that Sk−1, i.e. the subset of Mk−1 that cor-\\nrespond',\n",
       " 's to synthetic images, contains Pk−1 = ⌊M/(2k)⌋\\nimages from each generator in Gk−1. We now need to form\\nan updated set of synthetically generated images Sk that\\ncontains an equal number of images from all generators in\\nGk. To do this, we retain Pk = (k/(k + 1))Pk−1 images\\ncorresponding to each generator in Gk−1 from Sk−1 drawn\\nuniformly at random. These images are added to the new\\n\\n240\\n241\\n242\\n\\n243\\n244\\n245\\n\\n246\\n247\\n248\\n249\\n250\\n\\n251\\n\\n252\\n\\n253\\n254\\n255\\n256\\n\\n257\\n258\\n\\n259\\n260\\n\\n261\\n262\\n263\\n264\\n265\\n\\n266\\n267\\n268\\n\\n2',\n",
       " '69\\n270\\n271\\n272\\n273\\n\\nfor i = 1;\\n\\ni ≤ |Mk|;\\n\\ni += 1 do\\n\\nIi = Mk [i]; Xi ← {}\\nfor j = 1; j ≤ |Φk|; j += 1 do\\n\\nϕj ← Φk [j]\\nXi ← Xi ∪ {ϕj(Ii)}\\n\\nend for\\nX ← X ∪ {Xi}\\n\\nend for\\nL ← Lψ(Mk, X)\\nψ ← BackPropagate(L, ψ)\\n\\n▷ Compute loss with Eqn. 2\\n\\nend for\\nreturn ψ\\n\\nupdated memory buffer’s subset of synthetic images Sk. Fi-\\nnally, we add Pk images drawn uniformly at random from\\nDk to Sk to form a complete set of synthetic images from\\nall generators in Gk for the memory buffer. The updated\\nmemory buffer is now formed as ',\n",
       " 'Mk = Sk ∪ R.\\n\\nTraining The EKFN. Once we have updated the memory\\nbuffer, we use Mk to train the EKFN ψ. To do this, we first\\nuse the set of expert embedders Φk to extract a sequence of\\nembeddings {xℓ\\nk} for each image Iℓ ∈ Mk. Next,\\nwe randomly initialize ψ and train it using the loss function\\n\\n0, . . . , xℓ\\n\\n274\\n275\\n276\\n\\n277\\n278\\n\\n279\\n\\n280\\n281\\n282\\n283\\n\\nLψ = −\\n\\n(cid:88)\\n\\nIℓ∈Mk\\n\\nψ(xℓ\\n\\n0, . . . , xℓ\\n+ (1 − ψ(xℓ\\n\\nk) log tℓ\\n0, . . . , xℓ\\n\\nk)) log(1 − tℓ).\\n\\n(2)\\n\\n284\\n\\nAfter training the EKFN, the synthetic image d',\n",
       " 'etector has\\nbeen completely updated and the network is ready to per-\\nform detection.\\n\\nEKFN Architecture. We can think of each embedder ϕℓ\\nas a mechanism that searches for evidence of traces left\\nby generator gℓ. The resulting embedding xℓ can then be\\nviewed as a measurement of this evidence. When viewed\\nthis way, the purpose of the EKFN is to examine all forms\\nof evidence in the context of one another in order to make\\nan accurate forensic decision. In light of this, we design the\\nEKFN such that it uses a tr',\n",
       " 'ansformer’s self-attention mech-\\nanism to examine the sequence of embeddings in the con-\\ntext of one another.\\n\\nAn overview of our EKFN architecture is shown in\\n\\n285\\n286\\n287\\n\\n288\\n289\\n290\\n291\\n\\n292\\n293\\n294\\n295\\n296\\n297\\n\\n298\\n\\n4\\n\\n\\x0cCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nCVPR\\n#0007\\n\\ne\\nn\\ni\\nl\\ne\\ns\\na\\nB\\n\\ng\\nn\\ni\\ng\\nr\\ne\\nm\\nE\\n&\\nw\\ne\\nN\\n\\nAbbrv. Generator Name\\n\\nAbbrv. Generator Name\\n\\ns SG\\nr\\no\\nt\\na\\nr\\ne\\nn\\ne\\nG\\n\\nSG2\\n\\nProG\\n\\nStyleGAN [29]\\n\\nStyleGAN2 [30]\\n\\nProGAN [27]\\n\\ns\\nr\\no\\nt\\na\\nr\\ne\\nn\\ne\\nG\\n\\nS',\n",
       " 'D1.4\\n\\nStable Diffusion 1.4 [54]\\n\\nGLD\\n\\nGlide [48]\\n\\nMJ\\n\\nMidjourney [1]\\n\\nDLM DALL-E Mini [14]\\n\\nTT\\n\\nTaming Transformers [16]\\n\\nSD2.1\\n\\nStable Diffusion 2.1 [54]\\n\\nCIPS\\n\\nCond.Indep. Pix.Synt. [2]\\n\\nBG\\n\\nBigGAN [8]\\n\\ns\\nr\\no\\nt\\na\\nr\\ne\\nn\\ne\\nG\\ng\\nn\\ni\\ng\\nr\\ne\\nm\\nE\\n&\\nw\\ne\\nN\\n\\nVQD\\n\\nVec. Quant. Diff. [19]\\n\\nDIG\\n\\nSG3\\n\\nGF\\n\\nDL2\\n\\nLD\\n\\nEG3\\n\\nPG\\n\\nDiffusion GAN [63]\\n\\nStyleGAN3 [31]\\n\\nGANformer [24]\\n\\nDALL-E 2 [51]\\n\\nLatent Diffusion [54]\\n\\nEff.Geo. 3D GAN [11]\\n\\nProjected GAN [55]\\n\\nSD1.1\\n\\nStable Diffusion 1.1 [54]\\n\\nDDG\\n\\nDDP\\n\\nDenoise Diff. GAN [65]\\n\\nD',\n",
       " 'enoise Diff. Prob. [21]\\n\\nTable 1. The full names and abbreviations for all synthetic image\\ngenerators used in this paper.\\n\\nFig. 2. The sequence of embeddings is first passed di-\\nrectly to a transformer. Each transformer output is used to\\nweight its corresponding embedding through element-wise\\nmultiplication. The resulting weighted embeddings are then\\npassed to an MLP, which makes a final detection decision.\\nIn practice, we form the MLP using 2 layers and the trans-\\nformer using 20 layers in accordance to th',\n",
       " 'e results of an\\nablation study reported on in Sec. 7. We note that simpler\\nEKFN architectures can be utilized to reduce network com-\\nplexity at the cost of decreased performance.\\n\\n4.3. E3 Framework Summary\\n\\nE3 uses the following steps to update a synthetic image de-\\ntector after the emergence of a new generator:\\n1. Form a new training set using real images from the mem-\\nory buffer and a set of images from the new generator.\\n2. Utilize this training set to create an expert embedder\\nspecifically trained to ca',\n",
       " 'pture forensic traces associated\\nwith the new generator, and incorporate it into the en-\\nsemble of expert embedders.\\n\\n3. Update the memory buffer with the data from the new\\n\\ngenerator.\\n\\n4. Retrain the Expert Knowledge Fusion Network (EKFN)\\n\\nusing the data stored in the memory buffer.\\n\\n5. Experiments\\n\\n5.1. Experimental Setup\\n\\nTo form a diverse set of real\\n\\nBaseline Detector Dataset. We created a baseline dataset\\nin order to train and evaluate the baseline synthetic im-\\nage detectors.\\nimages,\\nwe gathered 72,0',\n",
       " '00 images equally distributed among the\\nCoCo dataset [39], the LSUN dataset [67], and the CelebA\\ndataset [40]. As for the set of synthetic images, we also\\ngathered 72,000 GAN-generated images from the datasets\\nused in prior work [17, 61]. The synthetic images in this set\\nare equally distributed among three GANs: StyleGAN [29],\\nStyleGAN 2 [30], and ProGAN [27]. Additionally, to form\\n\\n299\\n300\\n301\\n302\\n\\n303\\n304\\n305\\n306\\n307\\n\\n308\\n\\n309\\n\\n310\\n311\\n\\n312\\n\\n313\\n314\\n315\\n316\\n317\\n\\n318\\n319\\n320\\n321\\n\\n322\\n\\n323\\n\\n324\\n325\\n326\\n327\\n',\n",
       " '328\\n329\\n\\n330\\n331\\n332\\n333\\n\\n5\\n\\nthe set of images used for training, validation, and testing,\\nwe split the set of real images and the set of synthetic im-\\nages each into their own disjoint subsets. Specifically, we\\nused 132,000 images for training with 12,000 images held\\nout for validation, and 12,000 images for testing.\\n\\nEmerging Generators Dataset. To simulate a constantly\\nevolving environment in which new synthetic image gen-\\nerators rapidly emerge, we created a dataset to train and\\nevaluate our proposed ap',\n",
       " 'proach in a continual learning set-\\nting. To accomplish this, we first selected a diverse set of\\ngenerators, composed of 19 different generation techniques.\\nThese techniques are listed in Tab. 1. All synthetic im-\\nages in this dataset are assembled from publicly available\\ndatasets [17, 49, 58]. Then, from each generator, we gath-\\nered 500 images exclusively used for training and 400 im-\\nages exclusively used for testing.\\n\\nBaseline Detector Training. We obtained a baseline de-\\ntector by training MISLnet [6] ',\n",
       " 'to distinguish between real\\nand synthetic images in our baseline detector dataset. We\\nchose MISLnet as the architecture for our experiments be-\\ncause it is lightweight and has shown repeatedly to obtain\\nstrong performance on detecting synthetic images and other\\nimage forensic tasks [4, 5, 12, 43, 44]. We trained MISLnet\\nusing a Cross-Entropy Loss function via the ADAM opti-\\nmizer [34] with a constant learning rate of 5.0 × 10−5. This\\nmodel resulted in an accuracy of 0.97 on the testing portion\\nof our baseli',\n",
       " 'ne detector dataset.\\n\\nMemory Buffer. Throughout all experiments in this paper,\\nwe fixed the size of memory buffer, M, to 1000 images\\nfrom which 500 are synthetic and 500 are real. The syn-\\nthetic images are then equally distributed among all previ-\\nously known generators. We note that the memory buffer\\nis continually updated after each step to include samples of\\nthe new generator.\\n\\nPerformance Metrics. To evaluate the performance of our\\nproposed approach and others, we chose the Area Under the\\nROC Curve (AU',\n",
       " 'C) metric. Additionally, in order to obtain\\na more complete picture of our performance, we provided\\nthe Relative Error Reduction (RER) with respect to the sec-\\nond best performing method, reported as a percentage. This\\nmetric is calculated as follows:\\n\\nRER = 100 ×\\n\\nAUCN − AUCR\\n1 − AUCR\\n\\n,\\n\\n(3)\\n\\nwhere AUCR is the AUC of the referencing method, and\\nAUCN is the AUC of the method being compared against.\\nCompeting Methods. We compared our method against\\nmultiple approaches, including two naive strategies: Base-\\n',\n",
       " 'line, where the baseline detector is never updated, and\\nFine-tuning, where the baseline detector is updated exclu-\\nsively with data from new generators. Additionally, we\\nbenchmarked our approach against seven well-established\\ncontinual learning strategies: Learning Without Forgetting\\n\\n334\\n335\\n336\\n337\\n338\\n\\n339\\n\\n340\\n341\\n342\\n343\\n344\\n345\\n\\n346\\n347\\n348\\n349\\n\\n350\\n351\\n352\\n\\n353\\n354\\n355\\n356\\n357\\n358\\n\\n359\\n360\\n\\n361\\n362\\n363\\n364\\n365\\n366\\n\\n367\\n\\n368\\n369\\n370\\n371\\n372\\n373\\n\\n374\\n\\n375\\n\\n376\\n377\\n\\n378\\n379\\n380\\n\\n381\\n382\\n383\\n384\\n\\n\\x0cCVPR\\n#',\n",
       " '0007\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nMethod\\n\\nBaseline\\n\\nFine-Tune\\n\\nLWF [38]\\n\\nER [53]\\n\\nDER++ [10]\\n\\nUDIL [56]\\n\\nICaRL [52]\\n\\nMT-SC [42]\\n\\nMT-MC [42]\\n\\nOurs\\n\\nSynthetic Image Generators\\n\\nSD1.4 GLD MJ\\n\\nDLM TT\\n\\nSD2.1 CIPS\\n\\nBG VQD DIG SG3\\n\\nGF\\n\\nDL2\\n\\nLD\\n\\nEG3\\n\\nPG\\n\\nSD1.1 DDG DDP\\n\\n0.89\\n\\n0.65\\n\\n0.91\\n\\n0.94\\n\\n0.94\\n\\n0.94\\n\\n0.88\\n\\n0.91\\n\\n0.92\\n\\n0.99\\n\\n0.98\\n\\n0.83\\n\\n0.98\\n\\n0.99\\n\\n0.99\\n\\n1.00\\n\\n0.96\\n\\n0.99\\n\\n1.00\\n\\n1.00\\n\\n0.85\\n\\n0.65\\n\\n0.92\\n\\n0.97\\n\\n0.96\\n\\n0.97\\n\\n0.91\\n\\n0.93\\n\\n0.95\\n\\n0.99\\n\\n0.86\\n\\n0.67\\n\\n',\n",
       " '0.82\\n\\n0.96\\n\\n0.95\\n\\n0.97\\n\\n0.91\\n\\n0.93\\n\\n0.95\\n\\n0.99\\n\\n0.81\\n\\n0.64\\n\\n0.85\\n\\n0.96\\n\\n0.95\\n\\n0.96\\n\\n0.89\\n\\n0.92\\n\\n0.94\\n\\n0.98\\n\\n0.89\\n\\n0.69\\n\\n0.92\\n\\n0.94\\n\\n0.93\\n\\n0.94\\n\\n0.91\\n\\n0.92\\n\\n0.93\\n\\n0.98\\n\\n0.76\\n\\n0.85\\n\\n0.85\\n\\n0.99\\n\\n0.98\\n\\n0.99\\n\\n0.96\\n\\n0.98\\n\\n0.99\\n\\n0.99\\n\\n0.77\\n\\n0.75\\n\\n0.88\\n\\n0.91\\n\\n0.89\\n\\n0.92\\n\\n0.87\\n\\n0.90\\n\\n0.90\\n\\n0.99\\n\\n0.85\\n\\n0.88\\n\\n0.96\\n\\n0.98\\n\\n0.97\\n\\n0.98\\n\\n0.94\\n\\n0.96\\n\\n0.98\\n\\n0.99\\n\\n0.76\\n\\n0.88\\n\\n0.94\\n\\n0.98\\n\\n0.97\\n\\n0.98\\n\\n0.95\\n\\n0.96\\n\\n0.98\\n\\n0.99\\n\\n0.98\\n\\n0.90\\n\\n0.99\\n\\n0.99\\n\\n0.99\\n\\n0.99\\n\\n0.91\\n\\n0.97\\n\\n0.99\\n\\n0.99\\n\\n0.73\\n\\n0.92\\n\\n0.95\\n\\n0.99\\n\\n0.98\\n\\n0.99\\n\\n0.95\\n\\n0.',\n",
       " '98\\n\\n0.99\\n\\n0.99\\n\\n0.97\\n\\n0.79\\n\\n0.94\\n\\n0.99\\n\\n0.99\\n\\n0.99\\n\\n0.95\\n\\n0.97\\n\\n0.99\\n\\n1.00\\n\\n0.81\\n\\n0.68\\n\\n0.72\\n\\n0.90\\n\\n0.90\\n\\n0.91\\n\\n0.86\\n\\n0.88\\n\\n0.88\\n\\n0.96\\n\\n0.98\\n\\n0.89\\n\\n0.97\\n\\n1.00\\n\\n1.00\\n\\n1.00\\n\\n0.97\\n\\n0.99\\n\\n0.99\\n\\n1.00\\n\\n0.89\\n\\n0.76\\n\\n0.92\\n\\n0.96\\n\\n0.96\\n\\n0.97\\n\\n0.89\\n\\n0.94\\n\\n0.96\\n\\n0.99\\n\\n0.87\\n\\n0.74\\n\\n0.91\\n\\n0.94\\n\\n0.93\\n\\n0.93\\n\\n0.88\\n\\n0.91\\n\\n0.92\\n\\n0.99\\n\\n0.67\\n\\n0.87\\n\\n0.95\\n\\n0.99\\n\\n0.99\\n\\n0.99\\n\\n0.93\\n\\n0.98\\n\\n0.99\\n\\n1.00\\n\\n0.78\\n\\n0.84\\n\\n0.94\\n\\n0.99\\n\\n0.98\\n\\n0.99\\n\\n0.96\\n\\n0.98\\n\\n0.99\\n\\n0.99\\n\\nAvg. ± Std.\\n\\n0.85 ± .090\\n\\n0.78 ± .098\\n\\n0.91 ± .066\\n\\n0.97 ± .029\\n\\n0.96 ± .031',\n",
       " '\\n\\n0.97 ± .028\\n\\n0.92 ± .034\\n\\n0.95 ± .034\\n\\n0.96 ± .036\\n\\n0.99 ± .008\\n\\nTable 2. Detection performance of each method when adapting to a single new generator. Note that the “baseline” detector has only seen\\ndata from the baseline detector dataset and all approaches started from the same baseline detector network. Performance numbers are\\nmeasured in AUC.\\n\\n385\\n386\\n387\\n388\\n389\\n\\n390\\n391\\n\\n392\\n\\n393\\n\\n394\\n395\\n396\\n397\\n398\\n\\n399\\n400\\n\\n401\\n402\\n403\\n404\\n405\\n\\n406\\n407\\n408\\n\\n409\\n410\\n411\\n412\\n\\n413\\n414\\n415\\n416\\n\\n417\\n418\\n419\\n420\\n\\n(LWF)',\n",
       " ' [38], Experience Replay (ER) [53], Dark Experience\\nReplay (DER++) [10], Unified Domain Incremental Learn-\\ning (UDIL) [56], Incremental Classifier and Representation\\nLearning (ICaRL) [52], Multi-Task Single-Classifier (MT-\\nSC) [42], and Multi-Task Multi-Classifier (MT-MC) [42].\\nAdditionally we benchmarked ICaRL, MT-SC, and MT-MC\\nusing our own implementations.\\n\\n5.2. Adapting to One New Generator\\n\\nIn this experiment, we evaluated E3’s ability to detect syn-\\nthetic images from one new generator. This is import',\n",
       " 'ant\\nbecause in reality we do not know beforehand which gen-\\nerator we need to adapt our detector to. As a result, we re-\\npeated this experiment for all 19 generators in our dataset.\\nWe then compared our method’s performance to other tech-\\nniques used to update the baseline detector. We note that all\\napproaches started with the same baseline detector network.\\nWe present the results of these experiments in Tab. 2. In\\nthis table, each column represents the performance of differ-\\nent algorithms adapting the bas',\n",
       " 'eline detector to one specific\\ngenerator. The final column calculates the average AUC and\\nstandard deviation over all generators.\\n\\nThis experiment’s results in Tab. 2 show that our method\\nachieves the best detection performance across all possible\\nnew generators with an average AUC of 0.99. Compared\\nto the second best method, UDIL, whose an average AUC\\nis 0.97, we obtained a significant relative error reduction\\nof 66%. This result shows that our approach is better at\\nadapting to any one new generator than o',\n",
       " 'ther approaches.\\n\\nAdditionally, we note that other competing methods’\\nperformance has standard deviations of about 3 to 10 times\\nlarger than ours. This result can be further examined\\nby looking at certain occasions when the baseline detec-\\ntor experiences a significant performance drop detecting\\na specific unseen generator. When this happens, other\\napproaches also tend to exhibit a similar drop in perfor-\\nmance. For example, all competing methods have a signif-\\n\\nicant drop adapting to Latent Diffusion (LD) ',\n",
       " 'and BigGAN\\n(BG). Similarly, LWF experiences a performance drop on\\nDLM, DER++, MT-SC and MT-MC on SD1.1, SD1.4 &\\nSD2.1, and ICaRL on SD1.1 & SD1.4. This result is not\\nsurprising, however, because the baseline detector, which\\nwas trained on GAN, had to be adapted to detect diffusion\\nmodels’ images. This aligns with findings in prior work,\\nwhich showed that synthetic image detectors “cannot reli-\\nably detect images that present artifacts significantly differ-\\nent from those seen during training.” [13]\\n\\nIn cont',\n",
       " 'rast, our approach displays strong and stable per-\\nformance irrespective of which generator were added. This\\nis likely because our approach does not need to rely heavily\\non learning a single embedding space to capture traces of\\nboth existing and the new generators.\\n\\n5.3. Adapting to Multiple Emerging Generators\\n\\nIn this experiment, we tested E3’s ability to adapt to mul-\\ntiple sequentially emerging generators. We conducted this\\nevaluation to mimic real world scenarios in which a detec-\\ntor needs to be able ',\n",
       " 'to adapt to detect synthetic images from\\neach and every newly emerging generator. We then com-\\npared our method’s performance to other dedicated contin-\\nual learning techniques. After adapting to a new generator,\\nwe measured E3’s and competing methods’ performance in\\nterms of average AUC over the current and the previous\\ngenerators. We repeated this process until we exhausted all\\n19 emerging generators in our dataset.\\n\\nWe present the results of this experiment in Tab. 3. These\\nresults show that our approach',\n",
       " ' achieved the best perfor-\\nmance irrespective of the number of new generators added.\\nFurthermore, we observe that after continually adapting to\\n19 different generators, our method obtained an average\\nAUC of 0.97, a reduction of only 0.02 when compared to\\nthe initial performance of 0.99 average AUC. This result\\nis a significant improvement over the second best method,\\nUDIL, with 0.93 average AUC. Additionally, it can be seen\\n\\n421\\n422\\n423\\n424\\n\\n425\\n426\\n427\\n428\\n429\\n430\\n\\n431\\n432\\n433\\n434\\n435\\n\\n436\\n\\n437\\n438\\n439\\n440',\n",
       " '\\n\\n441\\n442\\n443\\n444\\n445\\n446\\n\\n447\\n\\n448\\n449\\n450\\n451\\n\\n452\\n453\\n454\\n455\\n456\\n\\n6\\n\\n\\x0cCVPR\\n#0007\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nMethod\\n\\nBaseline\\n\\nFine-Tune\\n\\nLWF [38]\\n\\nER [53]\\n\\nDER++ [10]\\n\\nUDIL [56]\\n\\nICaRL [52]\\n\\nMT-SC [42]\\n\\nMT-MC [42]\\n\\nOurs\\n\\nSynthetic Image Generators\\n\\nSD1.4 GLD MJ\\n\\nDLM TT\\n\\nSD2.1 CIPS\\n\\nBG VQD DIG SG3\\n\\nGF\\n\\nDL2\\n\\nLD\\n\\nEG3\\n\\nPG\\n\\nSD1 DDG DDP\\n\\n0.89\\n\\n0.65\\n\\n0.90\\n\\n0.94\\n\\n0.96\\n\\n0.95\\n\\n0.91\\n\\n0.87\\n\\n0.92\\n\\n0.99\\n\\n0.91\\n\\n0.79\\n\\n0.91\\n\\n0.96\\n\\n0.96\\n\\n0.97\\n\\n0.93\\n\\n0.85\\n\\n0.96\\n\\n',\n",
       " '0.99\\n\\n0.86\\n\\n0.73\\n\\n0.89\\n\\n0.96\\n\\n0.92\\n\\n0.96\\n\\n0.92\\n\\n0.77\\n\\n0.94\\n\\n0.99\\n\\n0.83\\n\\n0.81\\n\\n0.89\\n\\n0.96\\n\\n0.91\\n\\n0.96\\n\\n0.91\\n\\n0.76\\n\\n0.94\\n\\n0.99\\n\\n0.80\\n\\n0.83\\n\\n0.88\\n\\n0.95\\n\\n0.88\\n\\n0.96\\n\\n0.90\\n\\n0.75\\n\\n0.93\\n\\n0.99\\n\\n0.79\\n\\n0.83\\n\\n0.89\\n\\n0.95\\n\\n0.89\\n\\n0.95\\n\\n0.92\\n\\n0.74\\n\\n0.91\\n\\n0.98\\n\\n0.76\\n\\n0.80\\n\\n0.86\\n\\n0.95\\n\\n0.86\\n\\n0.96\\n\\n0.91\\n\\n0.76\\n\\n0.92\\n\\n0.98\\n\\n0.73\\n\\n0.84\\n\\n0.85\\n\\n0.91\\n\\n0.84\\n\\n0.93\\n\\n0.89\\n\\n0.69\\n\\n0.88\\n\\n0.98\\n\\n0.73\\n\\n0.82\\n\\n0.85\\n\\n0.93\\n\\n0.83\\n\\n0.94\\n\\n0.90\\n\\n0.73\\n\\n0.89\\n\\n0.98\\n\\n0.71\\n\\n0.82\\n\\n0.85\\n\\n0.93\\n\\n0.82\\n\\n0.94\\n\\n0.91\\n\\n0.74\\n\\n0.90\\n\\n0.98\\n\\n0.73\\n\\n0.85\\n\\n0.82\\n\\n0.94\\n\\n0.',\n",
       " '82\\n\\n0.94\\n\\n0.90\\n\\n0.74\\n\\n0.91\\n\\n0.97\\n\\n0.71\\n\\n0.85\\n\\n0.82\\n\\n0.94\\n\\n0.82\\n\\n0.94\\n\\n0.91\\n\\n0.73\\n\\n0.92\\n\\n0.97\\n\\n0.73\\n\\n0.73\\n\\n0.72\\n\\n0.93\\n\\n0.79\\n\\n0.94\\n\\n0.91\\n\\n0.74\\n\\n0.91\\n\\n0.97\\n\\n0.72\\n\\n0.79\\n\\n0.80\\n\\n0.91\\n\\n0.81\\n\\n0.93\\n\\n0.87\\n\\n0.70\\n\\n0.86\\n\\n0.97\\n\\n0.74\\n\\n0.81\\n\\n0.82\\n\\n0.93\\n\\n0.82\\n\\n0.94\\n\\n0.90\\n\\n0.69\\n\\n0.90\\n\\n0.97\\n\\n0.74\\n\\n0.70\\n\\n0.81\\n\\n0.90\\n\\n0.82\\n\\n0.92\\n\\n0.89\\n\\n0.70\\n\\n0.87\\n\\n0.97\\n\\n0.74\\n\\n0.76\\n\\n0.80\\n\\n0.90\\n\\n0.82\\n\\n0.92\\n\\n0.90\\n\\n0.70\\n\\n0.87\\n\\n0.97\\n\\n0.72\\n\\n0.83\\n\\n0.83\\n\\n0.93\\n\\n0.82\\n\\n0.93\\n\\n0.90\\n\\n0.76\\n\\n0.91\\n\\n0.97\\n\\n0.71\\n\\n0.74\\n\\n0.79\\n\\n0.93\\n\\n0.80\\n\\n0.93\\n\\n0.90\\n\\n0.74\\n\\n0.90\\n\\n0.97',\n",
       " '\\n\\nTable 3. Detection performance of each method when sequentially adapting to a series of new generators. Note that the “baseline” detector\\nhas only seen data from the baseline detector dataset and all approaches started from the same baseline detector network. Performance\\nnumbers are measured in AUC.\\n\\nArchitecture\\n\\nAccuracy\\n\\nAUC\\n\\nOurs UDIL Ours UDIL\\n\\nMISLnet [6]\\n\\n0.922\\n\\nResNet-50 [20]\\n\\n0.901\\n\\nDenseNet [23]\\n\\nSR-Net [7]\\n\\n0.810\\n\\n0.971\\n\\n0.860\\n\\n0.890\\n\\n0.800\\n\\n0.964\\n\\n0.970\\n\\n0.817\\n\\n0.890\\n\\n0.996\\n\\n0.932\\n\\n0.790\\n\\n0.86',\n",
       " '9\\n\\n0.994\\n\\nAUC\\nRER\\n\\n55.9%\\n\\n12.9%\\n\\n16.0%\\n\\n33.3%\\n\\nAverage\\n\\n0.901\\n\\n0.878\\n\\n0.918\\n\\n0.896\\n\\n21.2%\\n\\nTable 4. The performance of our approach versus UDIL using\\ndifferent basline architectures.\\n\\nFigure 4. Our method shows slight performance decline when\\nadapting to 19 generators with training image counts reduced from\\n500 to 50 per generator.\\n\\nfrom Tab. 3 that as the number of new generators introduced\\nincreases, the more degraded the performance of some com-\\npeting approaches becomes (i.e. LWF, DER++, MT-SC). In\\ncont',\n",
       " 'rast, our method retained strong performance through-\\nout our experiment, with minimal reduction in performance.\\nAdditionally, we note that other competing methods all\\nexperience difficulties detecting new synthetic images from\\nsome particular generator. For example, LWF & ER ex-\\nperienced significant performance drop when adapting to\\nDALL-E 2 (DL2), similarly, UDIL’s performance dropped\\nadapting to BigGAN (BG), and ICaRL, MT-SC, MT-MC\\nhave substantial difficulties adapting to Latent Diffusion\\n(LD). However',\n",
       " ', our proposed approach does not experience\\nsuch issues. This suggests that by fusing the embedding\\nspaces of all past and current expert embedders, we can\\ngracefully adapt to any new generator while avoid having\\nsignificant performance degradation as the number of gen-\\nerator being added increases.\\n\\n5.4. Effects of New Generator’s Training Data Size\\n\\nIn this experiment, we examined the impact of different\\ntraining data size from each new generator on our proposed\\n\\n457\\n458\\n\\n459\\n460\\n461\\n462\\n463\\n464\\n\\n465\\n466\\n',\n",
       " '467\\n468\\n469\\n470\\n\\n471\\n472\\n473\\n474\\n\\n475\\n\\n476\\n477\\n\\napproach. This is important because in real world scenarios,\\nthe amount of available data from a new generator is often\\nlimited, especially if such generator is a proprietary prod-\\nuct. Therefore, to conduct this experiment, we repeated the\\nexperiment in Sec 5.3, with different numbers of available\\ndata ranging from 50 to 500 data points. We then reported\\nthe average AUC after sequentially adding all 19 generators\\nto our dataset.\\n\\nThe results of this experimen',\n",
       " 't are depicted in Fig. 4.\\nThese results show that our method’s performance experi-\\nenced minimal reduction in detection performance as the\\nnumber of available training images decreased. Specifi-\\ncally, we received a very slight drop in performance (0.97\\nto 0.95 average AUC) while having 5 times less training\\ndata, and a small drop in performance (0.97 to 0.94 average\\nAUC) with 10 times less data. Notably, our performance\\nof 0.94 average AUC is still an improvement over UDIL\\n(0.93 AUC), whose training data s',\n",
       " 'ize is 10 times larger than\\nours (see Sec. 5.3). This suggests that our approach not\\nonly achieves the highest performance in adapting to de-\\ntect synthetic images from new generators but also requires\\nsignificantly less data than other methods to attain strong\\nperformance.\\n\\n478\\n\\n479\\n480\\n481\\n482\\n483\\n\\n484\\n485\\n\\n486\\n487\\n488\\n489\\n490\\n\\n491\\n492\\n493\\n494\\n495\\n496\\n\\n497\\n498\\n499\\n500\\n\\n7\\n\\n\\x0cCVPR\\n#0007\\n\\n501\\n\\n502\\n\\n503\\n504\\n505\\n506\\n507\\n508\\n\\n509\\n510\\n511\\n512\\n513\\n514\\n\\n515\\n516\\n517\\n518\\n519\\n520\\n\\n521\\n522\\n523\\n524\\n525\\n\\n526\\n\\n527\\n528\\n529',\n",
       " '\\n530\\n\\n531\\n532\\n533\\n534\\n535\\n\\n536\\n537\\n538\\n539\\n540\\n541\\n\\n542\\n543\\n544\\n545\\n546\\n547\\n\\n548\\n\\n549\\n550\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\n5.5. Effects of Different Detector Architectures\\n\\nIn this experiment, we examined the effects of different\\nnetwork architectures for the baseline detector. This is to\\ndemonstrate the generality of our approach over the di-\\nverse set of detection algorithms in the wild. To do this,\\nwe repeated the experiment in Sec. 5.3 with three add',\n",
       " 'i-\\ntional network architectures: ResNet-50 [20], SR-Net [7],\\nand DenseNet [23]. These networks are widely used in\\nprior work for synthetic image detection and other forensic\\ntasks [32, 36, 37, 57, 66, 70]. We then measured accuracy\\nand AUC after adapting to all 19 generators in our dataset.\\nWe note that due to space limitation, we will only compare\\nto UDIL, the best performing method in both experiments\\nin Secs. 5.2 and 5.3.\\n\\nWe present this experiment’s results in Tab. 4. These re-\\nsults show that our prop',\n",
       " 'osed approach achieves the best per-\\nformance irrespective of the underlying detector architec-\\nture. In particular, we notably outperform UDIL in both ac-\\ncuracy and AUC when employing MISLnet, ResNet-50, or\\nDenseNet as the network architectures for the baseline de-\\ntector. Across all architectures, our performance is a 21.2%\\nrelative error reduction when compared to UDIL. These re-\\nsults suggest that our proposed method is invariant to dif-\\nferent architecture designs and can be applied in a general\\nmanne',\n",
       " 'r to most, if not all, synthetic image detectors.\\n\\n6. Discussion\\n\\nOur method’s strong performance can be attributed to its\\nutilization of dedicated embedders for each new generator.\\nThese embeddings are specifically learned to capture foren-\\nsic traces left by their corresponding generators. As a result,\\nour algorithm does not rely solely on a single embedding\\nspace, but rather on the union of all embedding spaces from\\nevery expert embedder.\\n\\nHowever, our approach does come at a cost of increased\\nnetwork si',\n",
       " 'ze. Despite this, in many cases, the cost of mis-\\ndetecting AI-generated images, or false-alarming real im-\\nages as synthetic, is worth the increase in network param-\\neters. Additionally, it is important to note that after our\\nmethod adapted the baseline detector (based on MISLnet)\\nto 19 different generators, the total number of parameters\\nin our network is 27.8M, a comparable number to a sin-\\ngle ResNet-50 model with 23.5M parameters. Furthermore,\\n25.1M of these parameters are frozen because they come\\nfrom',\n",
       " ' the ensemble of embedders. In reality, only about 4M\\nparameters in our network are trainable. For perspective, af-\\nter adding 100 new generators, the trainable portion of our\\nnetwork is only 13.6M parameters.\\n\\n7. Ablation Results\\n\\nWe conducted an ablation study to assess the impact of dif-\\nferent design choices of our Expert Knowledge Fusion Net-\\n\\nSetup\\n\\nProposed\\n\\nAccuracy\\n\\n0.93\\n\\nAUC\\n\\n0.97\\n\\nMajority Voting\\n\\n0.50 (-0.43)\\n\\n0.90 (-0.07)\\n\\nKnowledge Fusion w/ MLP only\\n\\n0.91 (-0.02)\\n\\n0.96 (-0.01)\\n\\nNo Transformer',\n",
       " ' Weighting\\n\\n0.88 (-0.05)\\n\\n0.96 (-0.01)\\n\\n5 Transformer Layers\\n\\n0.91 (-0.02)\\n\\n0.97 (-0.00)\\n\\n10 Transformer Layers\\n\\n0.91 (-0.02)\\n\\n0.96 (-0.01)\\n\\nTable 5. Ablation study of the different design choices of the Ex-\\npert Knowledge Fusion Network.\\n\\nwork (EKFN). We present these results in Table 5.\\n\\nMajority Voting.\\nIn this setup, we fused the knowledge\\nfrom each expert embedder using a majority voting strat-\\negy. This approach involves each expert embedder produc-\\ning their own decisions about the input image and th',\n",
       " 'e final\\ndetection score is decided using a majority vote. As shown\\nin Tab. 5, this approach resulted in a significant drop in both\\naccuracy and AUC. We note that the reason why this ver-\\nsion has an AUC of 0.90 with an accuracy of 0.50 is be-\\ncause it produces an uncalibrated decision score, a similar\\nphenomenon observed in Corvi et al. [13].\\n\\nKnowledge Fusion with MLP Only. In this experiment,\\nwe removed the transformer from the EKFN and evaluated\\nthe performance of our approach. Tab. 5 shows that this ver',\n",
       " '-\\nsion experience a performance reduction compared to our\\nproposed method. Hence, the transformer is important to\\nobtain strong performance.\\n\\nNo Transformer Weighting. In this experiment setup, we\\ndiscarded the weighting between the transformer output and\\nits input. Results in Tab. 5 show that this version achieves\\nworse performance than our proposed method. Hence, this\\nintegration approach is important for our method.\\n\\nNumber of Transformer Layers. We tested our ap-\\nproach with the transformer made using o',\n",
       " 'nly 5 or 10 layers.\\nTab. 5’s results show that this is sub-optimal to our approach\\nin terms of AUC and accuracy.\\n\\n8. Conclusion\\n\\nThis paper introduces E3, a novel approach for effec-\\ntively updating synthetic image detectors to accurately de-\\ntect newly emerging generators with minimal training data.\\nBy developing expert embedders tailored to capture traces\\nfrom each new target generator, our method demonstrates\\nstrong adaptability. The proposed expert knowledge fusion\\nnetwork analyzes forensic evidence fro',\n",
       " 'm all experts, facil-\\nitating precise detection decisions. Through extensive ex-\\nperimentation, E3 consistently outperforms competing con-\\ntinual learning approaches, even across various detector ar-\\nchitectures and with limited data from new generators.\\n\\n551\\n\\n552\\n553\\n554\\n\\n555\\n556\\n557\\n558\\n559\\n\\n560\\n561\\n\\n562\\n563\\n564\\n565\\n566\\n567\\n\\n568\\n\\n569\\n570\\n571\\n572\\n\\n573\\n574\\n575\\n576\\n\\n577\\n\\n578\\n\\n579\\n580\\n581\\n582\\n583\\n\\n584\\n585\\n586\\n587\\n588\\n\\n8\\n\\n\\x0cCVPR\\n#0007\\n\\n589\\n\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\n600\\n601\\n602\\n603\\n604\\n605\\n606\\n607',\n",
       " '\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nReferences\\n\\n[1] Midjourney. https://www.midjourney.com/. Ac-\\n\\ncessed: March 19, 2024. 2, 5\\n\\n[2] Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb\\nImage\\nSterkin, Victor Lempitsky, and Denis Korzhenkov.\\ngenerators with conditionally-independent pixel synthesis.\\nIn Proce',\n",
       " 'edings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 14278–14287, 2021.\\n5\\n\\n[3] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¨onlieb,\\nand Christian Etmann. Conditional image generation with\\nscore-based diffusion models, 2021. 2\\n\\n[4] Belhassen Bayar and Matthew C Stamm. A deep learning\\napproach to universal image manipulation detection using\\na new convolutional layer. In Proceedings of the 4th ACM\\nworkshop on information hiding and multimedia security,\\npages 5–10, 2016. 5\\n',\n",
       " '\\n[5] Belhassen Bayar and Matthew C. Stamm. On the robustness\\nof constrained convolutional neural networks to jpeg post-\\nIn ICASSP,\\ncompression for image resampling detection.\\npages 2152–2156, 2017. 5\\n\\n[6] Belhassen Bayar and Matthew C. Stamm. Constrained con-\\nvolutional neural networks: A new approach towards general\\npurpose image manipulation detection. IEEE Transactions\\non Information Forensics and Security, 13(11):2691–2706,\\n2018. 5, 7\\n\\n[7] Mehdi Boroumand, Mo Chen, and Jessica Fridrich. Deep\\nIEEE\\nresidu',\n",
       " 'al network for steganalysis of digital images.\\nTransactions on Information Forensics and Security, 14(5):\\n1181–1193, 2019. 7, 8\\n\\n[8] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\\nscale gan training for high fidelity natural image synthesis.\\narXiv preprint arXiv:1809.11096, 2018. 2, 5\\n\\n[9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,\\nYufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-\\nman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya\\nRamesh. Video generation models as world simulator',\n",
       " 's.\\n2024. 3\\n\\n[10] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide\\nAbati, and Simone Calderara. Dark experience for gen-\\neral continual learning: a strong, simple baseline. Advances\\nin neural information processing systems, 33:15920–15930,\\n2020. 2, 6, 7\\n\\n[11] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\\ngeometry-aware 3d generative adversarial networks. In Pro-\\nceedings of the I',\n",
       " 'EEE/CVF conference on computer vision\\nand pattern recognition, pages 16123–16133, 2022. 5\\n[12] Chen Chen, Xinwei Zhao, and Matthew C. Stamm. Mislgan:\\nAn anti-forensic camera model falsification framework using\\na generative adversarial network. In ICIP, pages 535–539,\\n2018. 5\\n\\n[13] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Gio-\\nvanni Poggi, Koki Nagano, and Luisa Verdoliva. On the\\ndetection of synthetic images generated by diffusion mod-\\nels. In ICASSP 2023-2023 IEEE International Conference\\n\\non Aco',\n",
       " 'ustics, Speech and Signal Processing (ICASSP), pages\\n1–5. IEEE, 2023. 2, 3, 6, 8\\n\\n[14] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah,\\nTanishq Abraham, Ph´uc Lˆe Khac, Luke Melas, and Ritobrata\\nGhosh. Dall·e mini, 2021. 5\\n\\n[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models\\nbeat gans on image synthesis. In Advances in Neural Infor-\\nmation Processing Systems, pages 8780–8794. Curran Asso-\\nciates, Inc., 2021. 2\\n\\n[16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\\nIn Pro-\\ntransformer',\n",
       " 's for high-resolution image synthesis.\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 12873–12883, 2021.\\n5\\n\\n[17] Shengbang Fang, Tai D Nguyen, and Matthew c Stamm.\\nOpen set synthetic image source attribution. In 34th British\\nMachine Vision Conference 2023, BMVC 2023, Aberdeen,\\nUK, November 20-24, 2023. BMVA, 2023. 2, 5\\n\\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. Generative',\n",
       " ' adversarial nets. Advances in\\nneural information processing systems, 27, 2014. 2\\n\\n[19] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\\ntor quantized diffusion model for text-to-image synthesis. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition, pages 10696–10706, 2022. 5\\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE conference on',\n",
       " ' computer vision and pattern\\nrecognition, pages 770–778, 2016. 7, 8\\n\\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\\nfusion probabilistic models. Advances in neural information\\nprocessing systems, 33:6840–6851, 2020. 2, 5\\n\\n[22] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,\\nMohammad Norouzi, and Tim Salimans. Cascaded diffu-\\nsion models for high fidelity image generation. Journal of\\nMachine Learning Research, 23(47):1–33, 2022. 2\\n\\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maate',\n",
       " 'n, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 4700–4708, 2017. 7, 8\\n[24] Drew A Hudson and Larry Zitnick. Generative adversar-\\nIn Proceedings of the 38th International\\nial transformers.\\nConference on Machine Learning, pages 4487–4499. PMLR,\\n2021. 5\\n\\n[25] Nils Hulzebosch, Sarah Ibrahimi, and Marcel Worring. De-\\ntecting cnn-generated facial images in real-world scenarios.\\nIn Proceedings of the I',\n",
       " 'EEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) Workshops, 2020. 2\\n[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\\nEfros.\\nImage-to-image translation with conditional adver-\\nsarial networks. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 1125–1134,\\n2017. 2\\n\\n[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\\nProgressive growing of GANs for improved quality, stabil-\\n\\n646\\n647\\n\\n648\\n649\\n650\\n\\n651\\n652\\n653\\n654\\n\\n655\\n656\\n657\\n658\\n659',\n",
       " '\\n\\n660\\n661\\n662\\n663\\n\\n664\\n665\\n666\\n667\\n\\n668\\n669\\n670\\n671\\n672\\n\\n673\\n674\\n675\\n676\\n\\n677\\n678\\n679\\n\\n680\\n681\\n682\\n683\\n\\n684\\n685\\n686\\n687\\n\\n688\\n689\\n690\\n691\\n\\n692\\n693\\n694\\n695\\n\\n696\\n697\\n698\\n699\\n700\\n\\n701\\n702\\n\\n9\\n\\n\\x0cCVPR\\n#0007\\n\\n703\\n704\\n\\n705\\n706\\n707\\n708\\n\\n709\\n710\\n711\\n712\\n\\n713\\n714\\n715\\n716\\n717\\n\\n718\\n719\\n720\\n721\\n\\n722\\n723\\n724\\n725\\n\\n726\\n727\\n728\\n729\\n\\n730\\n731\\n732\\n733\\n\\n734\\n735\\n\\n736\\n737\\n738\\n739\\n\\n740\\n741\\n742\\n743\\n\\n744\\n745\\n746\\n\\n747\\n748\\n749\\n750\\n751\\n752\\n\\n753\\n754\\n755\\n\\n756\\n757\\n758\\n759\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIB',\n",
       " 'UTE.\\n\\nCVPR\\n#0007\\n\\nity, and variation. In International Conference on Learning\\nRepresentations, 2018. 5\\n\\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\\ngenerator architecture for generative adversarial networks.\\nIn Proceedings of the IEEE/CVF conference on computer vi-\\nsion and pattern recognition, pages 4401–4410, 2019. 2\\n[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based\\ngenerator architecture for generative adversarial networks.\\nIn Proceedings of the IEEE/CVF conference on computer',\n",
       " ' vi-\\nsion and pattern recognition, pages 4401–4410, 2019. 5\\n[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\\nJaakko Lehtinen, and Timo Aila. Analyzing and improv-\\nIn Proceedings of\\ning the image quality of stylegan.\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 8110–8119, 2020. 5\\n\\n[31] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,\\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\\ngenerative adversarial networks. Advances in neural infor-\\nmation',\n",
       " ' processing systems, 34:852–863, 2021. 5\\n\\n[32] Minha Kim, Shahroz Tariq, and Simon S Woo. Cored: Gen-\\neralizing fake media detection with continual representation\\nusing distillation. In Proceedings of the 29th ACM Interna-\\ntional Conference on Multimedia, pages 337–346, 2021. 8\\n\\n[33] Minha Kim, Shahroz Tariq, and Simon S Woo. Cored: Gen-\\neralizing fake media detection with continual representation\\nusing distillation. In Proceedings of the 29th ACM Interna-\\ntional Conference on Multimedia, pages 337–346, 202',\n",
       " '1. 2\\n\\n[34] Diederik Kingma and Jimmy Ba. Adam: A method for\\nIn International Conference on\\nstochastic optimization.\\nLearning Representations (ICLR), San Diega, CA, USA,\\n2015. 5\\n\\n[35] Diederik P Kingma and Max Welling. Auto-encoding varia-\\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\\n[36] Sangyup Lee, Shahroz Tariq, Youjin Shin, and Simon S Woo.\\nDetecting handcrafted facial image manipulations and gan-\\ngenerated facial images using shallow-fakefacenet. Applied\\nsoft computing, 105:107256, 2021. 8\\n\\n[3',\n",
       " '7] Weichuang Li, Peisong He, Haoliang Li, Hongxia Wang, and\\nRuimei Zhang. Detection of gan-generated images by esti-\\nmating artifact similarity. IEEE Signal Processing Letters,\\n29:862–866, 2022. 8\\n\\n[38] Zhizhong Li and Derek Hoiem. Learning without forgetting.\\nIEEE transactions on pattern analysis and machine intelli-\\ngence, 40(12):2935–2947, 2017. 1, 2, 6, 7\\n\\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\\nZitnick. Microsoft coco: Com',\n",
       " 'mon objects in context.\\nIn\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart V 13, pages 740–755. Springer, 2014. 5\\n\\n[40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\\nLarge-scale celebfaces attributes (celeba) dataset. Retrieved\\nAugust, 15(2018):11, 2018. 5\\n\\n[41] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and\\nIn\\nGiovanni Poggi. Do gans leave artificial fingerprints?\\n2019 IEEE conference on multimedia information process-\\ning ',\n",
       " 'and retrieval (MIPR), pages 506–511. IEEE, 2019. 2\\n\\n[42] Francesco Marra, Cristiano Saltori, Giulia Boato, and Luisa\\nVerdoliva. Incremental learning for the detection and clas-\\nIn 2019 IEEE inter-\\nsification of gan-generated images.\\nnational workshop on information forensics and security\\n(WIFS), pages 1–6. IEEE, 2019. 2, 6, 7\\n\\n[43] Owen Mayer and Matthew C. Stamm. Exposing fake images\\nIEEE Journal of Selected\\n\\nwith forensic similarity graphs.\\nTopics in Signal Processing, 14(5):1049–1064, 2020. 5\\n[44] Owen M',\n",
       " 'ayer, Brian Hosler, and Matthew C Stamm. Open\\nset video camera model verification. In ICASSP 2020-2020\\nIEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), pages 2962–2966. IEEE, 2020.\\n5\\n\\n[45] James L McClelland, Bruce L McNaughton, and Randall C\\nO’Reilly. Why there are complementary learning systems in\\nthe hippocampus and neocortex: insights from the successes\\nand failures of connectionist models of learning and memory.\\nPsychological review, 102(3):419, 1995. 2\\n\\n[46] Michael M',\n",
       " 'cCloskey and Neal J Cohen. Catastrophic inter-\\nference in connectionist networks: The sequential learning\\nproblem. In Psychology of learning and motivation, pages\\n109–165. Elsevier, 1989. 2\\n\\n[47] Mehdi Mirza and Simon Osindero. Conditional generative\\n\\nadversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2\\n\\n[48] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\\nPranav Shyam, Pamela Mishkin, Bob Mcgrew,\\nIlya\\nSutskever, and Mark Chen. GLIDE: Towards photorealis-\\ntic image generation and editing with',\n",
       " ' text-guided diffusion\\nmodels. In Proceedings of the 39th International Conference\\non Machine Learning, pages 16784–16804. PMLR, 2022. 5\\n[49] Md Awsafur Rahman, Bishmoy Paul, Najibul Haque Sarker,\\nZaber Ibn Abdul Hakim, and Shaikh Anowarul Fattah. Arti-\\nfact: A large-scale dataset with artificial and factual images\\nfor generalizable and robust synthetic image detection.\\nIn\\n2023 IEEE International Conference on Image Processing\\n(ICIP), pages 2200–2204. IEEE, 2023. 5\\n\\n[50] Aditya Ramesh, Mikhail Pavlov, Gabri',\n",
       " 'el Goh, Scott Gray,\\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\\nZero-shot text-to-image generation. In International confer-\\nence on machine learning, pages 8821–8831. Pmlr, 2021. 2\\n[51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\\nand Mark Chen. Hierarchical text-conditional image gener-\\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\\n(2):3, 2022. 5\\n\\n[52] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\\nSperl, and Christoph H Lampert. icarl: Incremental classifi',\n",
       " 'er\\nand representation learning. In Proceedings of the IEEE con-\\nference on Computer Vision and Pattern Recognition, pages\\n2001–2010, 2017. 2, 6, 7\\n\\n[53] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,\\nIrina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn\\nwithout forgetting by maximizing transfer and minimizing\\ninterference. arXiv preprint arXiv:1810.11910, 2018. 1, 6, 7\\n[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\\nIn Proceedings',\n",
       " ' of\\nsynthesis with latent diffusion models.\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 10684–10695, 2022. 2, 5\\n\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n800\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n\\n10\\n\\n\\x0cCVPR\\n#0007\\n\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832\\n833\\n834\\n835\\n836\\n837\\n838\\n839\\n840\\n841\\n842\\n843\\n844\\n845\\n846\\n847',\n",
       " '\\n848\\n849\\n850\\n851\\n852\\n853\\n854\\n855\\n856\\n857\\n858\\n859\\n860\\n861\\n862\\n863\\n864\\n865\\n866\\n867\\n868\\n869\\n870\\n871\\n872\\n873\\n874\\n875\\n\\nCVPR 2024 Submission #0007. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\\n\\nCVPR\\n#0007\\n\\nProceedings of the IEEE/CVF international conference on\\ncomputer vision, pages 7556–7566, 2019. 2\\n\\n[69] Xu Zhang, Svebor Karaman, and Shih-Fu Chang. Detecting\\nand simulating artifacts in gan fake images. In 2019 IEEE in-\\nternational workshop on information forensics and security\\n(WIFS), pages 1–6. IEEE, 2019. 2',\n",
       " '\\n\\n[70] Junjie Zhao, Junfeng Wu, James Msughter Adeke, Sen Qiao,\\nand Jinwei Wang. Detecting high-resolution adversarial im-\\nages with few-shot deep learning. Remote Sensing, 15(9):\\n2379, 2023. 8\\n\\n[71] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\\nEfros. Unpaired image-to-image translation using cycle-\\nconsistent adversarial networks. In Proceedings of the IEEE\\ninternational conference on computer vision, pages 2223–\\n2232, 2017. 2\\n\\n[72] Mingjian Zhu, Hanting Chen, Qiangyu YAN, Xudong\\nHuang, Guanyu Li',\n",
       " 'n, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu,\\nand Yunhe Wang. Genimage: A million-scale benchmark\\nfor detecting ai-generated image. In Advances in Neural In-\\nformation Processing Systems, pages 77771–77782. Curran\\nAssociates, Inc., 2023. 2\\n\\n[73] Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, and\\nCong Yao. Conditional text image generation with diffu-\\nsion models. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR), pages\\n14235–14245, 2023. 2\\n\\n876\\n877\\n878\\n879\\n880\\n881\\n\\n',\n",
       " '882\\n883\\n884\\n885\\n886\\n887\\n888\\n889\\n890\\n891\\n892\\n893\\n894\\n895\\n896\\n897\\n898\\n899\\n900\\n901\\n\\n[55] Axel Sauer, Kashyap Chitta, Jens M¨uller, and Andreas\\nGeiger. Projected gans converge faster. In Advances in Neu-\\nral Information Processing Systems, pages 17480–17492.\\nCurran Associates, Inc., 2021. 5\\n\\n[56] Haizhou Shi and Hao Wang. A unified approach to do-\\nmain incremental learning with memory: Theory and algo-\\nrithm. Advances in Neural Information Processing Systems,\\n36, 2024. 1, 2, 6, 7\\n\\n[57] Brijesh Singh, Arijit Sur',\n",
       " ', and Pinaki Mitra. Steganalysis of\\nIEEE Transac-\\ndigital images using deep fractal network.\\ntions on Computational Social Systems, 8(3):599–606, 2021.\\n8\\n\\n[58] Sergey Sinitsa and Ohad Fried. Deep image fingerprint: To-\\nwards low budget synthetic image detection and model lin-\\neage analysis. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision (WACV), pages\\n4067–4076, 2024. 2, 5\\n\\n[59] Sergey Sinitsa and Ohad Fried. Deep image fingerprint: To-\\nwards low budget synthetic image ',\n",
       " 'detection and model lin-\\neage analysis. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision, pages 4067–\\n4076, 2024. 2\\n\\n[60] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez,\\nAythami Morales, and Javier Ortega-Garcia. Deepfakes and\\nbeyond: A survey of face manipulation and fake detection.\\nInformation Fusion, 64:131–148, 2020. 2\\n\\n[61] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\\nOwens, and Alexei A Efros. Cnn-generated images are\\nIn Proceedings of\\nsurprisingly easy ',\n",
       " 'to spot... for now.\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 8695–8704, 2020. 5\\n\\n[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\\nOwens, and Alexei A Efros. Cnn-generated images are\\nIn Proceedings of\\nsurprisingly easy to spot... for now.\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 8695–8704, 2020. 2\\n\\n[63] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu\\nChen, and Mingyuan Zhou. Diffusion-gan: Training gans\\nwith diffusion. arXiv preprint',\n",
       " ' arXiv:2206.02262, 2022. 5\\n\\n[64] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin\\nTong. 3d-aware image generation using 2d diffusion mod-\\nels. In Proceedings of the IEEE/CVF International Confer-\\nence on Computer Vision (ICCV), pages 2383–2393, 2023.\\n2\\n\\n[65] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling\\nthe generative learning trilemma with denoising diffusion\\nGANs. In International Conference on Learning Represen-\\ntations (ICLR), 2022. 5\\n\\n[66] Yassine Yousfi, Jan Butora, Eugene Khvedchenya, ',\n",
       " 'and Jes-\\nsica Fridrich. Imagenet pre-trained cnns for jpeg steganal-\\nysis. In 2020 IEEE International Workshop on Information\\nForensics and Security (WIFS), pages 1–6. IEEE, 2020. 8\\n\\n[67] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\\nlarge-scale image dataset using deep learning with humans\\nin the loop. arXiv preprint arXiv:1506.03365, 2015. 5\\n[68] Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake\\nimages to gans: Learning and analyzing ',\n",
       " 'gan fingerprints. In\\n\\n11\\n\\n\\x0c']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department of Defense  \n",
      "U.S. Army Medical Research and Development Command  \n",
      "Congressionally Directed Medical Research Programs  \n",
      "Fiscal Year 2023 Traumatic Brain Injury and Psychological Health Research Program  Focused Program Award  \n",
      "Peer Review Summary Statement \n",
      "CDMRP Log Number: TP230309 Project Duration: 48 months Grants.gov ID Number: GRANT13982269 Budget Requested: $6,745,747 Review Panel: Focused Program - Biomarker - 1 Direct Costs: $4,996,850 Discussion Period: 11/30/2023-12/01/2023 Indirect Costs: $1,748,897 \n",
      "Title: Case-Based Reasoning to Inform Clinical Trajectory in a Collaborative and Augmented Learning  Environment for Traumatic Brain Injury (CRITICAL-TBI) \n",
      "Principal Investigator: Richard Moberg \n",
      "Performing Organization: Moberg Analytics, Inc.  \n",
      "Contracting Organization: Moberg Analytics, Inc. \n",
      "\n",
      "\n",
      "Procurement Sensitive Information \n",
      "Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 2 of 36)  \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Procurement Sensitive Information \n",
      "Do not copy or distribute  \n",
      "TP230309  (Page 3 of 36)  \n",
      "Case-Based Reasoning to Inform Clinical Trajectory in a Collaborative and Augmented Learning  Environment for Traumatic Brain Injury (CRITICAL-TBI)  \n",
      "OVERVIEW  \n",
      "The Principal Investigator (PI) of this application proposes to develop a rich data environment to support  reuse of historic annotated patient records for case-based reasoning (CBR) to enhance traumatic brain  injury (TBI) classification and clinical trajectory assessment and provide management guidance. The goal  of this program is to facilitate complex decision-making with informative and easy-to-find multimodal  information. Four research projects are included: (1) Findability - Selecting Cases for Informed Decision Making, (2) Accessibility - Optimizing Knowledge Transfer, (3) Interoperability - Augmenting Clinical  Workflows, and (4) Reusability - Growing a Shared Library of TBI Cases.  \n",
      "\n",
      "\n",
      "SCORED CRITERIA  \n",
      "Overall Program \n",
      "Average Score: 5.3 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The significance of this program is very strong. Harmonizing the clinical care of previous TBI  patients through the use of universally searchable metadata will allow clinicians at any stage to care for  their patients. The investigators also have a clear understanding of the models to be used to create such  harmonized metadata. Overall, the program is both significant and innovative in its respective goals. The  investigators are proposing to address a relevant problem by creating a database that is searchable across  their host of partnering institutions, revising the care based on individual cases, and informing future care.  The projects fit within the focus areas, and they are synergistic.  \n",
      "Weaknesses: The goals of the program, however, are very optimistic. The investigators, though repeatedly  admitting that TBI is heterogeneous, provide no evidence on why they hypothesize that TBI patients with  a similar trajectory will have a similar end point. They show some preliminary data from homogeneous  pathological cases that establish that the investigators know how to create such a CBR but provide no  evidence if such a pattern will repeat in heterogeneous TBI cases. Since the investigators have access to a \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  (Page 4 of 36)  \n",
      "rich multi-institutional team including their own CONNECT system, they should provide at least some  level of evidence that their hypothesis has a reasonable chance of being proven true.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The overall goal of Case-Based Reasoning to Inform Clinical Trajectory in a Collaborative and  Augmented Learning Environment for Traumatic Brain Injury (CRITICAL-TBI) is to create and deploy a  system that enables precision management of TBI patients using case-based reasoning. Addressing the  challenge of making data reusable and exploring the multidimensional characterization and trajectory of  brain injury is in alignment with each of the FY23 TBIPHRP focus areas: understand, prevent and assess,  and treat. It aims to address the current gap in TBI patient care with respect to use and reuse of data based  on CBR.  \n",
      "Weaknesses: An overall weakness of the program is that the research projects are overly dependent upon  each other. Project 4 is the capstone project that ties all the 3 subprojects together. If any of the  subprojects fail, this inherently implies the failure of the overall project. Project 3 assumes a successful  implementation of Project 2.  \n",
      "Discussion Notes  \n",
      "Reviewers agreed that the overall program addresses several challenges and multiple subareas within the  FY23 TBIPHRP focus areas. This research program was considered, by all reviewers, to be an extremely  ambitious high-risk/high-reward program. Reviewers’ enthusiasm was dampened by a clear  interdependence of the individual projects and the overall structure of the program, most notably for  Project 4, which directly relies on the success of all 3 prior projects. It was unclear to reviewers how each  project would independently contribute toward knowledge while also informing and contributing to a  broader understanding of the dynamics of brain injury and working synergistically to improve  management of severe TBI patients.  \n",
      "Leadership \n",
      "Average Score: 7.3 \n",
      "Scientist Reviewer A  \n",
      "The PI of this application is Mr Richard Moberg, founder and chief executive officer of Moberg  Analytics, Inc. Mr Moberg received an MSE in biomedical engineering from the University of  Pennsylvania in 1976.  \n",
      "Strengths: The overall program will be headed by Mr Moberg, who is an extremely successful innovator  studying TBI and has years of experience in understanding TBI and multimodal monitoring. He has  assembled a great team of researchers who will be independently successful in carrying out their  respective projects. The PI and project leaders are all well qualified and will be able to approach the goals  of the program.  \n",
      "Weaknesses: The entire program rests with Mr Moberg and Mr Maddux, which is a little concerning when  it comes to validating and distributing their completed version for a wider use.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The PI, Mr Moberg, has been in the neurocritical care industry for over 45 years as an  engineer, innovator, and business owner. He has notable experience bringing technology from idea \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  (Page 5 of 36)  \n",
      "conception, through development, to commercial use. He is a pioneer in the technology and applications  of multimodal neuromonitoring and is known internationally for these efforts. He has a wealth of  experience with leading large-sized funded projects from the NIH and DOD and has translated  this research into widely used products such as CONNECT. He has amassed a strong team of researchers  with expertise in TBI neurocritical care and software development. The majority of the team members  have an established working relationship with prior funded projects.  \n",
      "Weaknesses: A minor concern is with respect to the time effort of 1.2 calendar months allocated for Mr  Maddux. This seems low compared to his level of commitment in serving as a project lead for 3 of the 4  projects. Though the overall program lead team is strong, the lead team for Project 3 is weakly connected.  \n",
      "Discussion Notes  \n",
      "Reviewers agreed that the key personnel involved in this application, including Dr Amorim, Dr Foreman,  Dr Kim, and Dr Diaz-Arrastia, represent a strong team of researchers with appropriate complementary  expertise for the program. However, the strong emphasis on the role of Mr Maddux, chief technologist at  Moberg Analytics, as a leader for Projects 1 to 3 with a dedicated 10% effort, total, was noted as a major  weakness. It was not clear to reviewers how the leadership team would work together to achieve the  overarching goals of the proposed effort.  \n",
      "Implementation Plan \n",
      "Average Score: 4.5 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The proposed projects are supported by an implementation plan that identifies critical  milestones and outlines the knowledge, resources, and technical innovations that will be utilized to  achieve the milestones.  \n",
      "Weaknesses: It isn’t very clear how the cores would be generated and/or utilized to approach the overall  goal of the program. A detailed explanation of how individual project performances will be assessed  during the award, and mostly what are the procedures to rectify an issue when that happens seems to be  lacking. There is no mention of how the results will be communicated to the partnering institutions and  how consenting happens with the patients. This will also affect the overall communication for flow of  ideas among the team members. There is no explanation of creating standard operating procedures (SOP),  but that is not relevant at this stage as this is only the inception of the program.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The team has adequate resources and institutional support for this program. They have stated  data sharing plans among 6 participating sites. The statement of work outlines their implementation plan,  milestones, and tentative timeline per project. Mr Moberg, with his vast entrepreneurial leadership  experience, will take the lead on the overall program organization.  \n",
      "Weaknesses: There are no regularly scheduled overall team meetings across the 4 projects. According to  the timeline under statement of work, the project management meetings will be as scheduled. This raises  concerns about availability of opportunities for consistent communication and flow of ideas and feedback  among the team members and subprojects. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  (Page 6 of 36)  \n",
      "Discussion Notes  \n",
      "Reviewers agreed that the implementation plan lacks details related to a consistent and intensive exchange  of information among all team members. The feasibility of the studies, as proposed, was questioned given  the aggressive timeline provided (4 years).  \n",
      "Overall Impact and Relevance to Military Health \n",
      "Average Score: 7.0 \n",
      "Scientist Reviewer A  \n",
      "Strengths: When successful, the program will improve understanding and the treatment of individuals  with TBI. The long-range vision will impact both the field and a broader community. The program is  responsive to the health care needs of the veterans.  \n",
      "Weaknesses: Since the actual program is just collating the current care, it is unclear if it can make any  changes to the improvement of the clinical guidance in the short term. The study does not identify any  potential issues that might occur with the individual projects or their collaboration.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: This program, if successful, is of high impact and relevance to military health as it will ensure  continued transfer of knowledge and mining of information to support today’s clinical decision making. It  will leverage the use of existing data sets and DOD-funded grants’ products such as CONNECT.  \n",
      "Weaknesses: An overall concern is that this could result in an overreliance of prior knowledge or stifle  room for new innovations and/or methods for TBI patient care. It is important to note that there are  usually outliers in every data set. The notion of “similarity” could be relative. The model should be able  to account for null findings. This is why ethics of artificial intelligence (AI) need to be a key component  of this program in its application to military health; this appears to have been overlooked.  \n",
      "Consumer Reviewer  \n",
      "Strengths: By leveraging existing/historical patient data and recent advances in AI and LLM technology,  the proposed aims of the overall program (CRITICAL-TBI) are likely to be of high impact to the field of  TBI and contribute to novel discoveries, advances in scientific knowledge, and improve the  understanding, assessment, and treatment of TBI. The long-range vision of the proposed research is of  high impact to both the field of study and also represents a potential revolution to and transformation of  patient management and care. Through implementation of the advisory board (patients with TBI), the  program is directly responsive to health care needs of TBI patients; similarly, needs of clinical staff are  incorporated into the technology design at each partner institution. Multiple components of the overall  program would be immediately translatable to military health care.  \n",
      "Weaknesses: Some application components reduce enthusiasm for the proposed program as a whole.  Throughout all projects in the proposal, potential pitfalls and challenges are only cursorily discussed and  lack sufficient detail; at times, potential risks are acknowledged and raised without providing a mitigation  strategy (eg, Project 1 “curse of dimensionality” and “Bias in data set” [bias in particular appears to be  discussed as an important feature to be explored rather than a risk to mitigate]). While military  applications and impact are readily apparent throughout the proposal, these impacts are all long-term  vision and would require additional adaptation, implementation, and testing for mobile or in-the-field  applications in particular. The current commercialization plan is to first utilize government funding to \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  (Page 7 of 36)  \n",
      "develop the technology and then “sell back” this same technology—once fully realized—through a  subscription-based model. Though potentially revolutionary, the impacts would be limited to clinical  users as well as patients in hospitals/institutions with subscription based licenses; it would seem therefore  that the technology developed would be of high-impact but perhaps only to a select few. It is unclear if  lesser-resourced institutions (rural American communities, low-income neighborhoods, global developing  countries) will be left behind. Again, while potentially revolutionary, it might also further drive health  disparity both domestically and globally.  \n",
      "Community-Based Participatory Research \n",
      "Average Score: 6.3 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The investigators have plans to incorporate relevant partners from the community, but this is  only described in passing.  \n",
      "Weaknesses: The investigators will collaborate with the Mind Your Brain Foundation, and there are letters  of support from 2 additional TBI survivors. While there is an effort to involve the community in the  initiatives, it will benefit with active collaboration with all the project leaders, feedback, expansion among  a wider community, and incorporating those suggestions into the program.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The team plans to set up a community advisory board through partnership with the Mind Your  Brain Foundation. They have included some letters of support from community members who have  suffered from TBI. It appears that the CBPR team will be integrated with Project 1.  \n",
      "Weaknesses: The CBPR component of the program is weak. Though a community advisory board will be  set up, it appears that their input will be primarily applied in Project 1. The other projects do not address  the CBPR component. It raises some concerns and casts doubts on the significance of the input from  CBPR team, since the meetings will be led by the postdoctoral researchers rather than the PI and/or  project team leaders.  \n",
      "Discussion Notes  \n",
      "Reviewers commended the building of a community advisory board (CAB) of lived experience  consultants (LECs) to engage stakeholders in the program. However, reviewers indicated that CAB  collaboration seems to be limited to Project 1. One reviewer further noted that occupational therapists,  physical therapists, and speech therapists are a vital part of the clinical team, beginning acutely and  remaining through discharge to facility or home, and should be included to establish data point collection  to predict functional outcomes.  \n",
      "Data and Research Resources Sharing Plan \n",
      "Average Score: 5.3 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The application does intend to capture or create new common data elements (CDEs) as a part  of their proposal.  \n",
      "Weaknesses: The data sharing plan is loosely described. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  (Page 8 of 36)  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The overall goal of the program is to have a system of sharing siloed data among participating  sites without the data leaving the individual sites. For an external user to utilize the system, they have to  be a part of the data consortia and contribute to database. The idea of leveraging existing data especially  the public data repositories is commendable.  \n",
      "Weaknesses: The amount of commercialization efforts involved in the transition plan raises concerns  about limited access to the research community.  \n",
      "UNSCORED CRITERIA  \n",
      "Budget  \n",
      "Scientist Reviewer A  \n",
      "The direct costs do not exceed the allowable direct costs and are appropriate for the scope of the program.  Scientist Reviewer B  \n",
      "The budget appears to be sufficient to complete the studies proposed.  \n",
      "Application Presentation  \n",
      "Scientist Reviewer A  \n",
      "The writing of the proposal did not influence the review of the application.  \n",
      "Scientist Reviewer B  \n",
      "Overall, the proposal was well written. It provided a compelling case and motivation for why the field  needs a CBR model implemented in a findable, accessible, interoperable, and reusable (FAIR) framework  to advance to the next level of TBI patient care management. The major weaknesses were in the research  strategy of some of the projects and overdependence of the subprojects.  \n",
      "Consumer Reviewer  \n",
      "The application is well developed, clearly presented, and easily readable, with attention to detail  throughout. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  (Page 9 of 36)  \n",
      "Project 1: Findability - Selecting Cases for Informed Decision Making  \n",
      "OVERVIEW  \n",
      "The Project Leaders of this project propose to (1) establish a growing, indexable record of annotated TBI  cases to inform possible clinical trajectory and (2) validate CBR as a clinical tool to inform TBI  management and care. The specific aims are (1) software modules for data curation and feature  engineering, (2) indexable record of patient information, (3) method development for multimodal patient  similarity combining discrete and time-series data, (4) method development for defining and quantifying  clinical trajectory, and (5) validation of CBR for clinical TBI care.  \n",
      "\n",
      "\n",
      "SCORED CRITERIA  \n",
      "Research Strategy and Feasibility \n",
      "Average Score: 5.3 \n",
      "Scientist Reviewer A  \n",
      "Based on a data set of 300 samples obtained from the University of California, San Francisco (UCSF), the  project leaders will build the indexable database and design the methods using a train set (80%) while the  reminder 20% will be used as the validation set for testing. This work leverages the existing cloud-based  data management system for National Coordinating Center—New Medical Record for the Brain (NCC CONNECT). They will design software modules that can curate features from a diverse set of data drawn  from multimodal sources (physiology, imaging, medications, multiomics, blood-based biomarkers) based  on 2 timepoints: intensive care unit (ICU) admission and after 24 hours of monitoring. There are 6  modules: patient (structured patient data such as Glasgow Coma Scale [GCS], sex, age, lab data),  computed tomography (CT) (epidural, subdural, etc), quantitative electroencephalogram (EEG),  quantitative multimodal neuromonitoring, medication administration, multiomic and natural language  processing. These features will be amassed in a multilevel PostgreSQL database to facilitate extraction,  transformation, and loading in Python. The features determined suited for computing patient similarity  will be based on both empirical analysis and clinical perspectives (by involving the panel of experts and  project leaders). The empirical analysis will employ local sensitivity hashing (LSH) and Gaussian mixture  models (GMM) for multimodal trajectory matching. The clinical trajectory is determined based on a set of \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 10 of 36)  \n",
      "meaningful intermediate end points as well as final outcomes such as GOSE. Panel will also inform  selection of end points. The validation of CBR is based on the candidate feature set derived from  admission data along with the established intermediate end points. Using the test set, the system will be  employed to find the most similar patient and compare their trajectory based on the end points. The  neuromonitoring validation is conducted separately.  \n",
      "Strengths: The aim of Project 1 is to find similar TBI cases to inform clinical decision making for a new  patient case. The scientific rationale presented by the team is compelling, given that CBR isn’t new and  has been applied in clinical care such as breast cancer diagnosis. This project is relevant to FY23  TBIPHRP FPA focus area of “understand,” in understanding the role of genetics, endophenotypes, health  demographics, mechanism of injury, pathophysiology, and environmental factors in TBI clinical trajectory  based on a candidate set of features obtained as well as while modeling current trajectory. Based on  preliminary results presented, the team has demonstrated expertise in dimensionality reduction methods  as well as developing a novel technique for finding similar patient trajectories based on EEG data. The  team gives consideration to feedback from a panel of experts and Project Leaders to guide selection of  curated features as well as trajectory intermediate end points. The team reports a sample size calculation  to justify the sample size of 300. Dr Amorim and team have developed a dimensionality method (pairwise  controlled manifold approximation [PACMAP]) that utilizes EEG data. They have also employed  dynamic time-warping (DTW) for trajectory similarity matching of spikes for intracranial pressure (ICP)  crisis or seizures. It is exciting to think about the potential of this project, if successful, in accurately  identifying similar patients based on a wide range of data (demographics, injury data, labs, neuroimaging,  blood biomarkers, EEG) so that clinicians can be informed in real time about current patient cases.  Potential problem areas have been identified such as huge time complexity when searching and indexing  multiscale data and curse of dimensionality. The research team is very strong, they have an established  working relationship, and have obtained funding in related areas which has resulted in meaningful  outcomes such as CONNECT (which they plan to leverage in this project).  \n",
      "Weaknesses: The feasibility of the proposed strategy raises some concerns. For this to be successful, it is  essential to have the right features for determining similarities as well as appropriate method for  computing similarities. The investigators plan to explore various similarity search approaches that are  suited to different data modalities such as time series or learned embeddings. The preliminary results  presented only demonstrate one aspect of the types of data that would be collected (EEG). However, it  isn’t clear how the various multimodal data collected will be fused to design an overall patient trajectory  similarity score. Based on the methods employed, multiple similar patients’ cases might be obtained. How  the information will be compiled from these multiple cases to provide meaningful information for the care  team is not well described. CBR involves 4 steps: retrieve, reuse, revise, and retain. The team elaborates  on the process of retrieving (building database and feature engineering) and reuse (computing patient  similarity), but it isn’t clear how the steps of revise and retain are dealt with. The risk of scalability isn’t  addressed, given the fact that the size of sample data is really small compared to 25,000 patient cases that  need to be amassed for the CBR to be effective. Though they identify potential problem areas such as  curse of dimensionality, they do not address how they will mitigate the issue. Some items on the list of  relevant publications and/or patents are not affiliated with the project team.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The project fits perfectly within the intention of the funding mechanism. The proposed  approach toward generating the harmonized CBR is theoretically well-reasoned and the Statement of  Work (SOW) reflects these tasks. The research requirements of the project are supported by the  availability of and accessibility to facilities and resources including patient populations, samples, and  collaborative arrangement. The patient population proposed to develop their methodology is appropriate  for this study. The investigators have access to the proposed sites. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 11 of 36)  \n",
      "Weaknesses: There are no preliminary data relevant to the population the investigators want to study.  The investigators only outline the theoretical concepts of the approach they intend to develop in this  project. Hence, it is difficult to assess whether the proposed methods may approach the proposed  hypothesis. Any risk-mitigating approach in case their proposed approach does not work is not identified.  The proposed project is highly optimistic and cannot be completed within the duration of the project.  Though the patient population proposed is appropriate, it is difficult to assess how much of the  homogeneity of the cases exists within themselves to develop a CBR. The investigators do not explicitly  outline how they plan to collect the future specimens. It is difficult to assess whether the data collection  instruments are appropriate to the proposed study. The investigators have not outlined how the previous  cases and the future cases will be consented. The proposal does not identify any potential barriers to  accrual and provide mitigation plans for addressing unanticipated delays. Inclusion and exclusion criteria  including the distribution of the proposed enrollment or data set based on sex, race, or ethnicity are not  described, and whether there will be sex-specific training for CBR is also not described, though it is  known there are sex differences in TBI outcomes.  \n",
      "Impact \n",
      "Average Score: 7.6 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The potential outcome of a system that is able to identify historic patient cases with similar  characteristics via CBR and apply it to predict the clinical trajectory of a current patient would be  beneficial in the short-term care of patients. It should accelerate evidence-based patient care. If the proper  care is administered to the patients based on ease of clinical decision making, this would result in long  term improved quality of life for the patients.  \n",
      "Weaknesses: Concerns are the innovation might be stifled by the overreliance of historic patient data  information. It could lead to bias in clinical care and possibly hinder growth of medical innovation. In  addition, there is no consideration for null cases, whether it is possible that a current patient case might  not map sufficiently to a historic case. In that case, the system shouldn’t be biased to return the closest  case. There should be a confidence measure of the level of similarity with returned cases.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: There is an unmet clinical need to use the prior experience of clinicians to inform future care  and hence the study is impactful and important. If successful, the long-term benefits will significantly  impact patient care and quality of life.  \n",
      "Weaknesses: The investigators do not clearly outline how the proposed research will provide or improve  short-term benefits for individuals. This is a minor weakness as it is a start and when successful, the  project will be able to provide for those short-term benefits.  \n",
      "Consumer Reviewer  \n",
      "Strengths: If successful, the proposed project is of high impact. The project fits well into multiple topic  areas of interest and addresses multiple challenges in TBI research and clinical care including  understanding and treatment. By leveraging both patient heterogeneity as well as advances in machine  learning, Project 1 aims to develop tools to better classify TBI based on real-world case similarities and  with potential for predictive value for patient trajectory. The potential for the technology to synergize with  other findings (ie, biomarker identification) is also a major strength. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 12 of 36)  \n",
      "Weaknesses: Direct short-term benefits to individuals are a bit unclear. A potential short-term impact they  propose is improving upon current mild, moderate, and severe TBI classification; however, how this will  be achieved with the 2 time points assessed (within 48 hours of injury) is unclear. This is a moderate  weakness.  \n",
      "Relevance to Military Health \n",
      "Average Score: 7.5 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The long-term vision is to perform real-time CBR continuously from the point of injury to  critical upstream time points during care, not just 2 main points. This technology could be implemented  on the battlefield. Software built out in the civilian side (eg, data modules, transformation modules, AI  modules) can be translated to field environments for measurements that are routinely collected in both  environments. This method can be applied to any environment where data are in abundance, not just acute  TBI. By fully integrating precision medicine into the care pathway of a TBI patient or veteran, its effects  longitudinally can be investigated from the point of injury to the patient in the VA hospital, who is  experiencing the long-term effects, decades later.  \n",
      "Weaknesses: Time complexity is alluded to, as a potential problem area in the project description. This  would be a priority in addressing the feasibility of the system on the field for use by military personnel.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The proposed research complements DOD areas of research interest for TBI. The proposed  approach can provide dual benefits for both civilian and military population.  \n",
      "Weaknesses: It is unclear if the proposed research will enhance readiness and recovery on the battlefield,  during training, or in resource-limited environments as it is designed as a clinical care project.  \n",
      "Consumer Reviewer  \n",
      "Strengths: The proposed research project complements multiple areas of DOD and/or VA areas of  research interest. If successful, the proposed research is highly likely to enhance readiness and recovery  on the battlefield and/or during training. Further, the developed platform could be easily implemented in a  dual-use capacity, and even modified to have impacts beyond psychological health conditions and/or TBI.  \n",
      "Weaknesses: The feasibility of the long-term military application of the implemented technology, which  relies on Bluetooth technology, may not be possible in theater. This is a moderate weakness.  \n",
      "Regulatory Strategy and Transition Plan \n",
      "Average Score: 7.6 \n",
      "Scientist Reviewer A  \n",
      "Strengths: An intended transition plan for the software developed in Project 1 is commercialization via  Moberg Analytics.  \n",
      "Weaknesses: No regulatory strategy is presented. There is no discussion of a potential risk analysis for  cost, schedule, manufacturability, and sustainability of project software for findability. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 13 of 36)  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The transition plan is well-defined, and no additional funding will be sought.  \n",
      "Weaknesses: The entire software will rest with Moberg Analytics with all the potential data sources; that  raises concerns about how the software will be distributed in the community when realized. There is no  plan to involve external validation to verify the authenticity of the CBR.  \n",
      "Technology Transfer Specialist Reviewer  \n",
      "Strengths: The Transition Plan includes a roadmap to achieve the overall goal of launching a case-based  reasoning (CBR) system to inform clinical practice for diagnosis and treatment of TBI as well as  development pathways for the 4 component projects. The well-conceived transition plan for Project 1  relies on the existing partnership with Moberg Analytics. The new “findability” tools to capture and  characterize data to inform physician’s management of current patients will be deployed through  Moberg’s existing CONNECT product.  \n",
      "Assuming success of phase 1, the next development step (phase 2) is achievable. Investigators articulate  the steps for commercial deployment, as well as a defined regulatory pathway. The funding and partnering  plans are well conceived. The intellectual property (IP) strategy is well-defined and provides a defined  path for commercial development. The timeline and milestones are optimistic, but feasible assuming the  project progresses as expected. The risk analysis is well conceived.  \n",
      "Weaknesses: The successful transition of Project 1 relies exclusively on the existing partnership with  Moberg Analytics, with no backup plan discussed, which could be risky when seeking broad  dissemination of the software. A minor weakness, the plan could be improved by a more robust discussion  of possible ways to leverage DOD funding and collaborations in future development phases. The  “Regulatory Strategy” section of the application includes no information, but other parts of the  application outline plan for human subjects research and IRB approvals.  \n",
      "Statistical Plan \n",
      "Average Score: 6.5 \n",
      "Scientist Reviewer A  \n",
      "Strengths: A sample size calculation is included to justify the sample size of 300 having sufficient power.  \n",
      "Weaknesses: No inference is made to how that sample size is sufficient to serve as a prototype for 25,000  patients. There is no indication of statistical analysis of identified cases with respect to confidence level of  similarity.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The statistical plan is hard to assess as the statistics are based on a relatively homogeneous  database.  \n",
      "Weaknesses: No significant or relevant statistical tests are described to validate the project. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 14 of 36)  \n",
      "Biostatistician Reviewer  \n",
      "Strengths: The study design strategy is reasonable with independent training and validation samples.  What features to extract and how they will be extracted are clearly presented. The database will be  structured to have hierarchical layers to facilitate quick feature search. Both expert engineered features  and data-driven features will be extracted to cover a wide range of data information.  \n",
      "Weaknesses: The sample size/power is not correctly justified. The total sample size N = 300 will be split  into training (N1 = 240) and validation (N2 = 60). Many features will be extracted and evaluated in Aims  1.1 through 1.4. A justification for why N1 = 240 patients is sufficient to extract meaningful features is  not provided. Similarly, a justification why N2 = 60 patients is sufficient to validate features from Aim 1.3  is not provided. The method to find the most important intermediate end points is less developed.  Summary statistics and correlations do not adjust for many potential confounding variables such as age  and TBI severity. In the Aim 1.5 validation study, both tasks 1 and 2 will refine features using recursive  feature elimination method. However, what criteria will be used in feature elimination is not provided.  \n",
      "Personnel \n",
      "Average Score: 9.2 \n",
      "Scientist Reviewer A  \n",
      "Co-lead Dr Edilberto (Eddy) Amorim is assistant professor of neurology, Weill Institute for  Neurosciences, UCSF. Edilberto Amorim received an MD in 2008 from Escola Bahiana de Medicina,  Brazil. He completed a postdoctoral fellowship in neuroscience and machine learning at  MIT/Massachusetts General Hospital.  \n",
      "Co-lead Dr Dmitriy Petrov is assistant professor of neurosurgery, University of Pennsylvania (UPenn),  Philadelphia. Dmitriy Petrov received an MD in 2007 from Rutgers Medical School, New Brunswick.  \n",
      "Co-lead Mr Craig Maddux is chief technology officer at Moberg Analytics. Mr Maddux obtained a BS in  computer science from California State University in 1986 and an MA in film from Staffordshire  University, United Kingdom, in 2016.  \n",
      "Strengths: The Project 1 team will be co-led by Dr Edilberto Amorin (feature set curation and algorithm  development), Dr Dimitriv Petrov (clinical evaluation of CBR), and Mr Craig Maddux (software  engineering tasks). Mr Maddux is a programmer, database administrator, and data engineer and has  specialized in data and AI for neurocritical care since 2014. Currently, he is the main lead of top-level  strategy of Moberg Analytics cloud-based electronic medical record (EMR). Dr Petrov has both clinical  and research expertise in TBI. Dr Amorim is board-certified neurologist with subspeciality training in  neurocritical care and epilepsy. He has a patient-oriented research program in TBI neuromonitoring at  UCSF for posttraumatic epilepsy prevention using big data analytics. He has experience developing  harmonization software focused on physiology waveform, EMR, and imaging data. He has assembled and  harmonized EEG data for more than 1,000 subjects in the Critical Care EEG Monitoring Research  Consortium (CCEMRC) cardiac arrest taskforce. Dr Amorim is in charge of patient monitoring at  Zuckerberg San Francisco General Hospital (ZSFG) and is well positioned to provide access to the large  clinical and physiology database of TBI patients that his team has curated. In addition to his clinical role,  he has a computational research laboratory located at ZSFG, which is composed of engineers, computer  scientists, bioinformaticians, and research coordinators dedicated to clinical research in NCC. This is a  strong project team with an established working relationship. Moberg Analytics is close in proximity to  UPenn. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 15 of 36)  \n",
      "Weaknesses: There are concerns about the feasibility of Dr Petrov to conduct clinical evaluation of results  generated from USCF.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: Mr Maddux will complete software engineering tasks at Moberg Analytics. Dr Amorim is an  assistant professor of neurology at UCSF and will lead the feature set curation and algorithm development  for selected modalities. Dr Petrov is an assistant professor of neurosurgery at University of Pennsylvania  and will lead the strategy of the clinical evaluation of CBR. The team is experienced and can carry out the  proposed project.  \n",
      "Weaknesses: No weaknesses were noted. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 16 of 36)  \n",
      "Project 2: Accessibility - Optimizing Knowledge Transfer  \n",
      "OVERVIEW  \n",
      "The Project Leaders of this project propose to (1) develop a plug-and-play report template format that  allows for components to be added or removed on-demand depending on the use case, (2) design a nurse physician reporting workflow for effective communication among the care team, and (3) validate specific  use cases with the clinical team. The specific aims are to (1) build a pallet of components for the report  template, (2) design a report template format, and (3) create and validate a nurse-physician reporting  workflow.  \n",
      "\n",
      "\n",
      "SCORED CRITERIA  \n",
      "Research Strategy and Feasibility \n",
      "Average Score: 5.2 \n",
      "Scientist Reviewer A  \n",
      "This project proposes to develop software that will be designed to create a report structure that can be  shared across nurses or, in theory, across different neurocritical care units. The goal is to improve patient  care as the software is proposed to be designed as a graphical report that might be more useful than the  tabular reports currently generated. In Aim 1 a template will be designed, and a prototype will be  implemented. In Aim 2, a report template will be designed using known requirements and then expanded  to build a final PDF. In Aim 3 the report generator will be implemented and tested using simulated data.  \n",
      "Strengths: Creating a graphical report that can be used as a replacement for traditional EMR that is  usually either incomplete or prone to errors, especially for neurocritical care that has a strong time  dependence, is very significant. The project fits perfectly within the intention of the funding mechanism.  The proposed approach toward generating the reports is well-reasoned, and the SOW reflects these tasks.  The investigators have access to the proposed sites. Inclusion and exclusion criteria, including the  distribution of the proposed enrollment or data set based on sex, race, or ethnicity, are not described but  are not relevant to this project. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 17 of 36)  \n",
      "Weaknesses: It is difficult to assess, based on how the proposal is written, how much of this is done, how  difficult it is to create user inputs on the fly, what might happen if there is an error, how that error is  corrected, and how the reports are validated. It is difficult to assess whether the research requirements of  the project can be supported by the availability of and accessibility to facilities and resources, including  patient populations, samples, and collaborative arrangements. The patient population, on which the report  generation will be prioritized, is not discussed. There are no preliminary data relevant to the population  the investigators want to study. The investigators only outline the theoretical concepts of the approach  they intend to develop in this project. Hence, it is difficult to assess whether the proposed methods may  approach the proposed hypothesis. A risk-mitigating approach in case their proposed approach does not  work is not identified. Though the patient population proposed is appropriate, it is difficult to assess how  much of the homogeneity of the cases exists within themselves to develop harmonized metadata that can  be shared across institutions.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: There is well established rationale for the need of the proposed system; it can significantly  increase the accessibility of data to health care providers (overcoming the difficulties of EMR) and  decrease the potential of errors and miscommunications during patient’s handoff. This project is highly  relevant to FY23 TBIPHRP focus areas: (1) “Understand: Development and analysis of communication  and tools/technology adoption that would facilitate clinical translation and identification of risk factors,  educational barriers, social determinants of health, and other factors that may impede clinical translation.”  The novel communication technology of this application will be greatly beneficial for displaying  multitude of clinical data/information about the patient status that should be of great help to the  management of NCC patients. (2) “Treat: Personalized medicine approaches to treatment that may  include tailoring treatment to the biological and endophenotypic elements present.” The proposed  approach offers the opportunity to better personalize treatment by providing clinically relevant data in an  easy to understand and interpret fashion from individual patients.  \n",
      "Well designed and appropriate tasks for each of the 3 aims of the project are described (components of the  report, report template format, and report workflow). There is strong relevant previous/ongoing work that  should increase the likelihood of success: The software system of this application will build off the  applicants’ previous DOD-sponsored grant that has resulted in a cloud-based data management system for  NCC, called CONNECT. They have stated that this system (CONNECT) has entered the  commercialization phase as of September 2023. Supportive preliminary work where they have adapted  their CONNECT system to display patient status in a graphical fashion with data from their collaborative  sites University of Cincinnati (UC) and University of Texas Southwestern (UTSW) is presented.  \n",
      "Weaknesses: The discussion on the potential ways that their software module would work alongside the  EMR is vague and inadequate. For example, it is unclear whether it is a completely independent module,  or if there is communication with the EMR. There is difficulty in assessing preliminary work, due to the  \n",
      "very low-quality/low-resolution figures. There is inadequate description of the types of data in the  preliminary work shown in Figures 9 to 13 (particularly when the labels of plots in figures are hard to  see). They plan to generate the modules of the report via consultation with their collaborators at the  University of Cincinnati (UC) (Dr Foreman) and University of Texas Southwestern (UTSW) (Dr Olson);  however, it would be helpful if they provide more information about the types of data that the display  module will contain, for example, whether they plan to include any type of brain activity data (eg, EEG  features). If so, what features of the EEG they plan to display, etc, should be described. It is difficult to  assess the exact contribution (percentage of effort) and effort of the PI and collaborators for the Project 2  alone; the budget and its justification are apparently for the entire project (1 to 4) and not specific to  Project 2. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 18 of 36)  \n",
      "Impact \n",
      "Average Score: 7.9 \n",
      "Scientist Reviewer A  \n",
      "Strengths: There is an unmet clinical need to use time dependence of the measures in care to inform  patient care, and hence it is very impactful. If successful, the long-term benefits will significantly impact  patient care and quality of life.  \n",
      "Weaknesses: The investigators do not clearly outline how the proposed research will provide or improve  short-term benefits for individuals. The investigators do not outline based on their own experience or data  set, how much error was introduced due to the absence of such a report, and the absence of this  information is a major flaw.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: Project 2 alone will have a significant impact in management of patients with TBI by providing  more complete and easier to interpret instantaneous and trend data for each patient as well as for the data  pertinent to the patient population. The software prototype will address the shortcomings of the current  EMR by allowing the specific and useful multisensor/multidimensional information about each patient to  be displayed in a customized and easy to follow manner. The software module can potentially be highly  effective in increasing the accessibility of data to health care providers at many scales, and also decrease  the potential of errors and miscommunications during patient’s handoff. The modular structure of the  report will allow integration and display of additional and new data (eg, new brain monitoring devices) as  they become available and/or adopted in the management of TBI.  \n",
      "Weaknesses: The software module of this application appears to be mostly beneficial for NCC and for  short term monitoring of the patients following a brain injury. However, it is not clear whether/how this  system can be utilized for longer-term TBI management (such as after severe TBI), which would increase  the impact of the project. It is presumed that long-term monitoring of TBI patients will require  additional/different types of report modules that would encompass, for example, measurements of  neurocognition and neuropsychological abnormalities (eg, depression and PTSD). It is unclear if the  infrastructure of the software is designed for such expansion.  \n",
      "Consumer Reviewer  \n",
      "Strengths: The outcomes of this project will have immediate and significant impacts to current clinical  workflows in the neuro-ICU at participating sites, addressing a major unmet need to alleviate reporting  burdens and facilitate care team communication and treatment guidance. If successful, the development of  novel communication methods as a result of the completion of the aims could greatly increase  accessibility and usability of patient data, potentially supplanting traditional EMRs which are insufficient  with regard to accessibility and usability, particularly as they relate to TBI clinical care. Report sharing  across or within teams would be of high impact to both patients and care teams alike and mitigate  miscommunication events during patient handoff.  \n",
      "Weaknesses: As much of the project is focused on overcoming errors due to miscommunications, the  application could be improved by incorporating an outcome (even as simple as surveys among  nurses/users) or readout that would measure success of the tool.  \n",
      "Relevance to Military Health \n",
      "Average Score: 7.3\n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 19 of 36)  \n",
      "Scientist Reviewer A  \n",
      "Strengths: The proposed research complements DOD areas of research interest for TBI.  The proposed approach can provide dual benefits for both civilian and military populations.  \n",
      "Weaknesses: It is unclear if the proposed research will enhance readiness and recovery on the battlefield,  during training, or in resource-limited environments as it is designed as a clinical care project.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The software module for monitoring patients in NCC will have immediate application in the  VA specialized clinics and trauma centers. They expect that upon commercialization of the product, it will  become immediately applicable in Roles 3 and 4 in military health. Furthermore, individual components  of the overall system can be used in Role 1 (medical record) and Role (telehealth). The plan is to develop  more specific military related aspects of the project in phase 2 (following the termination of this 4-year  project).  \n",
      "Weaknesses: There is no planned input from the military health establishments and VA facilities for this  application. Such input would allow the design of the report modules to have the needed flexibility for  adding military/VA specific report items. The circumstances leading to brain injury in the military are  different than those encountered by the civilian population (eg, battlefield/military exercise injuries)  which often result in comorbidities such as depression, PTSD, and sleep disorders. It would be highly  beneficial for the software module to be developed to have the flexibility of analyzing/presenting data  reflecting such comorbidities.  \n",
      "Consumer Reviewer  \n",
      "Strengths: The project complements multiple DOD/VA areas of research interest as well as patient care  for TBI. The dual-use nature of the technology and resulting information are readily apparent, benefiting  civilian populations and military needs alike. Reducing miscommunication during patient handoffs will  likely result in fewer human-related errors that negatively impact patient health; these impacts are likely  to be even more pronounced in a military/field setting where environments can be more chaotic than in a  civilian hospital.  \n",
      "Weaknesses: Relevance to military health could be expounded upon with more details; input from  military or veteran population would be valuable to improve the approach and vision for military  relevance. It is unclear how this would be used in battlefield settings.  \n",
      "Regulatory Strategy and Transition Plan \n",
      "Average Score: 7.7 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The transition plan is well-defined.  \n",
      "Weaknesses: The entire software will rest with Moberg Analytics with all the potential data sources; that  raises concerns about how the software will be distributed in the community when realized. There is no  plan to involve external validation to verify the authenticity. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 20 of 36)  \n",
      "Scientist Reviewer B  \n",
      "Strengths: There are well planned/presented translational and funding plans for product development and  commercialization. The current application will be followed by a follow-up effort targeting the collection  of 1,000 highly annotated cases by partnering with clinical trials as well as the collaborators for this  application. Following this phase, they plan to offer the system for clinical use utilizing a subscription  model where hospitals contribute data and also pay to use the system of this application. For regulatory  approval, they plan to follow the FDA Final Guidance on Clinical Decision Support Software and use a  Software-only Medical Device Data System classification in their future FDA application. There is a  well-planned commercialization strategy: The software developed in this project will be commercialized  separately through their current product CONNECT as an additional feature. They plan to explore  obtaining reimbursements for this software module from Centers for Medicare and Medicaid Services  (CMS) which would significantly increase the commercial opportunity of this software module.  \n",
      "Weaknesses: No weaknesses were noted.  \n",
      "Technology Transfer Specialist Reviewer  \n",
      "Strengths: The Transition Plan includes a clear plan to achieve the overall goal of launching a case-based  reasoning (CBR) system to inform clinical practice for diagnosis and treatment of TBI as well as defined  development pathways for the 4 component projects. The clearly articulated transition plan for Project 2  \n",
      "relies on the existing partnership with Moberg Analytics. The new “accessibility” software, designed to  translate information from historic cases into guidance for managing current patients, will be deployed  through Moberg’s existing CONNECT product. Assuming success of phase 1, the next development step  (phase 2) is achievable. Investigators articulate the steps for commercial deployment, as well as a defined  regulatory pathway. The funding and partnering plans are well conceived. The intellectual property (IP)  strategy is well-defined and provides a defined path for commercial development. The timeline and  milestones are optimistic, but feasible assuming the project progresses as expected. The risk analysis is  well conceived.  \n",
      "Weaknesses: The successful transition of Project 2 relies exclusively on the existing partnership with  Moberg Analytics, with no backup plan discussed, which could be risky when seeking broad  dissemination of the software. A minor weakness, the plan could be improved by a more robust discussion  of possible ways to leverage DOD funding and collaborations in future development phases. The  “Regulatory Strategy” section of the application includes no information, but other parts of the  application outline plans for human subjects research and IRB approvals.  \n",
      "Statistical Plan \n",
      "Average Score: 8.1 \n",
      "Scientist Reviewer A  \n",
      "Strengths: This is a software development project.  \n",
      "Weaknesses: No weaknesses were noted.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: A statistical plan is not provided. This is appropriate given the nature of the research.  Weaknesses: No weaknesses were noted. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 21 of 36)  \n",
      "Biostatistician Reviewer  \n",
      "Strengths: Statistics are not relevant because the project is to develop some report technology.  The study design and methods are outstanding.  \n",
      "Weaknesses: There is no discussion for how reports created will be validated.  \n",
      "Discussion Notes  \n",
      "Discussion centered on whether the software development requires a validation study. Although reviewers  agreed that a priori statistical plan is not needed for the development of the tool, reviewers questioned the  value of the report generator if no plans for its validation are presented. Some reviewers indicated that if  the tool is generated but not tested and its success is not measured (such as by surveys), it is of no value.  \n",
      "Personnel \n",
      "Average Score: 8.8 \n",
      "Scientist Reviewer A  \n",
      "The project is co-led by Mr Craig Maddux, Dr Brandon Foreman, and Dr DaiWai Olson.  \n",
      "Co-leader Dr DaiWai Olson is professor, departments of neurology and neurosurgery, UTSW, Dallas.  DaiWai Olson received a PhD in nursing from the University of North Carolina at Chapel Hill in 2007.  \n",
      "Co-leader Dr Brandon Foreman is associate clinical professor, Department of Neurology, UC. Brandon  Foreman received an MD in 2007 from UT Southwestern Medical Center, Dallas.  \n",
      "Co-leader Mr Craig Maddux is chief technology officer at Moberg Analytics. Mr Maddux obtained a BS  in computer science from California State University in 1986 and an MA in film from Staffordshire  University, United Kingdom, in 2016.  \n",
      "Strengths: Mr Maddux will lead software engineering tasks at Moberg Analytics. Dr Foreman will lead  the strategic plan for designing the multidisciplinary report from the neurointensivist perspective. Dr  Olson will lead the strategic plan for designing the multidisciplinary report from the nursing perspective.  The team is experienced and can carry out the proposed project.  \n",
      "Weaknesses: The lack of synergy between the investigators dampened the enthusiasm.  Scientist Reviewer B  \n",
      "Strengths: The project is co-led by Craig Maddux; Brandon Foreman, MD; and DaiWai Olson, RN, PhD.  Mr Maddux from Moberg Analytics has a strong background and expertise to lead all the software  engineering tasks pertaining to this project. Dr Foreman at UC has strong qualification for leading the  effort of designing the multidisciplinary report from the neurointensivist perspective. Dr Olson at UTSW  has a highly appropriate background/expertise for providing input toward the proposed efforts for  designing the multidisciplinary report from the nursing perspective.  \n",
      "Weaknesses: No weaknesses were noted. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 22 of 36)  \n",
      "Project 3: Interoperability - Augmenting Clinical Workflows  \n",
      "OVERVIEW  \n",
      "The Project Leaders of this project propose to (1) build and test a patient-specific large language model  (LLM) interface to a text-based medical record; (2) implement quality control procedures and algorithms  for aligning the language model, retrieving patient context, and filtering other voices and sounds; (3)  design and validate clinical workflow with nursing team for creating, reviewing, and assigning voice-to text annotations; and (4) validate information retrieval process from the perspective of a user. The specific  aims are to (1) design and validate a clinical process for submitting voice-to-text annotations to LLMs and  (2) design and validate clinical processes for retrieving information from LLMs.  \n",
      "\n",
      "\n",
      "SCORED CRITERIA  \n",
      "Research Strategy and Feasibility \n",
      "Average Score: 4.0 \n",
      "Scientist Reviewer A  \n",
      "This project describes to establish a clinical workflow for submission of voice-to-text annotations in the  neuroICU by designing software models to interface patient-specific LLM. The deidentified clinical notes  provided by Dr Foreman (a team member) will serve as the train and test data set. An annotation audit  trail will be set in place that ensures that the raw data and its embedding are indexed together. During the  annotation submission process, the system will initiate the identification of the voice’s origin. Utilizing  speaker recognition algorithms, it will attach unique tags to annotations corresponding to specific users or  sources. Contradiction detection will be integrated into the annotation submission workflow as a  multistep process to ensure consistency and accuracy in annotations, particularly when recording patient  information. After the annotations are translated from speech to text, the team will employ advanced  natural language inference (NLI) techniques in language model libraries (LLMs) models specifically  trained for contradiction detection to analyze these texts. This project will use precollected patient data  and build on top of preliminary research and data. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 23 of 36)  \n",
      "For the information retrieval process, this entails building a Fast Healthcare Interoperability Resources  (FHIR) FHIR and applying LLMs to extract insights from the data as well as parse, clean, and integrate  with other data types. The project team will also address the issue of hallucinations due to inconsistencies,  biases, and noise in training data by controlling the temperature parameter. To increase trust in the model,  the ReACT framework will be utilized to perform information retrieval as well as offer transparency to  the user asking the question. ReAct prompts LLMs to generate both verbal reasoning traces and actions  pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to  create, maintain, and adjust high-level plans for acting, while also interacting with the external  environments to incorporate additional information into reasoning. Last, an annotation correction  workflow will be in place to allow the people in the care team to correct annotations by updating  information in the LLM if they observe that mistakes have been made in the past.  \n",
      "Strengths: The idea of creating an LLM-based workflow to annotate EMR by voice-to-text, implementing  strategies to ensure data integrity, ease of update of information, and subsequently, ease of information  retrieval, if successful, would enhance the richness of the information stored and its value for “reuse” for  future TBI cases. This project aligns with the TBIPHRP focus area of “Understand” by attempting to  develop a novel communication technology that could facilitate the clinical translation of evidence-based  treatments and guidance to the neuroICU. The project team has a robust and comprehensive computing  infrastructure at both Moberg Analytics and Drexel University.  \n",
      "Weaknesses: Though the overall idea of augmenting the clinical workflow to increase interpretability and  reuse of information using an LLM interface seems to be an innovative way to leverage the use of LLMs  for patient care, the proposed strategy needs to be more thoroughly fleshed out. This project strategy  could be strengthened by clarifying the testing phase of the annotation submission and retrieval process.  According to the project description, subject matter experts will evaluate both the annotations retrieved by  LLMs and those written into the database. The basis for the evaluation ranking, and how the performance  of the system based on the ranking leads to a more improved system are unclear. The preliminary  experiments focused on applying a LangChain to find similar patients from a doctor’s note. This uses an  approximate nearest neighbor search to identify sentences that are semantically similar to a given source  sentence. However, there is a theoretical gap between the proposed research strategy and the proof of  concept. Figure 14 presents the overview illustration of the preliminary experiment, but it does not clarify  how the embeddings from both the OpenAI and retrievalQA stored in the vector database contribute to the  results obtained in the similar patients search. The training data for this project will be notes provided by  Dr Brandon Foreman at UC Medical Center who pioneered “multimodal reports” that include summaries  of multimodal data as a clinical decision support tool. However, these reports are time consuming to  produce, though shown to improve clinical outcome in patients with severe TBI. There are no details  provided in Project 3 with respect to the size of the notes. This is an essential missing piece of  information, as LLMs thrive on abundance of data to train. This also raises questions about the scalability  of the project.  \n",
      "The focus of Project 3 is the development of a computing intensive technology to support the clinical  workflow; however, there are no details on the underlying technology and the structural data framework  that would support a seamless integration of the annotations along with the text based EMR. The team  plans to focus on the annotation submission process for the first 3 years of the overall project duration,  while the annotation retrieval process will overlap for Years 2 and 3. This timeline should facilitate an  iterative refinement process of the submission and information retrieval process if there is a strong  evaluation plan with room to enhance the process based on the results. However, given that hardly any  details are provided on the role of the evaluation process in the technology development, the timeline  appears less plausible. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 24 of 36)  \n",
      "Scientist Reviewer B  \n",
      "Strengths: Creating a patient-specific LLM model that can be used to drive text-based medical reports  will minimize errors, and hence the project is very significant. However, it is difficult to assess whether  the project is feasible. The project fits perfectly within the intention of the funding mechanism.  The proposed approach toward generating the patient-specific LLM interface to text-based medical  reports is highly significant.  \n",
      "Weaknesses: The way the proposal is written, it seems that the algorithm will be generated and tested on  one site and then deployed across various sites. No mitigating strategy to incorporate accents of different  users and the errors introduced therein by the model at different sites is mentioned, which dampens the  confidence that the project will have a successful outcome. It is difficult to assess whether the research  requirements of the project can be supported by the availability of and accessibility to facilities and  resources including patient populations, samples, and collaborative arrangements. The patient population,  on which the LLM model will be created, is not discussed. There are no preliminary data relevant to the  population the investigators want to study. The investigators only outline the theoretical concepts of the  approach they intend to develop in this project. Hence, it is difficult to assess whether the proposed  methods may approach the proposed hypothesis. It is difficult to assess whether the data collection  instruments are appropriate to the proposed study. The investigators have not outlined how the previous  cases and the future cases will be consented. The proposal does not identify any potential barriers to  accrual and provide mitigation plans for addressing unanticipated delays.  \n",
      "Discussion Notes  \n",
      "Reviewers expressed enthusiasm for the idea of developing an automated system to transcribe recorded  speech to supply a patient-specific LLM and support EMR. However, the voice-to-text technology is not  novel, and it was unclear to reviewers how the proposed model would be better than the methods that are  currently available. Reviewers expressed ethical concerns with the system development that were  deepened by a lack in detail on how the recorded information will be managed. Broader concerns include  a limited description or lack of discussion on the patient demographics and the type of data that will be  used to train the model, which could introduce bias, and also, whether the patient information that will be  used will come from neuro-ICU patients or whether the data/information will be relevant to critical care  patients, which is the population for which the tool is intended. Reviewers identified potential areas of  conflict, where, for example, 2 contradicting measurements are identified by the AI (which is typical in  the context of critical care/ICU setting). It was unclear how the LLM will be trained to decide the  appropriate countermeasures. Minor logistical and technical issues related to patient confidentiality and  HIPAA compliance were also noted, such as how the voice recordings would be protected and the data  stored. A general paucity in detail significantly reduced the reviewers’ enthusiasm for this project.  \n",
      "Impact \n",
      "Average Score: 5.8 \n",
      "Scientist Reviewer A  \n",
      "Strengths: In the short term, the implementation of a patient specific LLM based model that facilitates  ease of annotation of EMR and retrieval of information could decrease the time required to find  information about the patient. This would imply more time available for care providers, especially nurses,  to spend with patient and possibly improve quality of care. In the long term, a system that converts  comments from the nurse to a structured record of the context of care would enrich stored information  about the case and be more informative for possible reuse. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 25 of 36)  \n",
      "Weaknesses: The inherent assumption is that time saved in data entry by the novel voice technology that  captures patient context and enriches patient EMR will equal increased time by patient’s bedside.  However, that might not necessarily be the case. The technology could increase complexity in time  needed to verify, correct, and update information.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The use of LLMs to generate a user-specific text-based medical report is very impactful.  If successful, the long-term benefits will significantly impact patient care and quality of life.  \n",
      "Weaknesses: The investigators do not clearly outline how the proposed research will provide or improve  short-term benefits for individuals. The investigators do not outline based on their own experience or data  set how much error was introduced due to a change in the user using the speech-to-text converter.  \n",
      "Consumer Reviewer  \n",
      "Strengths: Project 3 aims to overcome current clinical challenges imposed by traditional EMR. By  leveraging advances in AI and large language models, the outcomes of the project, if successful, are of  high impact. Short-term impacts to clinical users are readily apparent: reducing time spent manually  annotating EMRs, reducing potential for human error, and increasing “data richness” of patient records; at  the same time, much could be gained from a system that reasons and “talks back,” by providing guidance  to clinical staff (assuming the suggestions are reasonable). Long-term benefits to TBI patients are also of  high impact through the reduction of error, increased resolution, and usability of patient data to inform  clinical decision making, and increased transparency and accountability. Overall, the proposed project has  potential to transform EMR management and use.  \n",
      "Weaknesses: A minor concern is that because the technology will need to be developed, potential impacts  to patients in particular appear to be all long term (commercialization slated for 2030).  \n",
      "Relevance to Military Health \n",
      "Average Score: 6.3 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The project team states that automated annotation workflows can be translated into the field as  the long-term goal is that each soldier has access to a personal AI assistant, specialized in medicine, who  can guide them through routine procedures and interventions related to TBI.  \n",
      "Weaknesses: However, this appears to be a futuristic idea that involves multiple steps before being  feasible on the field. The intensity of warfare and current limited resources could significantly limit the  use of a highly computational-intensive and complex model, as proposed in this project.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The proposed research complements DOD areas of research interest for TBI. The proposed  approach can provide dual benefits for both civilian and military populations.  \n",
      "Weaknesses: The proposed research may not enhance readiness and recovery on the battlefield, during  training, or in resource-limited environments as it is designed as a clinical care project. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 26 of 36)  \n",
      "Consumer Reviewer  \n",
      "Strengths: The project complements multiple areas of research interest and patient care for TBI by  developing a novel communication technology (based on large language models) to facilitate and enrich  patient charting, reduce the amount of time, increase the “richness” of a patient’s data. The military  benefit is easily gleaned, particularly as it relates to easing workflows in hectic environments or where a  diverse cadre of care providers is present/involved. The dual-use nature of the technology is readily  apparent; although the technology would need to be further adapted to and tested in field settings, the  potential impact, and benefits in a military setting (voice commands in a hectic environment where patient  data may otherwise be lost altogether) are readily apparent.  \n",
      "Weaknesses: A weakness is that potential benefits to military personnel are all long term, since the  technology will be developed, piloted, and initially employed in university hospitals.  \n",
      "Discussion Notes  \n",
      "Reviewers agreed that a technology that would increase the probability of entering and retrieving accurate  patient data in the context of the military environment would be impactful. However, reviewers remained  skeptical that any device or technology that depends on a battery or remote systems would be widely  implemented and disseminated. One reviewer indicated that forward and paratroop medics are limited for  what they can carry. Others expressed concerns with the idea of troops carrying potential confidential or  sensitive information in an AI-enabled device.  \n",
      "Regulatory Strategy and Transition Plan \n",
      "Average Score: 6.0 \n",
      "Scientist Reviewer A  \n",
      "Strengths: A general transition plan is provided that includes a recognition that developing the LLM  comes with risks that are recognized, as it has been done before.  \n",
      "Weaknesses: There is no regulatory strategy provided. A regulatory framework should apply to extent  required for a digital health solution that interacts directly with patient records. The novel voice  technology to be developed, if successful, would interact with patient’s EMR and could update or modify  information. The transition plan is insufficient as presented. The eventual goal is commercialization  which appears more focused on profit margin and is outside the scope and duration of project. Moberg  Analytics retains ownership of the software. It mentions nothing about the AI technology partner at  Drexel.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The transition plan is well-defined.  \n",
      "Weaknesses: The entire software will rest with Moberg Analytics with all the potential data sources; that  raises concerns about how the software will be distributed in the community when realized. There is no  plan to involve an external validation to verify the authenticity of the records.  \n",
      "Technology Transfer Specialist Reviewer  \n",
      "Strengths: The Transition Plan includes a plan to achieve the overall goal of launching a case-based  reasoning (CBR) system to inform clinical practice for diagnosis and treatment of TBI as well as defined \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 27 of 36)  \n",
      "development pathways for the 4 component projects. While the investigators outline a transition plan for  Project 3, a tool to save nursing time by augmenting electronic medical records through voice-to-text  annotations, the plan has challenges as noted in the weaknesses section. Assuming success of phase 1, the  next development step (phase 2), is achievable. Investigators articulate the steps for commercial  deployment, as well as a defined regulatory pathway. The intellectual property (IP) strategy is well defined and provides a defined path for commercial development. The timeline and milestones are  optimistic but feasible assuming the project progresses as expected. The risk analysis is well conceived.  \n",
      "Weaknesses: The proposed commercialization plan for Project 3 (a tool to augment EMR via voice-to-text  annotations) relies on a yet to be established partnership between Moberg Analytics and Vocera, which  already has a voice capture system. The investigators do not clearly articulate how the proposed tool,  specific to neurocritical care, will add value to Vocera’s existing tool to an extent that Vocera will want to  engage in and fund a commercialization partnership. There is no mention of other potential partners. A  minor weakness is that the plan could be improved by a more robust discussion of possible ways to  leverage DOD funding and collaborations in future development phases. The “Regulatory Strategy”  section of the application includes no information, but other parts of the application outline plans for  human subjects research and IRB approvals.  \n",
      "Statistical Plan \n",
      "Average Score: 6.6 \n",
      "Scientist Reviewer A  \n",
      "Strengths: The project description includes a brief description of a statistical analysis plan.  \n",
      "Weaknesses: The basis for the evaluation of annotation accuracy and relevance, especially as it relates to  the ranking to be obtained from the medical professionals, is not provided. Hence, it is hard to assess the  strength of the plan. It appears that the project team has not determined what metrics would be utilized for  the prompt engineering process. There are no descriptions or citations for the metrics suggested for  further investigation. The plan for evaluation of the machine learning component appears to be out of  alignment with the project tasks.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: This is a software development project, and thus a statistical plan is not required.  Weaknesses: No weaknesses were noted.  \n",
      "Biostatistician Reviewer  \n",
      "Strengths: This project is going to develop and validate a workflow for clinical care team to manage  voice-to-text annotations made in the neuro-ICU. This workflow includes both submitting voice-to-text  annotations to a patient-specific large language model (LLM) and retrieving information from LLM.  Annotation accuracy and relevance analysis methods are provided. Cross-validation will be used to fine tune the model.  \n",
      "Weaknesses: The LLM architecture and how it is trained are not provided. There is no mention how many  parameters will be used in the LLM training. It is a concern whether the study sample is sufficient to train  LLM. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 28 of 36)  \n",
      "Personnel \n",
      "Average Score: 5.5 \n",
      "Scientist Reviewer A  \n",
      "Project 3 will be led by both Mr Craig Maddux (Moberg Analytics) and Dr Edward Kim (Drexel  University).  \n",
      "Co-lead Dr Edward Kim is associate professor, Department of Computer Science, Drexel University,  Philadelphia. Edward Kim obtained PhD in computer science from Lehigh University in 2013.  \n",
      "Co-lead Mr Craig Maddux is chief technology officer at Moberg Analytics. Mr Maddux obtained a BS in  computer science from California State University in 1986 and an MA in film from Staffordshire  University, United Kingdom, in 2016.  \n",
      "Strengths: Mr Maddux and his team have a track record of producing impactful software for EMR clinical  applications, while Dr Kim and his team demonstrate a strong proficiency in neuro-inspired machine  learning.  \n",
      "Weaknesses: Dr Kim mentions in his biosketch that he has collaborated with Moberg Analytics on  multimodal machine learning for traumatic brain injury since 2019. However, there is no supporting  evidence. The details provided in the biosketches of both Project 3 team leaders and personnel description  are insufficient to demonstrate their expertise for the novel voice-based technology proposed. Providing  some evidence of expertise in use of LLMs would have strengthened the confidence in feasibility of the  proposed approach. Mr Maddux is a project team lead for each of the projects. This casts doubts on his  ability to provide the sufficient level of effort required for a project of this magnitude of complexity.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The project is co-led by Craig Maddux and Ed Kim, PhD. Mr Maddux will lead the data  integration and workflow design tasks at Moberg Analytics. Dr Kim will lead the AI development tasks at  Drexel University. The team is experienced and can carry out the proposed project.  \n",
      "Weaknesses: The project will benefit from an additional clinician from a different site. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 29 of 36)  \n",
      "Project 4: Reusability - Growing a Shared Network of Knowledge  \n",
      "OVERVIEW  \n",
      "The Project Leaders of this project propose to (1) initialize a scalable federated network of hospitals and  (2) deploy and evaluate the deliverables from the other 3 projects. The specific aims are to (1) initiate the  federated network and set up each of the 6 data sets, (2) deploy the software modules from each of the  projects, and (3) perform evaluation testing on software modules developed from the first 3 projects.  \n",
      "\n",
      "\n",
      "SCORED CRITERIA  \n",
      "Research Strategy and Feasibility \n",
      "Average Score: 4.8 \n",
      "Scientist Reviewer A  \n",
      "The overall strategy is to house patient data locally at interconnected sites in a data consortia network  using federated learning rather than pooling to a single central repository. Federated learning is a model  training solution that allows multiple parties to train models simultaneously with local data. Trained  models are then pooled together from various sites without the need to exchange protected health  information (PHI). There are 3 main project tasks: set up of federated network, software module  deployment, and information reusability evaluation. For task 1, CONNECT, as the underlying data  infrastructure, will be initiated at each of the 3 sites (UC, UTSW, and Penn). This entails extracting and  preprocessing of patient data as well as software installation and loading of patient data. Three publicly  available data sets (TRACK-TBI, BOOSTII, and BOOST3) will also be integrated by hosting them on the  IBM Cloud. CONNECT will be enhanced with additional capabilities for federal connectivity such that  all users in the network can request and access non-PHI containing patient data for similarity assessment.  In Aim 2, software module deployment, the deliverables from Projects 1, 2, and 3 (CBR, reporting  workflow, annotation workflow) will be deployed at each site. Aim 3, information reusability evaluation,  will focus on getting the software to be used by an independent set of users and collecting feedback that  could drive future development. This feedback will be prioritized such that it can be applied in improving  the system. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 30 of 36)  \n",
      "Strengths: The idea of implementing a decentralized infrastructure for sharing high resolution TBI data  across multiple sites such that they can be mined to support clinical decision making by an external user is  innovative. The framework includes publicly available data repositories as well. The project is relevant to  the TBIPHRP focus area, treat, by supporting dissemination of interventions that address  neurodegenerative processes associated with TBI.  \n",
      "Weaknesses: Projects 1, 2, and 3 address different aspects of CRITICAL-TBI aimed toward the overall  goal of optimizing patient characterization and management through the reuse of relevant information.  Federated learning allows for sharing of model results and parameters. According to the project  description, only model parameters will be retrieved and shared with the external user. It would have been  helpful to provide an example of sharable model parameters, in line with CBR, for mining similar patient  data. If Projects 2 and 3 are devoted to context rich reporting and annotation of clinical workflow, it is  unclear how the detailed patient characterization information will be summarized and shared when  mining for similar patient case. There are 3 physical sites and a cloud-based site that hosts 3 data sets. It  isn’t clear which project lead will be responsible for the management of the cloud-based sites.  No preliminary data or results are provided for Project 4 as it is a capstone overall deployment project.  This makes it highly dependent on the outcome of Projects 1, 2, and 3. The project team does not address  this issue of an alternate plan, if the outcomes of either Project 1, 2, or 3 fail. The second primary  objective of Project 4 is to deploy and evaluate (information reusability evaluation), according to the  project description, a technology validation effort. Yet, no evaluation plan is presented. The basis and  rationale for the evaluation protocol of each of the 3 components are not adequately described.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The application presents a well-established rationale for the need of the proposed system,  which will provide the capability for reusing the data collected from TBI patients, with the aim of  advancing clinical management of these patients using case-based reasoning method. Their main aim of  this project is to deploy the software modules of Projects 1 to 3 in a federated network of hospital sites for  the purpose of presenting the data in an informative manner, and also for practical data mining/reusing of  the vast and multiscale amount of data collected and annotate as part of TBI clinical management. Their  overall strategy is highly relevant to the following FY23 TBIPHRP focus areas: focused area 2 (prevent  and access): “Validation of objective tools/methods for assessing and real-time health status monitoring of  psychological health conditions and/or TBI” and focused area 3 (treat): “Identification and evaluation of  methods for successful dissemination and implementation of interventions.”  \n",
      "Storing/data analysis of TBI data with PHI at a local level, and only exchanging model parameters among  the network users, should reduce the likelihood of HIPAA violation and also be much more practical as  the sizes of some data modalities are too large for practical transfer to/from a central repository site.  Appropriate tasks for the 3 aims of the project are described in the Statement of Work (set up federated  network, deploy software modules of Projects 1 to 3, evaluate/validate the deployed system at multiple  clinical sites).  \n",
      "Weaknesses: The description of tasks and efforts to accomplish them is too general and lacks adequate  specificity. For example, for evaluation of each of the 3 software modules (developed in Projects 1 to 3),  it is only stated that they will “distribute it through project portals for the participating sites to download  and review.” A more detailed description of the planned qualitative/quantitative steps for this  evaluations/review is needed. The overall system will be based on their CONNECT platform with added  software modules of Projects 1 to 3. Information about this platform is very limited for a proper review of  the technology. Ambiguity/inconsistencies about the clinical sites that will evaluate the platform: Under  Aim 4.3 Strategy and Approach, they state that “three sites that were not involved during the development \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 31 of 36)  \n",
      "process will be used for evaluation: UC and UTSW.” Apart from a missing third site, both UC and UTSW  will be involved in the development of software modules (eg, Project 2).  \n",
      "Discussion Notes  \n",
      "Reviewers agreed that, although very well written, this project lacks detail and rigor that would  adequately demonstrate success of the CRITICAL-TBI program. Reviewers discussed the strengths and  weaknesses of the federated learning approach. Initially reviewers indicated that federated learning would  be applied to retrieve the parameters that could inform decision making. However, the decision-making  parameters are not adequately outlined. In the end, reviewers concluded that the output would not be  parameters, but data, suggesting a lack of clarity on how federated learning would be deployed in the  network of hospitals/institutions. Reviewers further discussed the methodology for evaluation of the  program. Although the project indicates that feedback will be collected, it is unclear how this will be  accomplished—whether questionnaires or surveys would be deployed. Further, the success—and  therefore potential impact—of Project 4 is heavily dependent on the successes of Projects 1 to 3. A final  concern is that current PHI standards were built around traditional EMR; the incorporation and analysis of  high-resolution data sets with advanced analytic technology significantly increase risk for reidentification  and pose potential for negative impacts. Mitigation of and safeguards against potential negative impacts  (ie, introduction of bias by AI in the CBR and treatment recommendations) are limitedly discussed and  minorly limit enthusiasm. Potential HIPAA violations due to voice-to-text concerns are completely  overlooked. Finally, many reviewers expressed concerns with PHI, specifically in the case of rare  conditions.  \n",
      "Impact \n",
      "Average Score: 7.8 \n",
      "Scientist Reviewer A  \n",
      "Strengths: If the project is successful, short term impact is that it will be the first large-scale, multi institution repository of high-resolution data for TBI patients that can utilized collectively for CBR and  clinical decision support. The network would provide an infrastructure that facilitates scalable  accumulation of high-quality TBI data. Regarding long term impact, an established federated data  repository will be highly useful in validating methods for assessing the real-time status of TBI patients.  \n",
      "Weaknesses: The positive impacts could be reduced due to limited open access. There are also ethical  concerns, and regulation across sites with different standards could hinder impact.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: Project 4 will develop a platform that would allow a new method of TBI management based on  case-based reasoning (CBR) methodology. Despite the promising power of CBR, it has not been utilized  in NCC including clinical management of TBI. The software platform to be developed and distributed in  multiple clinical sites is aimed at overcoming the factors that have limited the use of CBR in NCC. If  successful, it will have a very high impact in TBI management by providing highly individualized and  precision guidelines that will be based on learning from past cases of TBI with similar characteristics to  the individual TBI patient under care. In addition to enabling implementation of CBR-based AI, the  product will allow preserving and making easily accessible the high volume/type and multiscale,  multimodality type of data that are currently acquired during TBI clinical care. The platform will allow  easy accessibility of these data volumes for applications involving evolving machine learning and  artificial intelligence methods. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 32 of 36)  \n",
      "Weaknesses: The software modules of this application appear to be mostly beneficial for NCC and for  short term monitoring of the patients following a brain injury. However, it is not clear whether/how this  system can be utilized for longer-term TBI management which would increase the impact of the project. It  is presumed that long-term monitoring of TBI patients will require additional/different types of report  modules that would encompass, for example, measurements of neurocognition and neuropsychological  abnormalities (eg, depression and PTSD). It’s unclear if the infrastructure of the software is designed for  such expansion.  \n",
      "Consumer Reviewer  \n",
      "Strengths: If realized, this highly ambitious proposed project will “deploy and validate” tools developed  in Projects 1 to 3 at multiple test sites and address multiple subareas of the TBIPHRP focus areas (most  notably addressing prevent/assess focus areas) and is of high impact. This project contains multiple  significant strengths including leveraging historical data in existing repositories, collecting and analyzing  high-resolution data to enable CBR, allowing voice-to-text annotation of patient files, and reducing  bedside burden for data collection, while enhancing usability of existing data to inform clinical care.  Successfully implemented, the project has the potential to greatly improve TBI patient care.  \n",
      "Weaknesses: The project contains several weaknesses that limit overall enthusiasm for the project. First,  the realized potential for both short- and long-term impacts completely relies on the assumption that the  collection, annotation, and incorporation of historical and high-resolution data will actually result in  meaningful knowledge gains sufficient to improve—if not “revolutionize”—the clinical care and  outcomes of TBI patients in real time. Project 4 is essentially a “test and validation” system, leveraging  existing high-resolution data being collected at the partner sites and will have unknown real-time impacts  until it is successfully implemented.  \n",
      "Relevance to Military Health \n",
      "Average Score: 8.0 \n",
      "Scientist Reviewer A  \n",
      "Strengths: A decentralized data library, that consists of searchable, high-quality data sets, to aid in the  learning of TBI management will be of significant value to the military. The goal is to develop an  example system for improving care via CBR that could immediately be used in military hospitals, once  mobile digital platforms are integrated into the care process.  \n",
      "Weaknesses: No weaknesses were noted.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The software modules of this project will have immediate application in the VA specialized  clinics and trauma centers. They expect that upon commercialization of the product it will become  immediately applicable in Roles 3 and 4 in military health. Furthermore, individual components of the  overall system can be used in Role 1 (medical record) and Role (telehealth). The plan is to develop more  specific military related aspects of the project in phase 2 (following the termination of this 4-year project).  \n",
      "Weaknesses: There is no planned input from the military health establishments and VA facilities for this  application. Such input would allow the design of the report modules to have the needed flexibility for  adding military/VA specific report items. The circumstances leading to brain injury in the military are  different than those encountered by the civilian population (eg, battlefield/military exercise injuries)  which often result in comorbidities such as depression, PTSD, and sleep disorders. It would be highly \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 33 of 36)  \n",
      "beneficial for the software module to be developed to have the flexibility of analyzing/presenting data  reflecting such comorbidities.  \n",
      "Consumer Reviewer  \n",
      "Strengths: Fully realized, the proposed project is highly relevant to military health, with the ultimate goal  of bringing “precision care for TBI patients to the battlefield.” The project addresses multiple areas of  interest for clinical care related to TBI including that of disease understanding, prevention/assessment,  and treatment. Advances in understanding would be immediately transferrable to TBI patients and  clinicians in military hospitals. Longer term, the technology could be mobilized and integrated into care  in battlefield settings. Overall, the deliverables of this project are highly relevant to military health with  not only dual-use capacity but could also presumably be easily adapted to other diseases.  \n",
      "Weaknesses: The current project does not propose implementation in a military setting or use historical  data from military service associated TBI and therefore would require full implementation before  realizing its goals. This is a moderate weakness. Finally, specifically how success will be evaluated and  monitored is limitedly addressed.  \n",
      "Regulatory Strategy and Transition Plan \n",
      "Average Score: 6.3 \n",
      "Scientist Reviewer A  \n",
      "Strengths: Given Mr Moberg’s experience with bringing this type of EMR technology through  development and commercial use, the commercialization risks are low.  \n",
      "Weaknesses: There is no regulatory strategy provided. This is a significant weakness of this project. A  regulatory framework should apply to extent required for a digital health solution that interacts directly  with patient records and spans multiple institutions. The transition plan is insufficient as presented. No  \n",
      "details are provided for commercialization efforts. If Moberg Analytics retains ownership of the software,  the rights of the data consortia are unclear. The governance framework is not described.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: There is a well-planned and well-presented translational and funding plan for product  development and commercialization. The current application will be followed by a follow-up effort  targeting the collection of 1,000 highly annotated cases by partnering with clinical trials as well as the  collaborators for this application. Following this phase, they plan to offer the system for clinical use  utilizing a subscription model where hospitals contribute data and also pay to use the system of this  application. For regulatory approval, they plan to follow the FDA Final Guidance on Clinical Decision  Support Software and use a Software-only Medical Device Data System classification in their future FDA  application. There is a well-planned commercialization strategy: The software modules developed in this  project will be commercialized separately through their current product CONNECT as an additional  feature.  \n",
      "Weaknesses: No weaknesses were noted.  \n",
      "Technology Transfer Specialist Reviewer  \n",
      "Strengths: The Transition Plan includes a plan to achieve the overall goal of launching a case-based  reasoning (CBR) system to inform clinical practice for diagnosis and treatment of TBI as well as \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 34 of 36)  \n",
      "development pathways for the 4 component projects. Investigators outline a transition plan for Project 4  (creating a federated network of hospitals forming the library of cases for CBR), via an existing  partnership with Moberg Analytics. The plan has merit but also some challenges as noted in the  weaknesses section. Assuming success of phase 1, the next development step (phase 2) is achievable.  Investigators articulate the steps for commercial deployment, as well as a defined regulatory pathway. The  intellectual property (IP) strategy is well-defined and provides a defined path for commercial  development. The timeline and milestones are optimistic, but feasible assuming the project progresses as  expected. The risk analysis is well conceived.  \n",
      "Weaknesses: A moderate weakness is that the proposed commercialization plan for Project 4, to develop a  federated network of hospitals forming the library of cases for CBR, relies on individual hospitals (or  systems) joining a network which will require the hospitals to harmonize their data via a software licensed  by Moberg Analytics as well as licensing additional software required to enable the federation. It is  unclear how many hospitals are likely to license the software and whether or not enough hospitals will  join in order to scale the tool for maximum impact. The application would benefit from more details on  how Moberg Analytics will address the challenges associated with navigating the varying procurement  processes employed by different hospital systems. The successful transition of Project 4 relies exclusively  on the existing partnership with Moberg Analytics, with no backup plan discussed, which could be risky  when seeking broad dissemination. A minor weakness, the plan could be improved by a more robust  discussion of possible ways to leverage DOD funding and collaborations in future development phases.  The “Regulatory Strategy” section of the application includes no information, but other parts of the  application outline plans for human subjects research and IRB approvals.  \n",
      "Statistical Plan \n",
      "Average Score: 5.5 \n",
      "Scientist Reviewer A  \n",
      "Strengths: No strengths were noted.  \n",
      "Weaknesses: No statistical plan is provided. However, this is essential for Project 4, a technology  validation effort. The details of the evaluation protocol are not outlined. It is unclear how the sample size  at each local site vs the cloud-based repository affects the performance of the tool.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: A statistical plan is not provided. This is appropriate given the nature of the research.  Weaknesses: No weaknesses were noted.  \n",
      "Biostatistician Reviewer  \n",
      "Strengths: The study design is innovative. Hyperparameters will be retrieved and integrated from different  projects.  \n",
      "Weaknesses: There is no discussion about data curation. No statistical method is provided for the  validation, particularly whether the selected parameters capture the essential information for the  reference. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 35 of 36)  \n",
      "Discussion Notes  \n",
      "The PI notes that “no statistical plan is needed for this project because it is a technology validation  effort.” Reviewers disagreed on whether an a priori plan is needed; some argued that clear and explicit  criteria for success need to be described and measured. Most reviewers indicated that the evaluation  testing of the software modules should include a clear and well-described quantitative approach. Others  further noted that even a qualitative evaluation requires a plan that is not explicitly described for Project  4.  \n",
      "Personnel \n",
      "Average Score: 8.8 \n",
      "Scientist Reviewer A  \n",
      "Project 4 will be co-led by Dr Ramon Diaz-Arrastia and Mr Richard Moberg, along with Coinvestigators  Dr Jed Hartings and Dr DaiWai Olson.  \n",
      "Co-leader Mr Richard Moberg is founder and chief executive officer of Moberg Analytics, Inc. Mr  Moberg received an MSE in biomedical engineering from the University of Pennsylvania in 1976.  \n",
      "Co-leader Dr Ramon Diaz-Arrastia is professor of neurology at the University of Pennsylvania, where he  directs the Traumatic Brain Injury Research Center. Ramon Diaz-Arrastia has a PhD in biochemistry and  MD from Emory University in 1988.  \n",
      "Coinvestigator Dr Jed Hartings is professor and vice chair for research, Department of Neurosurgery, UC.  Jed Hartings has a PhD in neuroscience from University of Pittsburgh in 2000.  \n",
      "Coinvestigator Dr DaiWai Olson is professor, departments of neurology and neurosurgery, UTSW, Dallas.  DaiWai Olson received a PhD in nursing from University of North Carolina at Chapel Hill in 2007.  \n",
      "Strengths: The project is led by a team of established and productive researchers in TBI related research.  They all bring a wealth of experience to the project. Mr Moberg is a pioneer in the technology and  applications of multimodal neuromonitoring and is known internationally for these efforts. Mr Moberg  led the successful development of CONNECT, a neurocritical care platform for precision management of  brain-injured patients which is the underlying data infrastructure for Project 4. Dr Hartings is  internationally known in the field of neurotrauma research and particularly in endophenotypes.  \n",
      "Weaknesses: It is not clear which team lead is responsible for the deployment of the project deliverables  for the data sets hosted on the IBM cloud. According to the project description, Dr Olson will be  responsible for the evaluation of the protocols. His area of expertise is primarily focused on nursing care  impacts on patient outcomes following acquired brain injury. There is no brief description or outline of  the aspects of the evaluation protocol which raises concerns on limited expertise of the team in area of  evaluation.  \n",
      "Scientist Reviewer B  \n",
      "Strengths: The project is co-led by Richard Moberg (Moberg Analytics) and Ramon Diaz-Arrastia, MD,  PhD (UPenn). Mr Moberg is highly qualified to lead the efforts with regard to leading the software  deployment and data consortium effort among the participating sites in the project. Dr Diaz-Arrastia is  highly qualified to lead the evaluation effort among the participating sites. Coinvestigators Dr Olson and \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "TP230309  \n",
      "(Page 36 of 36)  \n",
      "Dr Hartings have highly appropriate qualifications to lead the software evaluation at UTSW and at UC,  respectively.  \n",
      "Weaknesses: No weaknesses were noted. \n",
      "Procurement Sensitive Information Do not copy or distribute  \n",
      "\n",
      "Average  \n",
      "Score\n",
      "Standard  \n",
      "Deviation\n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0 (lowest merit)\n",
      "2.7  \n",
      "(Fair) \n",
      "0.3 \n",
      "Application \n",
      "Overall  \n",
      "Program \n",
      "Project 1 \n",
      "Project 2 \n",
      "Project 3 \n",
      "Project 4 \n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0  (lowest merit) \n",
      "2.7 (0.3) \n",
      "2.7 (0.2) \n",
      "2.2 (0.2) \n",
      "2.4 (0.2) \n",
      "2.9 (0.2) \n",
      "2.6 (0.2) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluation Criteria \n",
      "Rating Scale: 10.0 (highest merit) to 1.0  (lowest merit) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Program \n",
      "\n",
      "5.3 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Leadership \n",
      "\n",
      "7.3 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Implementation Plan \n",
      "\n",
      "4.5 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Impact and Relevance to  Military Health \n",
      "\n",
      "7.0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Community-Based Participatory  Research \n",
      "\n",
      "6.3 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data and Research Resources  \n",
      "Sharing Plan \n",
      "\n",
      "5.3 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Research Strategy and Feasibility \n",
      "\n",
      "\n",
      "5.3 \n",
      "5.2 \n",
      "4.0 \n",
      "4.8 \n",
      "Impact \n",
      "\n",
      "\n",
      "7.6 \n",
      "7.9 \n",
      "5.8 \n",
      "7.8 \n",
      "Relevance to Military Health \n",
      "\n",
      "\n",
      "7.5 \n",
      "7.3 \n",
      "6.3 \n",
      "8.0 \n",
      "Regulatory Strategy and Transition  Plan \n",
      "\n",
      "\n",
      "7.6 \n",
      "7.7 \n",
      "6.0 \n",
      "6.3 \n",
      "Statistical Plan \n",
      "\n",
      "\n",
      "6.5 \n",
      "8.1 \n",
      "6.6 \n",
      "5.5 \n",
      "Personnel \n",
      "\n",
      "\n",
      "9.2 \n",
      "8.8 \n",
      "5.5 \n",
      "8.8 \n",
      "\n",
      "Average  \n",
      "Score\n",
      "Standard  \n",
      "Deviation\n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0 (lowest merit)\n",
      "2.7  \n",
      "(Fair) \n",
      "0.2 \n",
      "\n",
      "\n",
      "\n",
      "Criteria \n",
      "Rating Scale: 10 (highest merit) to 1 (lowest merit) \n",
      "Average  \n",
      "Score\n",
      "Average  \n",
      "Score\n",
      "Overall Program \n",
      "5.3 \n",
      "5.3 \n",
      "Leadership \n",
      "7.3 \n",
      "7.3 \n",
      "Implementation Plan \n",
      "4.5 \n",
      "4.5 \n",
      "Overall Impact and Relevance to Military Health \n",
      "7.0 \n",
      "7.0 \n",
      "Community-Based Participatory Research \n",
      "6.3 \n",
      "6.3 \n",
      "Data and Research Resources Sharing Plan \n",
      "5.3 \n",
      "5.3 \n",
      "\n",
      "Average  \n",
      "Score\n",
      "Standard  \n",
      "Deviation\n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0 (lowest merit)\n",
      "2.2  \n",
      "(Good) \n",
      "0.2 \n",
      "\n",
      "\n",
      "\n",
      "Criteria \n",
      "Rating Scale: 10 (highest merit) to 1 (lowest merit) \n",
      "Average  \n",
      "Score\n",
      "Average  \n",
      "Score\n",
      "Research Strategy and Feasibility \n",
      "5.3 \n",
      "5.3 \n",
      "Impact \n",
      "7.6 \n",
      "7.6 \n",
      "Relevance to Military Health \n",
      "7.5 \n",
      "7.5 \n",
      "Regulatory Strategy and Transition Plan \n",
      "7.6 \n",
      "7.6 \n",
      "Statistical Plan \n",
      "6.5 \n",
      "6.5 \n",
      "Personnel \n",
      "9.2 \n",
      "9.2 \n",
      "\n",
      "Average  \n",
      "Score\n",
      "Standard  \n",
      "Deviation\n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0 (lowest merit)\n",
      "2.4  \n",
      "(Good) \n",
      "0.2 \n",
      "\n",
      "\n",
      "\n",
      "Criteria \n",
      "Rating Scale: 10 (highest merit) to 1 (lowest merit) \n",
      "Average  \n",
      "Score\n",
      "Average  \n",
      "Score\n",
      "Research Strategy and Feasibility \n",
      "5.2 \n",
      "5.2 \n",
      "Impact \n",
      "7.9 \n",
      "7.9 \n",
      "Relevance to Military Health \n",
      "7.3 \n",
      "7.3 \n",
      "Regulatory Strategy and Transition Plan \n",
      "7.7 \n",
      "7.7 \n",
      "Statistical Plan \n",
      "8.1 \n",
      "8.1 \n",
      "Personnel \n",
      "8.8 \n",
      "8.8 \n",
      "\n",
      "Average  \n",
      "Score\n",
      "Standard  \n",
      "Deviation\n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0 (lowest merit)\n",
      "2.9  \n",
      "(Fair) \n",
      "0.2 \n",
      "\n",
      "\n",
      "\n",
      "Criteria \n",
      "Rating Scale: 10 (highest merit) to 1 (lowest merit) \n",
      "Average  \n",
      "Score\n",
      "Average  \n",
      "Score\n",
      "Research Strategy and Feasibility \n",
      "4.0 \n",
      "4.0 \n",
      "Impact \n",
      "5.8 \n",
      "5.8 \n",
      "Relevance to Military Health \n",
      "6.3 \n",
      "6.3 \n",
      "Regulatory Strategy and Transition Plan \n",
      "6.0 \n",
      "6.0 \n",
      "Statistical Plan \n",
      "6.6 \n",
      "6.6 \n",
      "Personnel \n",
      "5.5 \n",
      "5.5 \n",
      "\n",
      "Average  \n",
      "Score\n",
      "Standard  \n",
      "Deviation\n",
      "Overall Evaluation \n",
      "Rating Scale: 1.0 (highest merit) to 5.0 (lowest merit)\n",
      "2.6  \n",
      "(Fair) \n",
      "0.2 \n",
      "\n",
      "\n",
      "\n",
      "Criteria \n",
      "Rating Scale: 10 (highest merit) to 1 (lowest merit) \n",
      "Average  \n",
      "Score\n",
      "Average  \n",
      "Score\n",
      "Research Strategy and Feasibility \n",
      "4.8 \n",
      "4.8 \n",
      "Impact \n",
      "7.8 \n",
      "7.8 \n",
      "Relevance to Military Health \n",
      "8.0 \n",
      "8.0 \n",
      "Regulatory Strategy and Transition Plan \n",
      "6.3 \n",
      "6.3 \n",
      "Statistical Plan \n",
      "5.5 \n",
      "5.5 \n",
      "Personnel \n",
      "8.8 \n",
      "8.8 \n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "# Load the .docx file\n",
    "doc = docx.Document('../data/FPA Review.docx')\n",
    "\n",
    "# Read paragraphs and tables\n",
    "for para in doc.paragraphs:\n",
    "\tprint(para.text)\n",
    "\n",
    "for table in doc.tables:\n",
    "\tfor row in table.rows:\n",
    "\t\tfor cell in row.cells:\n",
    "\t\t\tprint(cell.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "model='be-bge-embeddings'\n",
    "be_api_key = os.getenv(\"BE_API_KEY\")\n",
    "base_url='https://api.blockentropy.ai/v1'\n",
    "\n",
    "client = OpenAI(base_url=base_url, api_key=be_api_key)\n",
    "\n",
    "def get_embedding(text, model=model):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "embedding = get_embedding('Manil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "clinet = chromadb.PersistentClient(path='../database/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
